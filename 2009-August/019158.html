<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Haiku-commits] r32330 - haiku/trunk/src/system/boot/platform/u-boot
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/haiku-commits/2009-August/index.html" >
   <LINK REL="made" HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r32330%20-%20haiku/trunk/src/system/boot/platform/u-boot&In-Reply-To=%3C200908132030.n7DKUMt2023279%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="019157.html">
   <LINK REL="Next"  HREF="019159.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Haiku-commits] r32330 - haiku/trunk/src/system/boot/platform/u-boot</H1>
    <B>mmu_man at mail.berlios.de</B> 
    <A HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r32330%20-%20haiku/trunk/src/system/boot/platform/u-boot&In-Reply-To=%3C200908132030.n7DKUMt2023279%40sheep.berlios.de%3E"
       TITLE="[Haiku-commits] r32330 - haiku/trunk/src/system/boot/platform/u-boot">mmu_man at mail.berlios.de
       </A><BR>
    <I>Thu Aug 13 22:30:22 CEST 2009</I>
    <P><UL>
        <LI>Previous message: <A HREF="019157.html">[Haiku-commits] r32329 - haiku/trunk/headers/private/kernel/arch/arm
</A></li>
        <LI>Next message: <A HREF="019159.html">[Haiku-commits] r32331 - haiku/trunk/src/add-ons/tracker/zipomatic
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#19158">[ date ]</a>
              <a href="thread.html#19158">[ thread ]</a>
              <a href="subject.html#19158">[ subject ]</a>
              <a href="author.html#19158">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: mmu_man
Date: 2009-08-13 22:30:20 +0200 (Thu, 13 Aug 2009)
New Revision: 32330
ViewCVS: <A HREF="http://svn.berlios.de/viewcvs/haiku?rev=32330&amp;view=rev">http://svn.berlios.de/viewcvs/haiku?rev=32330&amp;view=rev</A>

Modified:
   haiku/trunk/src/system/boot/platform/u-boot/mmu.cpp
Log:
[GSoC] [ARM] Patch by Johannes Wischert.
Partial implementation of mmu support for the bootloader, with lot of debugging output.


Modified: haiku/trunk/src/system/boot/platform/u-boot/mmu.cpp
===================================================================
--- haiku/trunk/src/system/boot/platform/u-boot/mmu.cpp	2009-08-13 18:57:36 UTC (rev 32329)
+++ haiku/trunk/src/system/boot/platform/u-boot/mmu.cpp	2009-08-13 20:30:20 UTC (rev 32330)
@@ -21,71 +21,262 @@
 #include &lt;string.h&gt;
 
 
-/*!	The (physical) memory layout of the boot loader is currently as follows:
-	  0x0500 - 0x10000	protected mode stack
-	  0x0500 - 0x09000	real mode stack
-	 0x10000 - ?		code (up to ~500 kB)
-	 0x90000			1st temporary page table (identity maps 0-4 MB)
-	 0x91000			2nd (4-8 MB)
-	 0x92000 - 0x92000	further page tables
-	 0x9e000 - 0xa0000	SMP trampoline code
-	[0xa0000 - 0x100000	BIOS/ROM/reserved area]
-	0x100000			page directory
-	     ...			boot loader heap (32 kB)
-	     ...			free physical memory
 
-	The first 8 MB are identity mapped (0x0 - 0x0800000); paging is turned
-	on. The kernel is mapped at 0x80000000, all other stuff mapped by the
-	loader (kernel args, modules, driver settings, ...) comes after
-	0x80020000 which means that there is currently only 2 MB reserved for
-	the kernel itself (see kMaxKernelSize).
 
-	The layout in PXE mode differs a bit from this, see definitions below.
-*/
 
-//#define TRACE_MMU
+#define TRACE_MMU
 #ifdef TRACE_MMU
 #	define TRACE(x) dprintf x
 #else
 #	define TRACE(x) ;
 #endif
 
-
-//#define TRACE_MEMORY_MAP
+#define ARRAY_SIZE(x) (sizeof(x) / sizeof((x)[0]))
+/*#define TRACE_MEMORY_MAP
 	// Define this to print the memory map to serial debug,
 	// You also need to define ENABLE_SERIAL in serial.cpp
 	// for output to work.
+*/
 
 
-struct gdt_idt_descr {
-	uint16 limit;
-	uint32 *base;
-} _PACKED;
 
-// memory structure returned by int 0x15, ax 0xe820
-struct extended_memory {
-	uint64 base_addr;
-	uint64 length;
-	uint32 type;
+
+/*
+TODO:
+	-recycle bit!
+*/
+
+/*!	The (physical) memory layout of the boot loader is currently as follows:
+	 0x000000000			interupt vectors
+	 0x???				u-boot
+	 0xa0000000			u-boot stuff like kernel arguments afaik
+	 0xa0100000 - 0xa0ffffff	boot.tgz (up to 15MB probably never needed so big...)
+	 0xa1000000 - 0xa1ffffff	pagetables
+	 0xa2000000 - ?			code (up to 1MB)
+	 0xa2100000			boot loader heap / free physical memory
+
+	The kernel is mapped at KERNEL_BASE, all other stuff mapped by the
+	loader (kernel args, modules, driver settings, ...) comes after
+	0x80020000 which means that there is currently only 2 MB reserved for
+	the kernel itself (see kMaxKernelSize).
+*/
+
+
+
+
+
+
+
+
+
+
+
+//defines for the page directory (aka Master/Section Table)
+#define MMU_L1_TYPE_COARSEPAGETABLE 0x1//only type used in Haiku by now (4k pages)
+//coarse pagetable entry :
+//
+//31			10 9 8      5 432 10
+//| page table address    |?| domain |000|01
+//
+// the domain is not used so and the ? is implementation specified... have not
+// found it in the cortex A8 reference... so I set t to 0
+// page table must obviously be on multiple of 1KB
+
+#define MMU_L1_TYPE_SECTION 0x2//map 1MB directly instead of using a page table (not used)
+#define MMU_L1_TYPE_FINEEPAGETABLE 0x3//map 1kb pages (not used and not supported on newer ARMs)
+
+
+
+
+/*
+L2-Page descriptors... now things get really complicated...
+there are three different types of pages large pages (64KB) and small(4KB) and &quot;small extended&quot;
+only small extende is used by now....
+and there is a new and a old format of page table entries I will use the old format...
+*/
+
+#define MMU_L2_TYPE_SMALLEXT 0x3
+//for B C and TEX see ARM arm B4-11
+#define MMU_L2_FLAG_B 0x4
+#define MMU_L2_FLAG_C 0x8
+#define MMU_L2_FLAG_TEX 0 //use 0b000 as TEX
+#define MMU_L2_FLAG_AP_RW 0x30 // allow read and write for user and system
+//#define MMU_L2_FLAG_AP_
+
+
+#define MMU_L1_TABLE_SIZE (4096 * 4) //4096 entries (one entry per MB) -&gt; 16KB
+#define MMU_L2_COARSE_TABLE_SIZE (256 * 4)//256 entries (one entry per 4KB) -&gt; 1KB
+
+
+
+
+
+
+
+//definitions for CP15 r1
+#define CR_R1_MMU 0x1 //enable MMU
+#define CP_R1_XP 0x800000 //if XP=0 then use backwards comaptible translation tables
+
+
+
+
+
+/*
+*defines a block in memory
+*/
+struct memblock {
+        const char      name[16]; // the name will be used for debugging etc later perhapse...
+        addr_t    start; // start of the block
+        addr_t    end; // end of the block
+	uint32	  flags; // which flags should be applied (device/normal etc..)
 };
 
 
-static const uint32 kDefaultPageTableFlags = 0x07;	// present, user, R/W
+//Memory map of the whole memory
+static struct memblock MEMORYMAP[] = {
+        {
+                &quot;ROM\0&quot;, 0x00000000, 0x12345467, MMU_L2_FLAG_AP_RW|MMU_L2_FLAG_C,
+        },
+        {
+                &quot;devices\0&quot;,
+		0x48000000,
+		0x48FFFFFF,
+		MMU_L2_FLAG_AP_RW|MMU_L2_FLAG_B,
+        },
+        {
+                &quot;RAM\0&quot;,
+		0x80000000,
+		0x9FFFFFFF,
+		MMU_L2_FLAG_AP_RW|MMU_L2_FLAG_C,
+        },
+};
+
+
+//memory used by the loader that should be identity mapped
+/*
+static struct memblock LOADER_MEMORYMAP[] = {
+        {
+                &quot;vectors\0&quot;,//interrupt vectors
+		0x00000000,
+		0x00000fff,
+		MMU_L2_FLAG_AP_RW|MMU_L2_FLAG_B,
+        },
+        {
+                &quot;devices\0&quot;,
+		0x48000000,
+		0x48FFFFFF,
+		MMU_L2_FLAG_AP_RW|MMU_L2_FLAG_B,
+        },
+        {
+                &quot;RAM_image\0&quot;,//15MB for the initrd should be enough..
+		0x80000000,
+		0x80ffffff,
+		MMU_L2_FLAG_AP_RW|MMU_L2_FLAG_C,
+        },
+        {
+                &quot;RAM_loader\0&quot;,//1MB loader
+		0x81000000,
+		0x810fffff,
+		MMU_L2_FLAG_AP_RW|MMU_L2_FLAG_C,
+        },
+        {
+                &quot;RAM_pt\0&quot;,//Page Table 1MB
+		0x81100000,
+		0x811FFFFF,
+		MMU_L2_FLAG_AP_RW|MMU_L2_FLAG_C,
+        },
+        {
+                &quot;RAM_free\0&quot;,//14MB free RAM (actually more but we don't identity map it automaticaly)
+		0x81200000,
+		0x82000000,
+		MMU_L2_FLAG_AP_RW|MMU_L2_FLAG_C,
+        },
+
+};
+*/
+static struct memblock LOADER_MEMORYMAP[] = {
+        {
+                &quot;vectors\0&quot;,//interrupt vectors
+		0x00000000,
+		0x00000fff,
+		MMU_L2_FLAG_B,
+        },
+        {
+                &quot;devices\0&quot;,
+		0x40000000,
+		0x40FFFFFF,
+		MMU_L2_FLAG_B,
+        },
+        {
+                &quot;RAM_image\0&quot;,//15MB for the initrd should be enough..
+		0xA0000000,
+		0xA1ffffff,
+		MMU_L2_FLAG_C,
+        },
+        {
+                &quot;RAM_loader\0&quot;,//1MB loader
+		0xA2000000,
+		0xA20fffff,
+		MMU_L2_FLAG_C,
+        },
+        {
+                &quot;RAM_pt\0&quot;,//Page Table 1MB
+		0xA2100000,
+		0xA21FFFFF,
+		MMU_L2_FLAG_C,
+        },
+        {
+                &quot;RAM_free\0&quot;,//16MB free RAM (actually more but we don't identity map it automaticaly)
+		0xA2200000,
+		0xA31FFFFF,
+		MMU_L2_FLAG_C,
+        },
+        {
+                &quot;RAM_stack\0&quot;,//stack
+		0xA3200000,
+		0xA4000000,
+		MMU_L2_FLAG_C,
+        },
+
+};
+
+
+
+
+
+
+
+
+
+
+//static const uint32 kDefaultPageTableFlags = MMU_FLAG_READWRITE;	// not cached not buffered, R/W
 static const size_t kMaxKernelSize = 0x200000;		// 2 MB for the kernel
 
-// working page directory and page table
-static uint32 *sPageDirectory = 0;
 
 
-static addr_t sNextPhysicalAddress = 0xa4000000; //ARM:TODO usefull value...
+static addr_t sNextPhysicalAddress = 0; //will be set by mmu_init
 static addr_t sNextVirtualAddress = KERNEL_BASE + kMaxKernelSize;
-static addr_t sMaxVirtualAddress = KERNEL_BASE + 0x400000;
+static addr_t sMaxVirtualAddress = KERNEL_BASE + kMaxKernelSize;
 
-static addr_t sNextPageTableAddress = 0x90000;
-static const uint32 kPageTableRegionEnd = 0x9e000;
-	// we need to reserve 2 pages for the SMP trampoline code
+static addr_t sNextPageTableAddress = 0;
+//the page directory is in front of the pagetable
+static uint32 kPageTableRegionEnd = 0;
 
 
+// working page directory and page table
+static uint32 *sPageDirectory = 0 ;
+//page directory has to be on a multiple of 16MB for
+//some arm processors
+
+
+
+
+
+
+
+
+
+
 static addr_t
 get_next_virtual_address(size_t size)
 {
@@ -95,7 +286,21 @@
 	return address;
 }
 
+static addr_t
+get_next_virtual_address_alligned (size_t size, uint32 mask)
+{
+	TRACE((&quot;MUH&quot;));
+	TRACE((&quot;\n\n NEXT VIRRUADRSSS: %lx\n&quot;, sNextVirtualAddress));
 
+	addr_t address = (sNextVirtualAddress) &amp; mask ;
+	TRACE((&quot;ADDR: %lx\n&quot;, address));
+
+	sNextVirtualAddress = address + size;
+
+	return address;
+}
+
+
 static addr_t
 get_next_physical_address(size_t size)
 {
@@ -105,80 +310,192 @@
 	return address;
 }
 
+static addr_t
+get_next_physical_address_alligned(size_t size, uint32 mask)
+{
+	addr_t address = sNextPhysicalAddress &amp; mask;
+	sNextPhysicalAddress = address + size;
 
+	return address;
+}
+
+
 static addr_t
-get_next_virtual_page()
+get_next_virtual_page(size_t pagesize)
 {
-	return get_next_virtual_address(B_PAGE_SIZE);
+	return get_next_virtual_address_alligned( pagesize, 0xffffffc0 );
 }
 
 
 static addr_t
-get_next_physical_page()
+get_next_physical_page(size_t pagesize)
 {
-	return get_next_physical_address(B_PAGE_SIZE);
+	return get_next_physical_address_alligned( pagesize, 0xffffffc0 );
 }
 
 
-static uint32 *
-get_next_page_table()
+
+void
+mmu_set_TTBR(uint32  ttb)
 {
-	TRACE((&quot;get_next_page_table, sNextPageTableAddress %p, kPageTableRegionEnd &quot;
-		&quot;%p\n&quot;, sNextPageTableAddress, kPageTableRegionEnd));
+	ttb &amp;=  0xffffc000;
+	asm volatile( &quot;MCR p15, 0, %[adr], c2, c0, 0&quot;::[adr] &quot;r&quot; (ttb)  ); /* set translation table base */
+}
 
-	addr_t address = sNextPageTableAddress;
-	if (address &gt;= kPageTableRegionEnd)
-		return (uint32 *)get_next_physical_page();
 
-	sNextPageTableAddress += B_PAGE_SIZE;
-	return (uint32 *)address;
+void
+mmu_flush_TLB()
+{
+	uint32 bla = 0;
+	asm volatile(&quot;MCR p15, 0, %[c8format], c8, c7, 0&quot;::[c8format] &quot;r&quot; (bla) ); /* flush TLB */
 }
 
+uint32
+mmu_read_C1()
+{
+	uint32 bla = 0;
+	asm volatile(&quot;MRC p15, 0, %[c1out], c1, c0, 0&quot;:[c1out] &quot;=r&quot; (bla));
+	return bla;
+}
 
-/*!	Adds a new page table for the specified base address */
-static void
-add_page_table(addr_t base)
+void
+mmu_write_C1(uint32 value)
 {
-/*	TRACE((&quot;add_page_table(base = %p)\n&quot;, (void *)base));
+	asm volatile(&quot;MCR p15, 0, %[c1in], c1, c0, 0&quot;::[c1in] &quot;r&quot; (value));
+}
 
-	// Get new page table and clear it out
-	uint32 *pageTable = get_next_page_table();
-	if (pageTable &gt; (uint32 *)(8 * 1024 * 1024)) {
-		panic(&quot;tried to add page table beyond the indentity mapped 8 MB &quot;
-			&quot;region\n&quot;);
+void
+mmu_write_DACR(uint32 value)
+{
+	asm volatile(&quot;MCR p15, 0, %[c1in], c3, c0, 0&quot;::[c1in] &quot;r&quot; (value));
+}
+
+
+static uint32 *
+get_next_page_table(uint32 type)
+{
+        TRACE((&quot;get_next_page_table, sNextPageTableAddress %p, kPageTableRegionEnd &quot;
+                &quot;%p, type  0x%lx\n&quot;, sNextPageTableAddress, kPageTableRegionEnd, type));
+	size_t size = 0;
+	switch(type){
+		case MMU_L1_TYPE_COARSEPAGETABLE:
+			size = 1024;
+		break;
+		case MMU_L1_TYPE_FINEEPAGETABLE:
+			size = 4096;
+		break;
 	}
+        addr_t address = sNextPageTableAddress;
+        if (address &gt;= kPageTableRegionEnd){
+		TRACE((&quot;outside of pagetableregion!&quot;));
+                return (uint32 *)get_next_physical_address_alligned(size,0xffffffc0);
+	}
+        sNextPageTableAddress += size;
+        return (uint32 *)address;
+}
 
-	gKernelArgs.arch_args.pgtables[gKernelArgs.arch_args.num_pgtables++]
-		= (uint32)pageTable;
 
-	for (int32 i = 0; i &lt; 1024; i++)
-		pageTable[i] = 0;
+void
+init_page_directory()
+{
+        TRACE((&quot;init_page_directory\n&quot;));
 
-	// put the new page table into the page directory
-	sPageDirectory[base / (4 * 1024 * 1024)]
-		= (uint32)pageTable | kDefaultPageTableFlags;*/
+        gKernelArgs.arch_args.phys_pgdir = (uint32)sPageDirectory;
+        TRACE((&quot;init_page_directory2\n&quot;));
+
+        // clear out the pgdir
+        for (uint32 i = 0; i &lt; 4096; i++) {
+               sPageDirectory[i] = 0;
+        }
+
+	 uint32 *pageTable = NULL;
+	for (uint32 i=0; i &lt; ARRAY_SIZE(LOADER_MEMORYMAP);i++){
+
+		pageTable = get_next_page_table(MMU_L1_TYPE_COARSEPAGETABLE);
+		TRACE((&quot;BLOCK: %s START: %lx END %lx&quot;,LOADER_MEMORYMAP[i].name,LOADER_MEMORYMAP[i].start,LOADER_MEMORYMAP[i].end));
+		addr_t pos = LOADER_MEMORYMAP[i].start;
+		int c=0;
+		while(pos&lt; LOADER_MEMORYMAP[i].end){
+			pageTable[c]=pos |  LOADER_MEMORYMAP[i].flags | MMU_L2_TYPE_SMALLEXT;
+//			TRACE((&quot;PAGE TABLE: %lx = [%lx] \n&quot;,c, pageTable[c]));				
+			c++;
+			if(c&gt;255){ //we filled a pagetable =&gt; we need a new one
+				//there is 1MB per pagetable so:
+				sPageDirectory[ pos / (1024*1024) ]= (uint32)pageTable | MMU_L1_TYPE_COARSEPAGETABLE;
+//				TRACE((&quot;DIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIR\n&quot;));
+//				for(uint32 j=0; j&lt;256;j++){
+//					TRACE((&quot;%lx &quot;,pageTable[j]));
+//				}
+//				TRACE((&quot;\nPAGEDIR: [%lx] = %lx \n&quot;,pos / (1024*1024), sPageDirectory[ pos / (1024*1024)  ] ));				
+//				TRACE((&quot;DIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIR\n&quot;));
+				pageTable = get_next_page_table(MMU_L1_TYPE_COARSEPAGETABLE);
+				c=0;
+			}
+			pos += B_PAGE_SIZE;
+		}
+		if(c&gt;0){
+			sPageDirectory[ pos / (1024*1024)  ]= (uint32)pageTable | MMU_L1_TYPE_COARSEPAGETABLE;
+//			TRACE((&quot;DIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIR\n&quot;));
+//				for(uint32 j=0; j&lt;256; j++){
+//					TRACE((&quot;%lx &quot;,pageTable[j]));
+//				}
+//			TRACE((&quot;\nPAGEDIR: %lx = [%lx] \n&quot;,pos / (1024*1024), sPageDirectory[ pos / (1024*1024)  ] ));				
+//			TRACE((&quot;DIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIRDIR\n&quot;));
+
+		}
+	}
+
+
+
+	mmu_flush_TLB();
+
+        /* set up the translation table base */
+        mmu_set_TTBR((uint32)sPageDirectory);
+
+	mmu_flush_TLB();
+
+        /* set up the domain access register */
+        mmu_write_DACR(0xFFFFFFFF);
+
+	dprintf(&quot;hallo1&quot;);
+
+
+        /* turn on the mmu */
+        mmu_write_C1(mmu_read_C1() | 0x1);
+
+
+
+	dprintf(&quot;hallo&quot;);
 }
 
-
+/*!     Adds a new page table for the specified base address */
 static void
-unmap_page(addr_t virtualAddress)
+add_page_table(addr_t base)
 {
-/*	TRACE((&quot;unmap_page(virtualAddress = %p)\n&quot;, (void *)virtualAddress));
+        TRACE((&quot;add_page_table(base = %p)\n&quot;, (void *)base));
 
-	if (virtualAddress &lt; KERNEL_BASE) {
-		panic(&quot;unmap_page: asked to unmap invalid page %p!\n&quot;,
-			(void *)virtualAddress);
-	}
+        // Get new page table and clear it out
+        uint32 *pageTable = get_next_page_table(MMU_L1_TYPE_COARSEPAGETABLE);
+/*        if (pageTable &gt; (uint32 *)(8 * 1024 * 1024)) {
+                panic(&quot;tried to add page table beyond the indentity mapped 8 MB &quot;
+                        &quot;region\n&quot;);
+        }
+*/
+        gKernelArgs.arch_args.pgtables[gKernelArgs.arch_args.num_pgtables++]
+                = (uint32)pageTable;
 
-	// unmap the page from the correct page table
-	uint32 *pageTable = (uint32 *)(sPageDirectory[virtualAddress
-		/ (B_PAGE_SIZE * 1024)] &amp; 0xfffff000);
-	pageTable[(virtualAddress % (B_PAGE_SIZE * 1024)) / B_PAGE_SIZE] = 0;
+        for (int32 i = 0; i &lt; 256; i++)
+                pageTable[i] = 0;
 
-	asm volatile(&quot;invlpg (%0)&quot; : : &quot;r&quot; (virtualAddress));*/
+        // put the new page table into the page directory
+        sPageDirectory[base / (1024 * 1024)]
+                = (uint32)pageTable | MMU_L1_TYPE_COARSEPAGETABLE;
 }
 
 
+
+
+
 /*!	Creates an entry to map the specified virtualAddress to the given
 	physicalAddress.
 	If the mapping goes beyond the current page table, it will allocate
@@ -187,7 +504,7 @@
 static void
 map_page(addr_t virtualAddress, addr_t physicalAddress, uint32 flags)
 {
-/*	TRACE((&quot;map_page: vaddr 0x%lx, paddr 0x%lx\n&quot;, virtualAddress, physicalAddress));
+	TRACE((&quot;map_page: vaddr 0x%lx, paddr 0x%lx\n&quot;, virtualAddress, physicalAddress));
 
 	if (virtualAddress &lt; KERNEL_BASE) {
 		panic(&quot;map_page: asked to map invalid page %p!\n&quot;,
@@ -198,7 +515,7 @@
 		// we need to add a new page table
 
 		add_page_table(sMaxVirtualAddress);
-		sMaxVirtualAddress += B_PAGE_SIZE * 1024;
+		sMaxVirtualAddress += B_PAGE_SIZE * 256;
 
 		if (virtualAddress &gt;= sMaxVirtualAddress) {
 			panic(&quot;map_page: asked to map a page to %p\n&quot;,
@@ -210,159 +527,78 @@
 
 	// map the page to the correct page table
 	uint32 *pageTable = (uint32 *)(sPageDirectory[virtualAddress
-		/ (B_PAGE_SIZE * 1024)] &amp; 0xfffff000);
-	uint32 tableEntry = (virtualAddress % (B_PAGE_SIZE * 1024)) / B_PAGE_SIZE;
+		/ (1024 * 1024)] &amp; 0xfffffc00);
+	TRACE((&quot;map_page: pageTable 0x%lx\n&quot;, (sPageDirectory[virtualAddress
+                / (1024 * 1024)] &amp; 0xfffffc00) ));
+	if(pageTable == NULL){
+		TRACE((&quot;panic&quot;));
+		panic(&quot;map:page no page_directory entry!!!&quot;);
+/*		add_page_table(virtualAddress);
+		pageTable = (uint32 *)(sPageDirectory[virtualAddress
+                / (1024 * 1024)] &amp; 0xfffffc00);
+*/
+	}
 
+	uint32 tableEntry = (virtualAddress % (B_PAGE_SIZE * 256)) / B_PAGE_SIZE;
+
+
 	TRACE((&quot;map_page: inserting pageTable %p, tableEntry %ld, physicalAddress &quot;
 		&quot;%p\n&quot;, pageTable, tableEntry, physicalAddress));
 
 	pageTable[tableEntry] = physicalAddress | flags;
 
-	asm volatile(&quot;invlpg (%0)&quot; : : &quot;r&quot; (virtualAddress));
 
-	TRACE((&quot;map_page: done\n&quot;));*/
-}
 
 
-static void
-sort_addr_range(addr_range *range, int count)
-{
-	addr_range tempRange;
-	bool done;
-	int i;
+	mmu_flush_TLB();
 
-	do {
-		done = true;
-		for (i = 1; i &lt; count; i++) {
-			if (range[i].start &lt; range[i - 1].start) {
-				done = false;
-				memcpy(&amp;tempRange, &amp;range[i], sizeof(addr_range));
-				memcpy(&amp;range[i], &amp;range[i - 1], sizeof(addr_range));
-				memcpy(&amp;range[i - 1], &amp;tempRange, sizeof(addr_range));
-			}
-		}
-	} while (!done);
+	TRACE((&quot;map_page: done\n&quot;));
 }
 
 
 
-#ifdef TRACE_MEMORY_MAP
-static const char *
-e820_memory_type(uint32 type)
-{
-	switch (type) {
-		case 1: return &quot;memory&quot;;
-		case 2: return &quot;reserved&quot;;
-		case 3: return &quot;ACPI reclaim&quot;;
-		case 4: return &quot;ACPI NVS&quot;;
-		default: return &quot;unknown/reserved&quot;;
-	}
-}
-#endif
 
+//	#pragma mark -
 
-static uint32
-get_memory_map(extended_memory **_extendedMemory)
+
+extern &quot;C&quot; addr_t
+mmu_map_physical_memory(addr_t physicalAddress, size_t size, uint32 flags)
 {
-/*	extended_memory *block = (extended_memory *)kExtraSegmentScratch;
-	bios_regs regs = {0, 0, sizeof(extended_memory), 0, 0, (uint32)block, 0, 0};
-	uint32 count = 0;
+	addr_t address = sNextVirtualAddress;
+	addr_t pageOffset = physicalAddress &amp; (B_PAGE_SIZE - 1);
 
-	TRACE((&quot;get_memory_map()\n&quot;));
+	physicalAddress -= pageOffset;
 
-	do {
-		regs.eax = 0xe820;
-		regs.edx = 'SMAP';
-
-		call_bios(0x15, &amp;regs);
-		if (regs.flags &amp; CARRY_FLAG)
-			return 0;
-
-		regs.edi += sizeof(extended_memory);
-		count++;
-	} while (regs.ebx != 0);
-
-	*_extendedMemory = block;
-
-#ifdef TRACE_MEMORY_MAP
-	dprintf(&quot;extended memory info (from 0xe820):\n&quot;);
-	for (uint32 i = 0; i &lt; count; i++) {
-		dprintf(&quot;    base 0x%08Lx, len 0x%08Lx, type %lu (%s)\n&quot;,
-			block[i].base_addr, block[i].length,
-			block[i].type, e820_memory_type(block[i].type));
+	for (addr_t offset = 0; offset &lt; size; offset += B_PAGE_SIZE) {
+		map_page(get_next_virtual_page(B_PAGE_SIZE), physicalAddress + offset, flags);
 	}
-#endif
 
-	return count;*/return 0;
+	return address + pageOffset;
 }
 
 
+
 static void
-init_page_directory(void)
+unmap_page(addr_t virtualAddress)
 {
-	TRACE((&quot;init_page_directory\n&quot;));
-/*
-	// allocate a new pgdir
-	sPageDirectory = (uint32 *)get_next_physical_page();
-	gKernelArgs.arch_args.phys_pgdir = (uint32)sPageDirectory;
+        TRACE((&quot;unmap_page(virtualAddress = %p)\n&quot;, (void *)virtualAddress));
 
-	// clear out the pgdir
-	for (int32 i = 0; i &lt; 1024; i++) {
-		sPageDirectory[i] = 0;
-	}
+        if (virtualAddress &lt; KERNEL_BASE) {
+                panic(&quot;unmap_page: asked to unmap invalid page %p!\n&quot;,
+                        (void *)virtualAddress);
+        }
 
-	// Identity map the first 8 MB of memory so that their
-	// physical and virtual address are the same.
-	// These page tables won't be taken over into the kernel.
+        // unmap the page from the correct page table
+        uint32 *pageTable = (uint32 *)(sPageDirectory[virtualAddress
+                / (B_PAGE_SIZE * 1024)] &amp; 0xfffff000);
+        pageTable[(virtualAddress % (B_PAGE_SIZE * 1024)) / B_PAGE_SIZE] = 0;
 
-	// make the first page table at the first free spot
-	uint32 *pageTable = get_next_page_table();
+	mmu_flush_TLB();
 
-	for (int32 i = 0; i &lt; 1024; i++) {
-		pageTable[i] = (i * 0x1000) | kDefaultPageFlags;
-	}
-
-	sPageDirectory[0] = (uint32)pageTable | kDefaultPageFlags;
-
-	// make the second page table
-	pageTable = get_next_page_table();
-
-	for (int32 i = 0; i &lt; 1024; i++) {
-		pageTable[i] = (i * 0x1000 + 0x400000) | kDefaultPageFlags;
-	}
-
-	sPageDirectory[1] = (uint32)pageTable | kDefaultPageFlags;
-
-	gKernelArgs.arch_args.num_pgtables = 0;
-	add_page_table(KERNEL_BASE);
-
-	// switch to the new pgdir and enable paging
-	asm(&quot;movl %0, %%eax;&quot;
-		&quot;movl %%eax, %%cr3;&quot; : : &quot;m&quot; (sPageDirectory) : &quot;eax&quot;);
-	// Important.  Make sure supervisor threads can fault on read only pages...
-	asm(&quot;movl %%eax, %%cr0&quot; : : &quot;a&quot; ((1 &lt;&lt; 31) | (1 &lt;&lt; 16) | (1 &lt;&lt; 5) | 1));*/
 }
 
 
-//	#pragma mark -
 
-
-extern &quot;C&quot; addr_t
-mmu_map_physical_memory(addr_t physicalAddress, size_t size, uint32 flags)
-{
-	addr_t address = sNextVirtualAddress;
-	addr_t pageOffset = physicalAddress &amp; (B_PAGE_SIZE - 1);
-
-	physicalAddress -= pageOffset;
-
-	for (addr_t offset = 0; offset &lt; size; offset += B_PAGE_SIZE) {
-		map_page(get_next_virtual_page(), physicalAddress + offset, flags);
-	}
-
-	return address + pageOffset;
-}
-
-
 extern &quot;C&quot; void *
 mmu_allocate(void *virtualAddress, size_t size)
 {
@@ -386,7 +622,7 @@
 			return NULL;
 
 		for (uint32 i = 0; i &lt; size; i++) {
-			map_page(address, get_next_physical_page(), kDefaultPageFlags);
+			map_page(address, get_next_physical_page(B_PAGE_SIZE), kDefaultPageFlags);
 			address += B_PAGE_SIZE;
 		}
 
@@ -396,7 +632,7 @@
 	void *address = (void *)sNextVirtualAddress;
 
 	for (uint32 i = 0; i &lt; size; i++) {
-		map_page(get_next_virtual_page(), get_next_physical_page(),
+		map_page(get_next_virtual_page(B_PAGE_SIZE), get_next_physical_page(B_PAGE_SIZE),
 			kDefaultPageFlags);
 	}
 
@@ -445,86 +681,7 @@
 mmu_init_for_kernel(void)
 {
 /*	TRACE((&quot;mmu_init_for_kernel\n&quot;));
-	// set up a new idt
-	{
-		struct gdt_idt_descr idtDescriptor;
-		uint32 *idt;
 
-		// find a new idt
-		idt = (uint32 *)get_next_physical_page();
-		gKernelArgs.arch_args.phys_idt = (uint32)idt;
-
-		TRACE((&quot;idt at %p\n&quot;, idt));
-
-		// map the idt into virtual space
-		gKernelArgs.arch_args.vir_idt = (uint32)get_next_virtual_page();
-		map_page(gKernelArgs.arch_args.vir_idt, (uint32)idt, kDefaultPageFlags);
-
-		// clear it out
-		uint32* virtualIDT = (uint32*)gKernelArgs.arch_args.vir_idt;
-		for (int32 i = 0; i &lt; IDT_LIMIT / 4; i++) {
-			virtualIDT[i] = 0;
-		}
-
-		// load the idt
-		idtDescriptor.limit = IDT_LIMIT - 1;
-		idtDescriptor.base = (uint32 *)gKernelArgs.arch_args.vir_idt;
-
-		asm(&quot;lidt	%0;&quot;
-			: : &quot;m&quot; (idtDescriptor));
-
-		TRACE((&quot;idt at virtual address 0x%lx\n&quot;, gKernelArgs.arch_args.vir_idt));
-	}
-
-	// set up a new gdt
-	{
-		struct gdt_idt_descr gdtDescriptor;
-		segment_descriptor *gdt;
-
-		// find a new gdt
-		gdt = (segment_descriptor *)get_next_physical_page();
-		gKernelArgs.arch_args.phys_gdt = (uint32)gdt;
-
-		TRACE((&quot;gdt at %p\n&quot;, gdt));
-
-		// map the gdt into virtual space
-		gKernelArgs.arch_args.vir_gdt = (uint32)get_next_virtual_page();
-		map_page(gKernelArgs.arch_args.vir_gdt, (uint32)gdt, kDefaultPageFlags);
-
-		// put standard segment descriptors in it
-		segment_descriptor* virtualGDT
-			= (segment_descriptor*)gKernelArgs.arch_args.vir_gdt;
-		clear_segment_descriptor(&amp;virtualGDT[0]);
-
-		// seg 0x08 - kernel 4GB code
-		set_segment_descriptor(&amp;virtualGDT[1], 0, 0xffffffff, DT_CODE_READABLE,
-			DPL_KERNEL);
-
-		// seg 0x10 - kernel 4GB data
-		set_segment_descriptor(&amp;virtualGDT[2], 0, 0xffffffff, DT_DATA_WRITEABLE,
-			DPL_KERNEL);
-
-		// seg 0x1b - ring 3 user 4GB code
-		set_segment_descriptor(&amp;virtualGDT[3], 0, 0xffffffff, DT_CODE_READABLE,
-			DPL_USER);
-
-		// seg 0x23 - ring 3 user 4GB data
-		set_segment_descriptor(&amp;virtualGDT[4], 0, 0xffffffff, DT_DATA_WRITEABLE,
-			DPL_USER);
-
-		// virtualGDT[5] and above will be filled later by the kernel
-		// to contain the TSS descriptors, and for TLS (one for every CPU)
-
-		// load the GDT
-		gdtDescriptor.limit = GDT_LIMIT - 1;
-		gdtDescriptor.base = (uint32 *)gKernelArgs.arch_args.vir_gdt;
-
-		asm(&quot;lgdt	%0;&quot;
-			: : &quot;m&quot; (gdtDescriptor));
-
-		TRACE((&quot;gdt at virtual address %p\n&quot;, (void *)gKernelArgs.arch_args.vir_gdt));
-	}
-
 	// save the memory we've physically allocated
 	gKernelArgs.physical_allocated_range[0].size
 		= sNextPhysicalAddress - gKernelArgs.physical_allocated_range[0].start;
@@ -543,50 +700,49 @@
 		gKernelArgs.num_physical_allocated_ranges);
 	sort_addr_range(gKernelArgs.virtual_allocated_range,
 		gKernelArgs.num_virtual_allocated_ranges);
+*/
+}
 
-#ifdef TRACE_MEMORY_MAP
-	{
-		uint32 i;
 
-		dprintf(&quot;phys memory ranges:\n&quot;);
-		for (i = 0; i &lt; gKernelArgs.num_physical_memory_ranges; i++) {
-			dprintf(&quot;    base 0x%08lx, length 0x%08lx\n&quot;, gKernelArgs.physical_memory_range[i].start, gKernelArgs.physical_memory_range[i].size);
+extern &quot;C&quot; void
+mmu_init(void)
+{
+	TRACE((&quot;mmu_init\n&quot;));
+	mmu_write_C1(mmu_read_C1() &amp; ~((1&lt;&lt;29)|(1&lt;&lt;28)|(1&lt;&lt;0)));// access flag disabled, TEX remap disabled, mmu disabled
+	//calculate lowest RAM adress from MEMORYMAP
+	for(int i=0;i&lt;ARRAY_SIZE(LOADER_MEMORYMAP);i++){
+		if(strcmp(&quot;RAM_free&quot;,LOADER_MEMORYMAP[i].name)){
+			sNextPhysicalAddress=LOADER_MEMORYMAP[i].start;
 		}
+		if(strcmp(&quot;RAM_pt&quot;,LOADER_MEMORYMAP[i].name)){
+			sNextPageTableAddress=LOADER_MEMORYMAP[i].start + MMU_L1_TABLE_SIZE;
+			kPageTableRegionEnd = LOADER_MEMORYMAP[i].end;
+			sPageDirectory = (uint32 *) LOADER_MEMORYMAP[i].start ;
 
-		dprintf(&quot;allocated phys memory ranges:\n&quot;);
-		for (i = 0; i &lt; gKernelArgs.num_physical_allocated_ranges; i++) {
-			dprintf(&quot;    base 0x%08lx, length 0x%08lx\n&quot;, gKernelArgs.physical_allocated_range[i].start, gKernelArgs.physical_allocated_range[i].size);
 		}
-
-		dprintf(&quot;allocated virt memory ranges:\n&quot;);
-		for (i = 0; i &lt; gKernelArgs.num_virtual_allocated_ranges; i++) {
-			dprintf(&quot;    base 0x%08lx, length 0x%08lx\n&quot;, gKernelArgs.virtual_allocated_range[i].start, gKernelArgs.virtual_allocated_range[i].size);
-		}
 	}
-#endif*/
-}
 
 
-extern &quot;C&quot; void
-mmu_init(void)
-{
-/*	TRACE((&quot;mmu_init\n&quot;));
-
 	gKernelArgs.physical_allocated_range[0].start = sNextPhysicalAddress;
 	gKernelArgs.physical_allocated_range[0].size = 0;
 	gKernelArgs.num_physical_allocated_ranges = 1;
 		// remember the start of the allocated physical pages
 
+
+
+
 	init_page_directory();
 
 	// Map the page directory into kernel space at 0xffc00000-0xffffffff
 	// this enables a mmu trick where the 4 MB region that this pgdir entry
 	// represents now maps the 4MB of potential pagetables that the pgdir
 	// points to. Thrown away later in VM bringup, but useful for now.
+#warning ARM:TODO!
+/*
 	sPageDirectory[1023] = (uint32)sPageDirectory | kDefaultPageFlags;
-
+*/
 	// also map it on the next vpage
-	gKernelArgs.arch_args.vir_pgdir = get_next_virtual_page();
+	gKernelArgs.arch_args.vir_pgdir = get_next_virtual_page(B_PAGE_SIZE);
 	map_page(gKernelArgs.arch_args.vir_pgdir, (uint32)sPageDirectory,
 		kDefaultPageFlags);
 
@@ -599,83 +755,6 @@
 	TRACE((&quot;kernel stack at 0x%lx to 0x%lx\n&quot;, gKernelArgs.cpu_kstack[0].start,
 		gKernelArgs.cpu_kstack[0].start + gKernelArgs.cpu_kstack[0].size));
 
-	extended_memory *extMemoryBlock;
-	uint32 extMemoryCount = get_memory_map(&amp;extMemoryBlock);
-
-	// figure out the memory map
-	if (extMemoryCount &gt; 0) {
-		gKernelArgs.num_physical_memory_ranges = 0;
-
-		for (uint32 i = 0; i &lt; extMemoryCount; i++) {
-			// Type 1 is available memory
-			if (extMemoryBlock[i].type == 1) {
-				// round everything up to page boundaries, exclusive of pages
-				// it partially occupies
-				if ((extMemoryBlock[i].base_addr % B_PAGE_SIZE) != 0) {
-					extMemoryBlock[i].length -= B_PAGE_SIZE
-						- extMemoryBlock[i].base_addr % B_PAGE_SIZE;
-				}
-				extMemoryBlock[i].base_addr
-					= ROUNDUP(extMemoryBlock[i].base_addr, B_PAGE_SIZE);
-				extMemoryBlock[i].length
-					= ROUNDDOWN(extMemoryBlock[i].length, B_PAGE_SIZE);
-
-				// we ignore all memory beyond 4 GB
-				if (extMemoryBlock[i].base_addr &gt; 0xffffffffULL)
-					continue;
-
-				if (extMemoryBlock[i].base_addr + extMemoryBlock[i].length
-						&gt; 0xffffffffULL) {
-					extMemoryBlock[i].length
-						= 0x100000000ULL - extMemoryBlock[i].base_addr;
-				}
-
-				if (gKernelArgs.num_physical_memory_ranges &gt; 0) {
-					// we might want to extend a previous hole
-					addr_t previousEnd = gKernelArgs.physical_memory_range[
-							gKernelArgs.num_physical_memory_ranges - 1].start
-						+ gKernelArgs.physical_memory_range[
-							gKernelArgs.num_physical_memory_ranges - 1].size;
-					addr_t holeSize = extMemoryBlock[i].base_addr - previousEnd;
-
-					// If the hole is smaller than 1 MB, we try to mark the

[... truncated: 41 lines follow ...]

</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="019157.html">[Haiku-commits] r32329 - haiku/trunk/headers/private/kernel/arch/arm
</A></li>
	<LI>Next message: <A HREF="019159.html">[Haiku-commits] r32331 - haiku/trunk/src/add-ons/tracker/zipomatic
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#19158">[ date ]</a>
              <a href="thread.html#19158">[ thread ]</a>
              <a href="subject.html#19158">[ subject ]</a>
              <a href="author.html#19158">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/haiku-commits">More information about the Haiku-commits
mailing list</a><br>
</body></html>
