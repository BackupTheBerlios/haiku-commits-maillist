<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Haiku-commits] r20251 - in haiku/trunk: headers/private/kernel	src/system/kernel/vm
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/haiku-commits/2007-February/index.html" >
   <LINK REL="made" HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r20251%20-%20in%20haiku/trunk%3A%20headers/private/kernel%0A%09src/system/kernel/vm&In-Reply-To=%3C200702281324.l1SDOsi3022649%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000942.html">
   <LINK REL="Next"  HREF="000945.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Haiku-commits] r20251 - in haiku/trunk: headers/private/kernel	src/system/kernel/vm</H1>
    <B>axeld at BerliOS</B> 
    <A HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r20251%20-%20in%20haiku/trunk%3A%20headers/private/kernel%0A%09src/system/kernel/vm&In-Reply-To=%3C200702281324.l1SDOsi3022649%40sheep.berlios.de%3E"
       TITLE="[Haiku-commits] r20251 - in haiku/trunk: headers/private/kernel	src/system/kernel/vm">axeld at mail.berlios.de
       </A><BR>
    <I>Wed Feb 28 14:24:54 CET 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000942.html">[Haiku-commits] r20250 - haiku/trunk/headers/private/kernel/util
</A></li>
        <LI>Next message: <A HREF="000945.html">[Haiku-commits] r20252 - haiku/trunk/headers/private/kernel/util
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#943">[ date ]</a>
              <a href="thread.html#943">[ thread ]</a>
              <a href="subject.html#943">[ subject ]</a>
              <a href="author.html#943">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: axeld
Date: 2007-02-28 14:24:53 +0100 (Wed, 28 Feb 2007)
New Revision: 20251
ViewCVS: <A HREF="http://svn.berlios.de/viewcvs/haiku?rev=20251&amp;view=rev">http://svn.berlios.de/viewcvs/haiku?rev=20251&amp;view=rev</A>

Modified:
   haiku/trunk/headers/private/kernel/vm.h
   haiku/trunk/headers/private/kernel/vm_types.h
   haiku/trunk/src/system/kernel/vm/vm.cpp
   haiku/trunk/src/system/kernel/vm/vm_daemons.c
   haiku/trunk/src/system/kernel/vm/vm_page.c
   haiku/trunk/src/system/kernel/vm/vm_store_device.c
Log:
Another work-in-progress towards having extra structures per mapping per page:
* vm_area and vm_page now have a new field &quot;mappings&quot; where they will store lists
  of vm_page_mapping structures. vm_page::ref_count is gone, as it's no longer
  needed (it was never updated correctly, anyway).
* vm_caches now have a type field, ie. CACHE_TYPE_RAM for anonymous areas - this
  makes the stores a bit less independent, but is quite handy in several places.
* Added new vm_map_page() and vm_unmap_pages() functions to be used whenever you
  map in or unmap pages into/from an area. They don't do much more than handling
  vm_page::wired_count correctly right now, though (ie. B_LAZY_LOCK is now working
  as expected as well).
* Moved the device fault handler to vm_map_physical_memory(); it was not really
  used as a fault handler, anyway.
* Didn't notice Ingo's changes to the I/O space region broke lock_memory(). It
  now checks the type of the area that contains the memory, and doesn't lock
  anymore if not needed which solves the problem in a platform independent way.
* Implemented lock_memory() and unlock_memory() for real: they now change the
  vm_page::wired_count member to identify pages that shouldn't be paged out.
* vm_area_for() now uses vm_area_lookup() internally.
* Fixed various potential overflow conditions with areas that reach 0xffffffff.
* Creating anonymous areas with B_FULL_LOCK no longer causes vm_soft_fault()
  to be called, instead, the pages are allocated and mapped (via vm_map_page())
  directly.
* Removed the _vm_ prefix for create_area_struct() and create_reserved_area_struct().
* Fixed a bug in vm_page_write_modified() that would not have enqueued pages that
  failed to be written to the modified queue again when needed.


Modified: haiku/trunk/headers/private/kernel/vm.h
===================================================================
--- haiku/trunk/headers/private/kernel/vm.h	2007-02-28 11:31:19 UTC (rev 20250)
+++ haiku/trunk/headers/private/kernel/vm.h	2007-02-28 13:24:53 UTC (rev 20251)
@@ -60,10 +60,11 @@
 status_t vm_delete_area(team_id aid, area_id id);
 status_t vm_create_vnode_cache(void *vnode, vm_cache_ref **_cacheRef);
 vm_area *vm_area_lookup(vm_address_space *addressSpace, addr_t address);
-
 status_t vm_set_area_memory_type(area_id id, addr_t physicalBase, uint32 type);
-
 status_t vm_get_page_mapping(team_id team, addr_t vaddr, addr_t *paddr);
+status_t vm_unmap_pages(vm_area *area, addr_t base, size_t length);
+status_t vm_map_page(vm_area *area, vm_page *page, addr_t address,
+			uint32 protection);
 status_t vm_get_physical_page(addr_t paddr, addr_t *vaddr, uint32 flags);
 status_t vm_put_physical_page(addr_t vaddr);
 

Modified: haiku/trunk/headers/private/kernel/vm_types.h
===================================================================
--- haiku/trunk/headers/private/kernel/vm_types.h	2007-02-28 11:31:19 UTC (rev 20250)
+++ haiku/trunk/headers/private/kernel/vm_types.h	2007-02-28 13:24:53 UTC (rev 20251)
@@ -10,11 +10,61 @@
 
 
 #include &lt;kernel.h&gt;
-#include &lt;sys/uio.h&gt;
 #include &lt;arch/vm_types.h&gt;
 #include &lt;arch/vm_translation_map.h&gt;
+#include &lt;util/DoublyLinkedQueue.h&gt;
 
+#include &lt;sys/uio.h&gt;
 
+
+#ifdef __cplusplus
+struct vm_page_mapping;
+typedef DoublyLinkedListLink&lt;vm_page_mapping&gt; vm_page_mapping_link;
+#else
+typedef struct { void *previous; void *next; } vm_page_mapping_link;
+#endif
+
+typedef struct vm_page_mapping {
+	vm_page_mapping_link page_link;
+	vm_page_mapping_link area_link;
+	struct vm_page *page;
+	struct vm_area *area;
+} vm_page_mapping;
+
+#ifdef __cplusplus
+class DoublyLinkedPageLink {
+	public:
+		inline vm_page_mapping_link *operator()(vm_page_mapping *element) const
+		{
+			return &amp;element-&gt;page_link;
+		}
+
+		inline const vm_page_mapping_link *operator()(const vm_page_mapping *element) const
+		{
+			return &amp;element-&gt;page_link;
+		}
+};
+
+class DoublyLinkedAreaLink {
+	public:
+		inline vm_page_mapping_link *operator()(vm_page_mapping *element) const
+		{
+			return &amp;element-&gt;area_link;
+		}
+
+		inline const vm_page_mapping_link *operator()(const vm_page_mapping *element) const
+		{
+			return &amp;element-&gt;area_link;
+		}
+};
+
+typedef class DoublyLinkedQueue&lt;vm_page_mapping, DoublyLinkedPageLink&gt; vm_page_mappings;
+typedef class DoublyLinkedQueue&lt;vm_page_mapping, DoublyLinkedAreaLink&gt; vm_area_mappings;
+#else	// !__cplusplus
+typedef void *vm_page_mappings;
+typedef void *vm_area_mappings;
+#endif
+
 // vm page
 typedef struct vm_page {
 	struct vm_page		*queue_prev;
@@ -31,12 +81,15 @@
 	struct vm_page		*cache_prev;
 	struct vm_page		*cache_next;
 
-	int32				ref_count;
+	vm_page_mappings	mappings;
 
-	uint32				type : 2;
-	uint32				state : 3;
-	uint32				busy_reading : 1;
-	uint32				busy_writing : 1;
+	uint8				type : 2;
+	uint8				state : 3;
+	uint8				busy_reading : 1;
+	uint8				busy_writing : 1;
+
+	uint16				wired_count;
+	int8				usage_count;
 } vm_page;
 
 enum {
@@ -56,6 +109,13 @@
 	PAGE_STATE_UNUSED
 };
 
+enum {
+	CACHE_TYPE_RAM = 0,
+	CACHE_TYPE_VNODE,
+	CACHE_TYPE_DEVICE,
+	CACHE_TYPE_NULL
+};
+
 // vm_cache_ref
 typedef struct vm_cache_ref {
 	struct vm_cache		*cache;
@@ -82,6 +142,7 @@
 	uint32				temporary : 1;
 	uint32				scan_skip : 1;
 	uint32				busy : 1;
+	uint32				type : 5;
 } vm_cache;
 
 // vm area
@@ -97,6 +158,7 @@
 
 	struct vm_cache_ref	*cache_ref;
 	off_t				cache_offset;
+	vm_area_mappings	mappings;
 
 	struct vm_address_space *address_space;
 	struct vm_area		*address_space_next;

Modified: haiku/trunk/src/system/kernel/vm/vm.cpp
===================================================================
--- haiku/trunk/src/system/kernel/vm/vm.cpp	2007-02-28 11:31:19 UTC (rev 20250)
+++ haiku/trunk/src/system/kernel/vm/vm.cpp	2007-02-28 13:24:53 UTC (rev 20251)
@@ -117,7 +117,7 @@
 
 
 static vm_area *
-_vm_create_reserved_region_struct(vm_address_space *addressSpace, uint32 flags)
+create_reserved_area_struct(vm_address_space *addressSpace, uint32 flags)
 {
 	vm_area *reserved = (vm_area *)malloc(sizeof(vm_area));
 	if (reserved == NULL)
@@ -134,7 +134,7 @@
 
 
 static vm_area *
-_vm_create_area_struct(vm_address_space *addressSpace, const char *name,
+create_area_struct(vm_address_space *addressSpace, const char *name,
 	uint32 wiring, uint32 protection)
 {
 	vm_area *area = NULL;
@@ -234,7 +234,7 @@
 	} else {
 		// the area splits the reserved range into two separate ones
 		// we need a new reserved area to cover this space
-		vm_area *reserved = _vm_create_reserved_region_struct(addressSpace,
+		vm_area *reserved = create_reserved_area_struct(addressSpace,
 			next-&gt;protection);
 		if (reserved == NULL)
 			return B_NO_MEMORY;
@@ -258,8 +258,7 @@
 }
 
 
-/**	must be called with this address space's sem held */
-
+/*!	Must be called with this address space's sem held */
 static status_t
 find_and_insert_area_slot(vm_address_space *addressSpace, addr_t start,
 	addr_t size, addr_t end, uint32 addressSpec, vm_area *area)
@@ -268,7 +267,7 @@
 	vm_area *next;
 	bool foundSpot = false;
 
-	TRACE((&quot;find_and_insert_region_slot: address space %p, start 0x%lx, &quot;
+	TRACE((&quot;find_and_insert_area_slot: address space %p, start 0x%lx, &quot;
 		&quot;size %ld, end 0x%lx, addressSpec %ld, area %p\n&quot;, addressSpace, start,
 		size, end, addressSpec, area));
 
@@ -506,7 +505,7 @@
 		addressSpace, cacheRef, *_virtualAddress, offset, size, addressSpec,
 		wiring, protection, _area, areaName));
 
-	vm_area *area = _vm_create_area_struct(addressSpace, areaName, wiring, protection);
+	vm_area *area = create_area_struct(addressSpace, areaName, wiring, protection);
 	if (area == NULL)
 		return B_NO_MEMORY;
 
@@ -581,6 +580,7 @@
 	// attach the cache to the area
 	area-&gt;cache_ref = cacheRef;
 	area-&gt;cache_offset = offset;
+
 	// point the cache back to the area
 	vm_cache_insert_area_locked(cacheRef, area);
 	mutex_unlock(&amp;cacheRef-&gt;lock);
@@ -682,7 +682,7 @@
 	if (addressSpace == NULL)
 		return B_BAD_TEAM_ID;
 
-	area = _vm_create_reserved_region_struct(addressSpace, flags);
+	area = create_reserved_area_struct(addressSpace, flags);
 	if (area == NULL) {
 		status = B_NO_MEMORY;
 		goto err1;
@@ -808,17 +808,17 @@
 		goto err3;
 
 	cache-&gt;temporary = 1;
+	cache-&gt;type = CACHE_TYPE_RAM;
 	cache-&gt;virtual_size = size;
 
 	switch (wiring) {
-		case B_LAZY_LOCK:	// for now
+		case B_LAZY_LOCK:
 		case B_FULL_LOCK:
 		case B_CONTIGUOUS:
 		case B_ALREADY_WIRED:
 			cache-&gt;scan_skip = 1;
 			break;
 		case B_NO_LOCK:
-		//case B_LAZY_LOCK:
 			cache-&gt;scan_skip = 0;
 			break;
 	}
@@ -840,22 +840,33 @@
 
 		case B_FULL_LOCK:
 		{
-			// Pages aren't mapped at this point, but we just simulate a fault on
-			// every page, which should allocate them
-			// ToDo: at this point, it would probably be cheaper to allocate 
-			// and map the pages directly
-			addr_t va;
-			for (va = area-&gt;base; va &lt; area-&gt;base + area-&gt;size; va += B_PAGE_SIZE) {
+			// Allocate and map all pages for this area
+			mutex_lock(&amp;cacheRef-&gt;lock);
+
+			off_t offset = 0;
+			for (addr_t address = area-&gt;base; address &lt; area-&gt;base + (area-&gt;size - 1);
+					address += B_PAGE_SIZE, offset += B_PAGE_SIZE) {
 #ifdef DEBUG_KERNEL_STACKS
 #	ifdef STACK_GROWS_DOWNWARDS
-				if (isStack &amp;&amp; va &lt; area-&gt;base + KERNEL_STACK_GUARD_PAGES * B_PAGE_SIZE)
+				if (isStack &amp;&amp; address &lt; area-&gt;base + KERNEL_STACK_GUARD_PAGES
+						* B_PAGE_SIZE)
 #	else
-				if (isStack &amp;&amp; va &gt;= area-&gt;base + area-&gt;size - KERNEL_STACK_GUARD_PAGES * B_PAGE_SIZE)
+				if (isStack &amp;&amp; address &gt;= area-&gt;base + area-&gt;size
+						- KERNEL_STACK_GUARD_PAGES * B_PAGE_SIZE)
 #	endif
 					continue;
 #endif
-				vm_soft_fault(va, false, false);
+				vm_page *page = vm_page_allocate_page(PAGE_STATE_CLEAR);
+				if (page == NULL) {
+					// this shouldn't really happen, as we reserve the memory upfront
+					panic(&quot;couldn't fulfill B_FULL lock!&quot;);
+				}
+
+				vm_cache_insert_page(cacheRef, page, offset);
+				vm_map_page(area, page, address, protection);
 			}
+
+			mutex_unlock(&amp;cacheRef-&gt;lock);
 			break;
 		}
 
@@ -865,33 +876,38 @@
 			// boot time. Find the appropriate vm_page objects and stick them in
 			// the cache object.
 			vm_translation_map *map = &amp;addressSpace-&gt;translation_map;
-			addr_t va;
-			addr_t pa;
-			uint32 flags;
-			int err;
 			off_t offset = 0;
 
 			if (!kernel_startup)
 				panic(&quot;ALREADY_WIRED flag used outside kernel startup\n&quot;);
 
 			mutex_lock(&amp;cacheRef-&gt;lock);
-			(*map-&gt;ops-&gt;lock)(map);
-			for (va = area-&gt;base; va &lt; area-&gt;base + area-&gt;size; va += B_PAGE_SIZE, offset += B_PAGE_SIZE) {
-				err = (*map-&gt;ops-&gt;query)(map, va, &amp;pa, &amp;flags);
-				if (err &lt; 0) {
-//					dprintf(&quot;vm_create_anonymous_area: error looking up mapping for va 0x%x\n&quot;, va);
-					continue;
+			map-&gt;ops-&gt;lock(map);
+
+			for (addr_t virtualAddress = area-&gt;base; virtualAddress &lt; area-&gt;base
+					+ (area-&gt;size - 1); virtualAddress += B_PAGE_SIZE,
+					offset += B_PAGE_SIZE) {
+				addr_t physicalAddress;
+				uint32 flags;
+				status = map-&gt;ops-&gt;query(map, virtualAddress,
+					&amp;physicalAddress, &amp;flags);
+				if (status &lt; B_OK) {
+					panic(&quot;looking up mapping failed for va 0x%lx\n&quot;,
+						virtualAddress);
 				}
-				page = vm_lookup_page(pa / B_PAGE_SIZE);
+				page = vm_lookup_page(physicalAddress / B_PAGE_SIZE);
 				if (page == NULL) {
-//					dprintf(&quot;vm_create_anonymous_area: error looking up vm_page structure for pa 0x%x\n&quot;, pa);
-					continue;
+					panic(&quot;looking up page failed for pa 0x%lx\n&quot;,
+						physicalAddress);
 				}
-				atomic_add(&amp;page-&gt;ref_count, 1);
+
+				page-&gt;wired_count++;
+					// TODO: needs to be atomic on all platforms!
 				vm_page_set_state(page, PAGE_STATE_WIRED);
 				vm_cache_insert_page(cacheRef, page, offset);
 			}
-			(*map-&gt;ops-&gt;unlock)(map);
+
+			map-&gt;ops-&gt;unlock(map);
 			mutex_unlock(&amp;cacheRef-&gt;lock);
 			break;
 		}
@@ -906,25 +922,27 @@
 			off_t offset = 0;
 
 			mutex_lock(&amp;cacheRef-&gt;lock);
-			(*map-&gt;ops-&gt;lock)(map);
+			map-&gt;ops-&gt;lock(map);
 
-			for (virtualAddress = area-&gt;base; virtualAddress &lt; area-&gt;base + area-&gt;size;
-					virtualAddress += B_PAGE_SIZE, offset += B_PAGE_SIZE,
-					physicalAddress += B_PAGE_SIZE) {
+			for (virtualAddress = area-&gt;base; virtualAddress &lt; area-&gt;base
+					+ (area-&gt;size - 1); virtualAddress += B_PAGE_SIZE,
+					offset += B_PAGE_SIZE, physicalAddress += B_PAGE_SIZE) {
 				page = vm_lookup_page(physicalAddress / B_PAGE_SIZE);
 				if (page == NULL)
 					panic(&quot;couldn't lookup physical page just allocated\n&quot;);
 
-				atomic_add(&amp;page-&gt;ref_count, 1);
-				status = (*map-&gt;ops-&gt;map)(map, virtualAddress, physicalAddress, protection);
-				if (status &lt; 0)
+				status = map-&gt;ops-&gt;map(map, virtualAddress, physicalAddress,
+					protection);
+				if (status &lt; B_OK)
 					panic(&quot;couldn't map physical page in page run\n&quot;);
 
+				page-&gt;wired_count++;
+					// TODO: needs to be atomic on all platforms!
 				vm_page_set_state(page, PAGE_STATE_WIRED);
 				vm_cache_insert_page(cacheRef, page, offset);
 			}
 
-			(*map-&gt;ops-&gt;unlock)(map);
+			map-&gt;ops-&gt;unlock(map);
 			mutex_unlock(&amp;cacheRef-&gt;lock);
 			break;
 		}
@@ -1010,6 +1028,7 @@
 
 	// tell the page scanner to skip over this area, it's pages are special
 	cache-&gt;scan_skip = 1;
+	cache-&gt;type = CACHE_TYPE_DEVICE;
 	cache-&gt;virtual_size = size;
 
 	cacheRef = cache-&gt;ref;
@@ -1029,13 +1048,18 @@
 
 	if (status &gt;= B_OK) {
 		mutex_lock(&amp;cacheRef-&gt;lock);
-		store = cacheRef-&gt;cache-&gt;store;
 
 		// make sure our area is mapped in completely
-		// (even if that makes the fault routine pretty much useless)
+
+		vm_translation_map *map = &amp;addressSpace-&gt;translation_map;
+		map-&gt;ops-&gt;lock(map);
+
 		for (addr_t offset = 0; offset &lt; size; offset += B_PAGE_SIZE) {
-			store-&gt;ops-&gt;fault(store, addressSpace, offset);
+			map-&gt;ops-&gt;map(map, area-&gt;base + offset, physicalAddress + offset,
+				protection);
 		}
+
+		map-&gt;ops-&gt;unlock(map);
 		mutex_unlock(&amp;cacheRef-&gt;lock);
 	}
 
@@ -1093,6 +1117,7 @@
 
 	// tell the page scanner to skip over this area, no pages will be mapped here
 	cache-&gt;scan_skip = 1;
+	cache-&gt;type = CACHE_TYPE_NULL;
 	cache-&gt;virtual_size = size;
 
 	cacheRef = cache-&gt;ref;
@@ -1145,6 +1170,8 @@
 	if (status &lt; B_OK)
 		goto err2;
 
+	cache-&gt;type = CACHE_TYPE_VNODE;
+
 	*_cacheRef = cache-&gt;ref;
 	vfs_acquire_vnode(vnode);
 	return B_OK;
@@ -1433,10 +1460,7 @@
 
 	// unmap the virtual address space the area occupied. any page faults at this
 	// point should fail in vm_area_lookup().
-	vm_translation_map *map = &amp;addressSpace-&gt;translation_map;
-	map-&gt;ops-&gt;lock(map);
-	map-&gt;ops-&gt;unmap(map, area-&gt;base, area-&gt;base + (area-&gt;size - 1));
-	map-&gt;ops-&gt;unlock(map);
+	vm_unmap_pages(area, area-&gt;base, area-&gt;size);
 
 	// ToDo: do that only for vnode stores
 	vm_cache_write_modified(area-&gt;cache_ref, false);
@@ -1765,6 +1789,61 @@
 }
 
 
+status_t
+vm_unmap_pages(vm_area *area, addr_t base, size_t size)
+{
+	vm_translation_map *map = &amp;area-&gt;address_space-&gt;translation_map;
+
+	map-&gt;ops-&gt;lock(map);
+
+	if (area-&gt;wiring != B_NO_LOCK &amp;&amp; area-&gt;cache_ref-&gt;cache-&gt;type != CACHE_TYPE_DEVICE) {
+		// iterate through all pages and decrease their wired count
+		for (addr_t virtualAddress = base; virtualAddress &lt; base + (size - 1);
+				virtualAddress += B_PAGE_SIZE) {
+			addr_t physicalAddress;
+			uint32 flags;
+			status_t status = map-&gt;ops-&gt;query(map, virtualAddress,
+				&amp;physicalAddress, &amp;flags);
+			if (status &lt; B_OK || (flags &amp; PAGE_PRESENT) == 0)
+				continue;
+
+			vm_page *page = vm_lookup_page(physicalAddress / B_PAGE_SIZE);
+			if (page == NULL) {
+				panic(&quot;area %p looking up page failed for pa 0x%lx\n&quot;, area,
+					physicalAddress);
+			}
+
+			page-&gt;wired_count--;
+				// TODO: needs to be atomic on all platforms!
+		}
+	}
+
+	map-&gt;ops-&gt;unmap(map, base, base + (size - 1));
+	map-&gt;ops-&gt;unlock(map);
+	return B_OK;
+}
+
+
+status_t
+vm_map_page(vm_area *area, vm_page *page, addr_t address, uint32 protection)
+{
+	vm_translation_map *map = &amp;area-&gt;address_space-&gt;translation_map;
+
+	map-&gt;ops-&gt;lock(map);
+	map-&gt;ops-&gt;map(map, address, page-&gt;physical_page_number * B_PAGE_SIZE,
+		protection);
+	map-&gt;ops-&gt;unlock(map);
+
+	if (area-&gt;wiring != B_NO_LOCK) {
+		page-&gt;wired_count++;
+			// TODO: needs to be atomic on all platforms!
+	}
+
+	vm_page_set_state(page, PAGE_STATE_ACTIVE);
+	return B_OK;
+}
+
+
 static int
 display_mem(int argc, char **argv)
 {
@@ -2048,14 +2127,14 @@
 				continue;
 
 			if (page-&gt;type == PAGE_TYPE_PHYSICAL) {
-				kprintf(&quot;\t%p ppn 0x%lx offset 0x%lx type %ld state %ld (%s) ref_count %ld\n&quot;,
-					page, page-&gt;physical_page_number, page-&gt;cache_offset, page-&gt;type, page-&gt;state, 
-					page_state_to_text(page-&gt;state), page-&gt;ref_count);
+				kprintf(&quot;\t%p ppn 0x%lx offset 0x%lx type %u state %u (%s) wired_count %u\n&quot;,
+					page, page-&gt;physical_page_number, page-&gt;cache_offset, page-&gt;type, page-&gt;state,
+					page_state_to_text(page-&gt;state), page-&gt;wired_count);
 			} else if(page-&gt;type == PAGE_TYPE_DUMMY) {
-				kprintf(&quot;\t%p DUMMY PAGE state %ld (%s)\n&quot;, 
+				kprintf(&quot;\t%p DUMMY PAGE state %u (%s)\n&quot;, 
 					page, page-&gt;state, page_state_to_text(page-&gt;state));
 			} else
-				kprintf(&quot;\t%p UNKNOWN PAGE type %ld\n&quot;, page, page-&gt;type);
+				kprintf(&quot;\t%p UNKNOWN PAGE type %u\n&quot;, page, page-&gt;type);
 		}
 
 		if (!showPages)
@@ -2211,7 +2290,6 @@
 {
 	vm_address_space *addressSpace;
 	area_id id = B_ERROR;
-	vm_area *area;
 
 	addressSpace = vm_get_address_space_by_id(team);
 	if (addressSpace == NULL)
@@ -2219,18 +2297,10 @@
 
 	acquire_sem_etc(addressSpace-&gt;sem, READ_COUNT, 0, 0);
 
-	area = addressSpace-&gt;areas;
-	for (; area != NULL; area = area-&gt;address_space_next) {
-		// ignore reserved space regions
-		if (area-&gt;id == RESERVED_AREA_ID)
-			continue;
+	vm_area *area = vm_area_lookup(addressSpace, address);
+	if (area != NULL)
+		id = area-&gt;id;
 
-		if (address &gt;= area-&gt;base &amp;&amp; address &lt; area-&gt;base + area-&gt;size) {
-			id = area-&gt;id;
-			break;
-		}
-	}
-
 	release_sem_etc(addressSpace-&gt;sem, READ_COUNT, 0);
 	vm_put_address_space(addressSpace);
 
@@ -3218,18 +3288,9 @@
 		if (page-&gt;cache != topCacheRef-&gt;cache &amp;&amp; !isWrite)
 			newProtection &amp;= ~(isUser ? B_WRITE_AREA : B_KERNEL_WRITE_AREA);
 
-		atomic_add(&amp;page-&gt;ref_count, 1);
-
-		//vm_page_map(area, page, newProtection);
-		map-&gt;ops-&gt;lock(map);
-		map-&gt;ops-&gt;map(map, address, page-&gt;physical_page_number * B_PAGE_SIZE,
-			newProtection);
-		map-&gt;ops-&gt;unlock(map);
+		vm_map_page(area, page, address, newProtection);
 	}
 
-	vm_page_set_state(page, area-&gt;wiring == B_NO_LOCK
-		? PAGE_STATE_ACTIVE : PAGE_STATE_WIRED);
-
 	release_sem_etc(addressSpace-&gt;sem, READ_COUNT, 0);
 
 	mutex_unlock(&amp;pageSourceRef-&gt;lock);
@@ -3256,14 +3317,14 @@
 
 	// check the areas list first
 	area = addressSpace-&gt;area_hint;
-	if (area &amp;&amp; area-&gt;base &lt;= address &amp;&amp; (area-&gt;base + area-&gt;size) &gt; address)
+	if (area &amp;&amp; area-&gt;base &lt;= address &amp;&amp; area-&gt;base + (area-&gt;size - 1) &gt;= address)
 		goto found;
 
 	for (area = addressSpace-&gt;areas; area != NULL; area = area-&gt;address_space_next) {
 		if (area-&gt;id == RESERVED_AREA_ID)
 			continue;
 
-		if (area-&gt;base &lt;= address &amp;&amp; (area-&gt;base + area-&gt;size) &gt; address)
+		if (area-&gt;base &lt;= address &amp;&amp; area-&gt;base + (area-&gt;size - 1) &gt;= address)
 			break;
 	}
 
@@ -3382,6 +3443,39 @@
 }
 
 
+/*!
+	Tests wether or not the area that contains the specified address
+	needs any kind of locking, and actually exists.
+	Used by both lock_memory() and unlock_memory().
+*/
+status_t
+test_lock_memory(vm_address_space *addressSpace, addr_t address,
+	bool &amp;needsLocking)
+{
+	acquire_sem_etc(addressSpace-&gt;sem, READ_COUNT, 0, 0);
+
+	vm_area *area = vm_area_lookup(addressSpace, address);
+	if (area != NULL) {
+		mutex_lock(&amp;area-&gt;cache_ref-&gt;lock);
+
+		// This determines if we need to lock the memory at all
+		needsLocking = area-&gt;cache_ref-&gt;cache-&gt;type != CACHE_TYPE_NULL
+			&amp;&amp; area-&gt;cache_ref-&gt;cache-&gt;type != CACHE_TYPE_DEVICE
+			&amp;&amp; area-&gt;wiring != B_FULL_LOCK
+			&amp;&amp; area-&gt;wiring != B_CONTIGUOUS;
+
+		mutex_unlock(&amp;area-&gt;cache_ref-&gt;lock);
+	}
+
+	release_sem_etc(addressSpace-&gt;sem, READ_COUNT, 0);
+
+	if (area == NULL)
+		return B_BAD_ADDRESS;
+
+	return B_OK;
+}
+
+
 //	#pragma mark -
 
 
@@ -3427,20 +3521,8 @@
 	addr_t base = (addr_t)address;
 	addr_t end = base + numBytes;
 	bool isUser = IS_USER_ADDRESS(address);
+	bool needsLocking = true;
 
-	// ToDo: Our VM currently doesn't support locking, this function
-	//	will now at least make sure that the memory is paged in, but
-	//	that's about it.
-	//	Nevertheless, it must be implemented as soon as we're able to
-	//	swap pages out of memory.
-
-	// ToDo: this is a hack, too; the iospace area is a null region and
-	//	officially cannot be written to or read; ie. vm_soft_fault() will
-	//	fail there. Furthermore, this is x86 specific as well.
-	#define IOSPACE_SIZE (256 * 1024 * 1024)
-	if (base &gt;= KERNEL_BASE + IOSPACE_SIZE &amp;&amp; base + numBytes &lt; KERNEL_BASE + 2 * IOSPACE_SIZE)
-		return B_OK;
-
 	if (isUser)
 		addressSpace = vm_get_current_user_address_space();
 	else
@@ -3448,44 +3530,125 @@
 	if (addressSpace == NULL)
 		return B_ERROR;
 
+	// test if we're on an area that allows faults at all
+
 	map = &amp;addressSpace-&gt;translation_map;
 
+	status_t status = test_lock_memory(addressSpace, base, needsLocking);
+	if (status &lt; B_OK)
+		goto out;
+	if (!needsLocking)
+		goto out;
+
 	for (; base &lt; end; base += B_PAGE_SIZE) {
 		addr_t physicalAddress;
 		uint32 protection;
 		status_t status;
 
 		map-&gt;ops-&gt;lock(map);
-		map-&gt;ops-&gt;query(map, base, &amp;physicalAddress, &amp;protection);
+		status = map-&gt;ops-&gt;query(map, base, &amp;physicalAddress, &amp;protection);
 		map-&gt;ops-&gt;unlock(map);
 
+		if (status &lt; B_OK)
+			goto out;
+
 		if ((protection &amp; PAGE_PRESENT) != 0) {
 			// if B_READ_DEVICE is set, the caller intents to write to the locked
 			// memory, so if it hasn't been mapped writable, we'll try the soft
 			// fault anyway
 			if ((flags &amp; B_READ_DEVICE) == 0
-				|| (protection &amp; (B_WRITE_AREA | B_KERNEL_WRITE_AREA)) != 0)
-			continue;
+				|| (protection &amp; (B_WRITE_AREA | B_KERNEL_WRITE_AREA)) != 0) {
+				// update wiring
+				vm_page *page = vm_lookup_page(physicalAddress / B_PAGE_SIZE);
+				if (page == NULL)
+					panic(&quot;couldn't lookup physical page just allocated\n&quot;);
+
+				page-&gt;wired_count++;
+					// TODO: needs to be atomic on all platforms!
+				continue;
+			}
 		}
 
 		status = vm_soft_fault(base, (flags &amp; B_READ_DEVICE) != 0, isUser);
 		if (status != B_OK)	{
 			dprintf(&quot;lock_memory(address = %p, numBytes = %lu, flags = %lu) failed: %s\n&quot;,
 				address, numBytes, flags, strerror(status));
-			vm_put_address_space(addressSpace);
-			return status;
+			goto out;
 		}
+
+		map-&gt;ops-&gt;lock(map);
+		status = map-&gt;ops-&gt;query(map, base, &amp;physicalAddress, &amp;protection);
+		map-&gt;ops-&gt;unlock(map);
+
+		if (status &lt; B_OK)
+			goto out;
+
+		// update wiring
+		vm_page *page = vm_lookup_page(physicalAddress / B_PAGE_SIZE);
+		if (page == NULL)
+			panic(&quot;couldn't lookup physical page&quot;);
+
+		page-&gt;wired_count++;
+			// TODO: needs to be atomic on all platforms!
 	}
 
+out:
 	vm_put_address_space(addressSpace);
-	return B_OK;
+	return status;
 }
 
 
 long
-unlock_memory(void *buffer, ulong numBytes, ulong flags)
+unlock_memory(void *address, ulong numBytes, ulong flags)
 {
-	return B_OK;
+	vm_address_space *addressSpace = NULL;
+	struct vm_translation_map *map;
+	addr_t base = (addr_t)address;
+	addr_t end = base + numBytes;
+	bool needsLocking = true;
+
+	if (IS_USER_ADDRESS(address))
+		addressSpace = vm_get_current_user_address_space();
+	else
+		addressSpace = vm_get_kernel_address_space();
+	if (addressSpace == NULL)
+		return B_ERROR;
+
+	map = &amp;addressSpace-&gt;translation_map;
+
+	status_t status = test_lock_memory(addressSpace, base, needsLocking);
+	if (status &lt; B_OK)
+		goto out;
+	if (!needsLocking)
+		goto out;
+
+	for (; base &lt; end; base += B_PAGE_SIZE) {
+		map-&gt;ops-&gt;lock(map);
+
+		addr_t physicalAddress;
+		uint32 protection;
+		status = map-&gt;ops-&gt;query(map, base, &amp;physicalAddress,
+			&amp;protection);
+
+		map-&gt;ops-&gt;unlock(map);
+
+		if (status &lt; B_OK)
+			goto out;
+		if ((protection &amp; PAGE_PRESENT) == 0)
+			panic(&quot;calling unlock_memory() on unmapped memory!&quot;);
+
+		// update wiring
+		vm_page *page = vm_lookup_page(physicalAddress / B_PAGE_SIZE);
+		if (page == NULL)
+			panic(&quot;couldn't lookup physical page&quot;);
+
+		page-&gt;wired_count--;
+			// TODO: needs to be atomic on all platforms!
+	}
+
+out:
+	vm_put_address_space(addressSpace);
+	return status;
 }
 
 
@@ -3832,7 +3995,7 @@
 
 	sourceAddressSpace = area-&gt;address_space;
 
-	reserved = _vm_create_reserved_region_struct(sourceAddressSpace, 0);
+	reserved = create_reserved_area_struct(sourceAddressSpace, 0);
 	if (reserved == NULL) {
 		status = B_NO_MEMORY;
 		goto err2;

Modified: haiku/trunk/src/system/kernel/vm/vm_daemons.c
===================================================================
--- haiku/trunk/src/system/kernel/vm/vm_daemons.c	2007-02-28 11:31:19 UTC (rev 20250)
+++ haiku/trunk/src/system/kernel/vm/vm_daemons.c	2007-02-28 13:24:53 UTC (rev 20251)
@@ -22,6 +22,7 @@
 static addr_t free_memory_high_water;
 
 
+#if 0
 static void
 scan_pages(vm_address_space *aspace, addr_t free_target)
 {
@@ -202,6 +203,7 @@
 		}
 	}
 }
+#endif
 
 
 status_t

Modified: haiku/trunk/src/system/kernel/vm/vm_page.c
===================================================================
--- haiku/trunk/src/system/kernel/vm/vm_page.c	2007-02-28 11:31:19 UTC (rev 20250)
+++ haiku/trunk/src/system/kernel/vm/vm_page.c	2007-02-28 13:24:53 UTC (rev 20251)
@@ -182,9 +182,11 @@
 	kprintf(&quot;cache:           %p\n&quot;, page-&gt;cache);
 	kprintf(&quot;cache_offset:    %ld\n&quot;, page-&gt;cache_offset);
 	kprintf(&quot;cache_next,prev: %p, %p\n&quot;, page-&gt;cache_next, page-&gt;cache_prev);
-	kprintf(&quot;ref_count:       %ld\n&quot;, page-&gt;ref_count);
+	kprintf(&quot;mappings:        %p\n&quot;, page-&gt;mappings);
 	kprintf(&quot;type:            %d\n&quot;, page-&gt;type);
 	kprintf(&quot;state:           %d\n&quot;, page-&gt;state);
+	kprintf(&quot;wired_count:     %u\n&quot;, page-&gt;wired_count);
+	kprintf(&quot;usage_count:     %u\n&quot;, page-&gt;usage_count);
 
 	return 0;
 }
@@ -637,7 +639,7 @@
 				state = disable_interrupts();
 				acquire_spinlock(&amp;sPageLock);
 
-				if (page-&gt;ref_count &gt; 0)
+				if (page-&gt;mappings != NULL || page-&gt;wired_count)
 					page-&gt;state = PAGE_STATE_ACTIVE;
 				else
 					page-&gt;state = PAGE_STATE_INACTIVE;
@@ -650,6 +652,16 @@
 		} else {
 			// We don't have to put the PAGE_MODIFIED bit back, as it's still
 			// in the modified pages list.
+			if (dequeuedPage) {
+				state = disable_interrupts();
+				acquire_spinlock(&amp;sPageLock);
+
+				page-&gt;state = PAGE_STATE_MODIFIED;
+				enqueue_page(&amp;page_modified_queue, page);
+
+				release_spinlock(&amp;sPageLock);
+				restore_interrupts(state);
+			}
 		}
 	}
 
@@ -713,9 +725,9 @@
 		sPages[i].physical_page_number = sPhysicalPageOffset + i;
 		sPages[i].type = PAGE_TYPE_PHYSICAL;
 		sPages[i].state = PAGE_STATE_FREE;
-		sPages[i].ref_count = 0;
-		//sPages[i].wired_count = 0;
-		//sPages[i].usage_count = 0;
+		sPages[i].mappings = NULL;
+		sPages[i].wired_count = 0;
+		sPages[i].usage_count = 0;
 		enqueue_page(&amp;page_free_queue, &amp;sPages[i]);
 	}
 

Modified: haiku/trunk/src/system/kernel/vm/vm_store_device.c
===================================================================
--- haiku/trunk/src/system/kernel/vm/vm_store_device.c	2007-02-28 11:31:19 UTC (rev 20250)
+++ haiku/trunk/src/system/kernel/vm/vm_store_device.c	2007-02-28 13:24:53 UTC (rev 20251)
@@ -62,42 +62,11 @@
 }
 
 
-/** this fault handler should take over the page fault routine and map the page in
- *
- *	setup: the cache that this store is part of has a ref being held and will be
- *	released after this handler is done
- */
-
 static status_t
 device_fault(struct vm_store *_store, struct vm_address_space *aspace, off_t offset)
 {
-	struct device_store *store = (struct device_store *)_store;
-	vm_cache_ref *cache_ref = store-&gt;vm.cache-&gt;ref;
-	vm_translation_map *map = &amp;aspace-&gt;translation_map;
-	vm_area *area;
-
-	// figure out which page needs to be mapped where
-	map-&gt;ops-&gt;lock(map);
-
-	// cycle through all of the regions that map this cache and map the page in
-	for (area = cache_ref-&gt;areas; area != NULL; area = area-&gt;cache_next) {
-		// make sure this page in the cache that was faulted on is covered in this area
-		if (offset &gt;= area-&gt;cache_offset &amp;&amp; (offset - area-&gt;cache_offset) &lt; area-&gt;size) {
-			// don't map already mapped pages
-			addr_t physicalAddress;
-			uint32 flags;
-			map-&gt;ops-&gt;query(map, area-&gt;base + (offset - area-&gt;cache_offset),
-				&amp;physicalAddress, &amp;flags);
-			if (flags &amp; PAGE_PRESENT)
-				continue;
-
-			map-&gt;ops-&gt;map(map, area-&gt;base + (offset - area-&gt;cache_offset),
-				store-&gt;base_address + offset, area-&gt;protection);
-		}
-	}
-
-	map-&gt;ops-&gt;unlock(map);
-	return B_OK;
+	// devices are mapped in completely, so we shouldn't experience faults
+	return B_BAD_ADDRESS;
 }
 
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000942.html">[Haiku-commits] r20250 - haiku/trunk/headers/private/kernel/util
</A></li>
	<LI>Next message: <A HREF="000945.html">[Haiku-commits] r20252 - haiku/trunk/headers/private/kernel/util
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#943">[ date ]</a>
              <a href="thread.html#943">[ thread ]</a>
              <a href="subject.html#943">[ subject ]</a>
              <a href="author.html#943">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/haiku-commits">More information about the Haiku-commits
mailing list</a><br>
</body></html>
