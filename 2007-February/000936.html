<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Haiku-commits] r20244 - in haiku/trunk: headers/private/kernel	src/system/kernel/arch/generic src/system/kernel/arch/x86	src/system/kernel/vm
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/haiku-commits/2007-February/index.html" >
   <LINK REL="made" HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r20244%20-%20in%20haiku/trunk%3A%20headers/private/kernel%0A%09src/system/kernel/arch/generic%20src/system/kernel/arch/x86%0A%09src/system/kernel/vm&In-Reply-To=%3C200702271926.l1RJQgLf030334%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000934.html">
   <LINK REL="Next"  HREF="000947.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Haiku-commits] r20244 - in haiku/trunk: headers/private/kernel	src/system/kernel/arch/generic src/system/kernel/arch/x86	src/system/kernel/vm</H1>
    <B>axeld at BerliOS</B> 
    <A HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r20244%20-%20in%20haiku/trunk%3A%20headers/private/kernel%0A%09src/system/kernel/arch/generic%20src/system/kernel/arch/x86%0A%09src/system/kernel/vm&In-Reply-To=%3C200702271926.l1RJQgLf030334%40sheep.berlios.de%3E"
       TITLE="[Haiku-commits] r20244 - in haiku/trunk: headers/private/kernel	src/system/kernel/arch/generic src/system/kernel/arch/x86	src/system/kernel/vm">axeld at mail.berlios.de
       </A><BR>
    <I>Tue Feb 27 20:26:42 CET 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000934.html">[Haiku-commits] r20243 - haiku/trunk/src/preferences/backgrounds
</A></li>
        <LI>Next message: <A HREF="000947.html">[Haiku-commits] r20244 - in haiku/trunk: headers/private/kernel	src/system/kernel/arch/generic src/system/kernel/arch/x86	src/system/kernel/vm
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#936">[ date ]</a>
              <a href="thread.html#936">[ thread ]</a>
              <a href="subject.html#936">[ subject ]</a>
              <a href="author.html#936">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: axeld
Date: 2007-02-27 20:26:40 +0100 (Tue, 27 Feb 2007)
New Revision: 20244
ViewCVS: <A HREF="http://svn.berlios.de/viewcvs/haiku?rev=20244&amp;view=rev">http://svn.berlios.de/viewcvs/haiku?rev=20244&amp;view=rev</A>

Modified:
   haiku/trunk/headers/private/kernel/vm.h
   haiku/trunk/headers/private/kernel/vm_page.h
   haiku/trunk/headers/private/kernel/vm_priv.h
   haiku/trunk/src/system/kernel/arch/generic/generic_vm_physical_page_mapper.cpp
   haiku/trunk/src/system/kernel/arch/x86/arch_vm_translation_map.c
   haiku/trunk/src/system/kernel/vm/vm.cpp
   haiku/trunk/src/system/kernel/vm/vm_page.c
Log:
* Moved the early startup VM allocation functions from vm_page.c to vm.cpp.
* Renamed them, made everything static besides vm_allocate_early() (previous
  vm_alloc_from_kernel_args()) which now allows you to specify a different
  virtual than physical size, and therefore makes vm_alloc_virtual_from_kernel_args()
  superfluous (which isn't exported anymore, and is now called allocate_early_virtual()).
* Enabled printing a stack trace on serial output on team crash - it doesn't hurt
  for now, anyway.
* Cleanup.


Modified: haiku/trunk/headers/private/kernel/vm.h
===================================================================
--- haiku/trunk/headers/private/kernel/vm.h	2007-02-27 00:31:31 UTC (rev 20243)
+++ haiku/trunk/headers/private/kernel/vm.h	2007-02-27 19:26:40 UTC (rev 20244)
@@ -21,13 +21,15 @@
 extern &quot;C&quot; {
 #endif
 
-//void vm_dump_areas(vm_address_space *aspace);
+// startup only
 status_t vm_init(kernel_args *args);
 status_t vm_init_post_sem(struct kernel_args *args);
 status_t vm_init_post_thread(struct kernel_args *args);
 status_t vm_init_post_modules(struct kernel_args *args);
 void vm_free_kernel_args(kernel_args *args);
 void vm_free_unused_boot_loader_range(addr_t start, addr_t end);
+addr_t vm_allocate_early(kernel_args *args, size_t virtualSize,
+			size_t physicalSize, uint32 attributes);
 
 // to protect code regions with interrupts turned on
 void permit_page_faults(void);

Modified: haiku/trunk/headers/private/kernel/vm_page.h
===================================================================
--- haiku/trunk/headers/private/kernel/vm_page.h	2007-02-27 00:31:31 UTC (rev 20243)
+++ haiku/trunk/headers/private/kernel/vm_page.h	2007-02-27 19:26:40 UTC (rev 20244)
@@ -1,5 +1,5 @@
 /*
- * Copyright 2002-2006, Axel D&#246;rfler, <A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">axeld at pinc-software.de.</A>
+ * Copyright 2002-2007, Axel D&#246;rfler, <A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">axeld at pinc-software.de.</A>
  * Distributed under the terms of the MIT License.
  *
  * Copyright 2001-2002, Travis Geiselbrecht. All rights reserved.
@@ -24,8 +24,6 @@
 status_t vm_page_init_post_area(struct kernel_args *args);
 status_t vm_page_init_post_thread(struct kernel_args *args);
 
-addr_t vm_alloc_virtual_from_kernel_args(kernel_args *ka, size_t size);
-
 status_t vm_mark_page_inuse(addr_t page);
 status_t vm_mark_page_range_inuse(addr_t startPage, addr_t length);
 status_t vm_page_set_state(vm_page *page, int state);

Modified: haiku/trunk/headers/private/kernel/vm_priv.h
===================================================================
--- haiku/trunk/headers/private/kernel/vm_priv.h	2007-02-27 00:31:31 UTC (rev 20243)
+++ haiku/trunk/headers/private/kernel/vm_priv.h	2007-02-27 19:26:40 UTC (rev 20244)
@@ -1,5 +1,5 @@
 /*
- * Copyright 2002-2005, The Haiku Team. All rights reserved.
+ * Copyright 2002-2007, Haiku. All rights reserved.
  * Distributed under the terms of the MIT License.
  *
  * Copyright 2001-2002, Travis Geiselbrecht. All rights reserved.
@@ -53,9 +53,6 @@
 void vm_address_space_walk_start(struct hash_iterator *i);
 vm_address_space *vm_address_space_walk_next(struct hash_iterator *i);
 
-// allocates memory from the kernel_args structure
-addr_t vm_alloc_from_kernel_args(kernel_args *args, size_t size, uint32 lock);
-
 #ifdef __cplusplus
 }
 #endif

Modified: haiku/trunk/src/system/kernel/arch/generic/generic_vm_physical_page_mapper.cpp
===================================================================
--- haiku/trunk/src/system/kernel/arch/generic/generic_vm_physical_page_mapper.cpp	2007-02-27 00:31:31 UTC (rev 20243)
+++ haiku/trunk/src/system/kernel/arch/generic/generic_vm_physical_page_mapper.cpp	2007-02-27 19:26:40 UTC (rev 20244)
@@ -1,18 +1,17 @@
 /*
- * Copyright 2002-2006, Axel D&#246;rfler, <A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">axeld at pinc-software.de.</A> All rights reserved.
+ * Copyright 2002-2007, Axel D&#246;rfler, <A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">axeld at pinc-software.de.</A> All rights reserved.
  * Distributed under the terms of the MIT License.
  *
  * Copyright 2001-2002, Travis Geiselbrecht. All rights reserved.
  * Distributed under the terms of the NewOS License.
  */
 
+
 #include &quot;generic_vm_physical_page_mapper.h&quot;
 
 #include &lt;vm_address_space.h&gt;
 #include &lt;vm_page.h&gt;
 #include &lt;vm_priv.h&gt;
-//#include &lt;smp.h&gt;
-//#include &lt;memheap.h&gt;
 #include &lt;thread.h&gt;
 #include &lt;util/queue.h&gt;
 
@@ -253,8 +252,8 @@
 	// reserve virtual space for the IO space
 	// We reserve (ioSpaceChunkSize - B_PAGE_SIZE) bytes more, so that we
 	// can guarantee to align the base address to ioSpaceChunkSize.
-	sIOSpaceBase = vm_alloc_virtual_from_kernel_args(args,
-		sIOSpaceSize + ioSpaceChunkSize - B_PAGE_SIZE);
+	sIOSpaceBase = vm_allocate_early(args,
+		sIOSpaceSize + ioSpaceChunkSize - B_PAGE_SIZE, 0, 0);
 	if (sIOSpaceBase == 0) {
 		panic(&quot;generic_vm_physical_page_mapper_init(): Failed to reserve IO &quot;
 			&quot;space in virtual address space!&quot;);
@@ -267,11 +266,13 @@
 	*ioSpaceBase = sIOSpaceBase;
 
 	// allocate some space to hold physical page mapping info
-	paddr_desc = (paddr_chunk_desc *)vm_alloc_from_kernel_args(args,
-		sizeof(paddr_chunk_desc) * 1024, B_KERNEL_READ_AREA | B_KERNEL_WRITE_AREA);
+	paddr_desc = (paddr_chunk_desc *)vm_allocate_early(args,
+		sizeof(paddr_chunk_desc) * 1024, ~0L,
+		B_KERNEL_READ_AREA | B_KERNEL_WRITE_AREA);
 	num_virtual_chunks = sIOSpaceSize / sIOSpaceChunkSize;
-	virtual_pmappings = (paddr_chunk_desc **)vm_alloc_from_kernel_args(args,
-		sizeof(paddr_chunk_desc *) * num_virtual_chunks, B_KERNEL_READ_AREA | B_KERNEL_WRITE_AREA);
+	virtual_pmappings = (paddr_chunk_desc **)vm_allocate_early(args,
+		sizeof(paddr_chunk_desc *) * num_virtual_chunks, ~0L,
+		B_KERNEL_READ_AREA | B_KERNEL_WRITE_AREA);
 
 	TRACE((&quot;paddr_desc %p, virtual_pmappings %p&quot;/*&quot;, iospace_pgtables %p&quot;*/&quot;\n&quot;,
 		paddr_desc, virtual_pmappings/*, iospace_pgtables*/));

Modified: haiku/trunk/src/system/kernel/arch/x86/arch_vm_translation_map.c
===================================================================
--- haiku/trunk/src/system/kernel/arch/x86/arch_vm_translation_map.c	2007-02-27 00:31:31 UTC (rev 20243)
+++ haiku/trunk/src/system/kernel/arch/x86/arch_vm_translation_map.c	2007-02-27 19:26:40 UTC (rev 20244)
@@ -845,8 +845,9 @@
 	tmap_list = NULL;
 
 	// allocate some space to hold physical page mapping info
-	iospace_pgtables = (page_table_entry *)vm_alloc_from_kernel_args(args,
-		B_PAGE_SIZE * (IOSPACE_SIZE / (B_PAGE_SIZE * 1024)), B_KERNEL_READ_AREA | B_KERNEL_WRITE_AREA);
+	iospace_pgtables = (page_table_entry *)vm_allocate_early(args,
+		B_PAGE_SIZE * (IOSPACE_SIZE / (B_PAGE_SIZE * 1024)), ~0L,
+		B_KERNEL_READ_AREA | B_KERNEL_WRITE_AREA);
 
 	TRACE((&quot;iospace_pgtables %p\n&quot;, iospace_pgtables));
 

Modified: haiku/trunk/src/system/kernel/vm/vm.cpp
===================================================================
--- haiku/trunk/src/system/kernel/vm/vm.cpp	2007-02-27 00:31:31 UTC (rev 20243)
+++ haiku/trunk/src/system/kernel/vm/vm.cpp	2007-02-27 19:26:40 UTC (rev 20244)
@@ -962,9 +962,8 @@
 
 
 area_id
-vm_map_physical_memory(team_id areaID, const char *name, void **_address,
-	uint32 addressSpec, addr_t size, uint32 protection,
-	addr_t physicalAddress)
+vm_map_physical_memory(team_id aspaceID, const char *name, void **_address,
+	uint32 addressSpec, addr_t size, uint32 protection, addr_t physicalAddress)
 {
 	vm_cache_ref *cacheRef;
 	vm_area *area;
@@ -975,13 +974,13 @@
 
 	TRACE((&quot;vm_map_physical_memory(aspace = %ld, \&quot;%s\&quot;, virtual = %p, spec = %ld,&quot;
 		&quot; size = %lu, protection = %ld, phys = %p)\n&quot;,
-		areaID, name, _address, addressSpec, size, protection,
+		aspaceID, name, _address, addressSpec, size, protection,
 		(void *)physicalAddress));
 
 	if (!arch_vm_supports_protection(protection))
 		return B_NOT_SUPPORTED;
 
-	vm_address_space *addressSpace = vm_get_address_space_by_id(areaID);
+	vm_address_space *addressSpace = vm_get_address_space_by_id(aspaceID);
 	if (addressSpace == NULL)
 		return B_BAD_TEAM_ID;
 
@@ -2421,6 +2420,128 @@
 }
 
 
+static addr_t
+allocate_early_virtual(kernel_args *args, size_t size)
+{
+	addr_t spot = 0;
+	uint32 i;
+	int last_valloc_entry = 0;
+
+	size = PAGE_ALIGN(size);
+	// find a slot in the virtual allocation addr range
+	for (i = 1; i &lt; args-&gt;num_virtual_allocated_ranges; i++) {
+		addr_t previousRangeEnd = args-&gt;virtual_allocated_range[i - 1].start
+			+ args-&gt;virtual_allocated_range[i - 1].size;
+		last_valloc_entry = i;
+		// check to see if the space between this one and the last is big enough
+		if (previousRangeEnd &gt;= KERNEL_BASE
+			&amp;&amp; args-&gt;virtual_allocated_range[i].start
+				- previousRangeEnd &gt;= size) {
+			spot = previousRangeEnd;
+			args-&gt;virtual_allocated_range[i - 1].size += size;
+			goto out;
+		}
+	}
+	if (spot == 0) {
+		// we hadn't found one between allocation ranges. this is ok.
+		// see if there's a gap after the last one
+		addr_t lastRangeEnd
+			= args-&gt;virtual_allocated_range[last_valloc_entry].start 
+				+ args-&gt;virtual_allocated_range[last_valloc_entry].size;
+		if (KERNEL_BASE + (KERNEL_SIZE - 1) - lastRangeEnd &gt;= size) {
+			spot = lastRangeEnd;
+			args-&gt;virtual_allocated_range[last_valloc_entry].size += size;
+			goto out;
+		}
+		// see if there's a gap before the first one
+		if (args-&gt;virtual_allocated_range[0].start &gt; KERNEL_BASE) {
+			if (args-&gt;virtual_allocated_range[0].start - KERNEL_BASE &gt;= size) {
+				args-&gt;virtual_allocated_range[0].start -= size;
+				spot = args-&gt;virtual_allocated_range[0].start;
+				goto out;
+			}
+		}
+	}
+
+out:
+	return spot;
+}
+
+
+static bool
+is_page_in_physical_memory_range(kernel_args *args, addr_t address)
+{
+	// TODO: horrible brute-force method of determining if the page can be allocated
+	for (uint32 i = 0; i &lt; args-&gt;num_physical_memory_ranges; i++) {
+		if (address &gt;= args-&gt;physical_memory_range[i].start 
+			&amp;&amp; address &lt; args-&gt;physical_memory_range[i].start 
+				+ args-&gt;physical_memory_range[i].size)
+			return true;
+	}
+	return false;
+}
+
+
+static addr_t
+allocate_early_physical_page(kernel_args *args)
+{
+	for (uint32 i = 0; i &lt; args-&gt;num_physical_allocated_ranges; i++) {
+		addr_t nextPage;
+
+		nextPage = args-&gt;physical_allocated_range[i].start
+			+ args-&gt;physical_allocated_range[i].size;
+		// see if the page after the next allocated paddr run can be allocated
+		if (i + 1 &lt; args-&gt;num_physical_allocated_ranges
+			&amp;&amp; args-&gt;physical_allocated_range[i + 1].size != 0) {
+			// see if the next page will collide with the next allocated range
+			if (nextPage &gt;= args-&gt;physical_allocated_range[i+1].start)
+				continue;
+		}
+		// see if the next physical page fits in the memory block
+		if (is_page_in_physical_memory_range(args, nextPage)) {
+			// we got one!
+			args-&gt;physical_allocated_range[i].size += B_PAGE_SIZE;
+			return nextPage / B_PAGE_SIZE;
+		}
+	}
+
+	return 0;
+		// could not allocate a block
+}
+
+
+/*!
+	This one uses the kernel_args' physical and virtual memory ranges to
+	allocate some pages before the VM is completely up.
+*/
+addr_t
+vm_allocate_early(kernel_args *args, size_t virtualSize, size_t physicalSize,
+	uint32 attributes)
+{
+	if (physicalSize &gt; virtualSize)
+		physicalSize = virtualSize;
+
+	// find the vaddr to allocate at
+	addr_t virtualBase = allocate_early_virtual(args, virtualSize);
+	//dprintf(&quot;vm_allocate_early: vaddr 0x%lx\n&quot;, virtualAddress);
+
+	// map the pages
+	for (uint32 i = 0; i &lt; PAGE_ALIGN(physicalSize) / B_PAGE_SIZE; i++) {
+		addr_t physicalAddress = allocate_early_physical_page(args);
+		if (physicalAddress == 0)
+			panic(&quot;error allocating early page!\n&quot;);
+
+		//dprintf(&quot;vm_allocate_early: paddr 0x%lx\n&quot;, physicalAddress);
+
+		arch_vm_translation_map_early_map(args, virtualBase + i * B_PAGE_SIZE,
+			physicalAddress * B_PAGE_SIZE, attributes,
+			&amp;allocate_early_physical_page);
+	}
+
+	return virtualBase;
+}
+
+
 status_t
 vm_init(kernel_args *args)
 {
@@ -2449,7 +2570,7 @@
 		heapSize /= 2;
 
 	// map in the new heap and initialize it
-	addr_t heapBase = vm_alloc_from_kernel_args(args, heapSize,
+	addr_t heapBase = vm_allocate_early(args, heapSize, heapSize,
 		B_KERNEL_READ_AREA | B_KERNEL_WRITE_AREA);
 	TRACE((&quot;heap at 0x%lx\n&quot;, heapBase));
 	heap_init(heapBase, heapSize);
@@ -2628,7 +2749,7 @@
 				area ? area-&gt;name : &quot;???&quot;, faultAddress - (area ? area-&gt;base : 0x0));
 
 			// We can print a stack trace of the userland thread here.
-#if 0
+#if 1
 			if (area) {
 				struct stack_frame {
 					#ifdef __INTEL__
@@ -3099,15 +3220,18 @@
 
 		atomic_add(&amp;page-&gt;ref_count, 1);
 
+		//vm_page_map(area, page, newProtection);
 		map-&gt;ops-&gt;lock(map);
 		map-&gt;ops-&gt;map(map, address, page-&gt;physical_page_number * B_PAGE_SIZE,
 			newProtection);
 		map-&gt;ops-&gt;unlock(map);
 	}
 
+	vm_page_set_state(page, area-&gt;wiring == B_NO_LOCK
+		? PAGE_STATE_ACTIVE : PAGE_STATE_WIRED);
+
 	release_sem_etc(addressSpace-&gt;sem, READ_COUNT, 0);
 
-	vm_page_set_state(page, PAGE_STATE_ACTIVE);
 	mutex_unlock(&amp;pageSourceRef-&gt;lock);
 	vm_cache_release_ref(pageSourceRef);
 
@@ -3405,8 +3529,8 @@
 
 		if (interrupts) {
 			uint32 flags;
-			status = map-&gt;ops-&gt;query(map, (addr_t)address + offset, &amp;physicalAddress,
-				&amp;flags);
+			status = map-&gt;ops-&gt;query(map, (addr_t)address + offset,
+				&amp;physicalAddress, &amp;flags);
 		} else {
 			status = map-&gt;ops-&gt;query_interrupt(map, (addr_t)address + offset,
 				&amp;physicalAddress);
@@ -3421,7 +3545,8 @@
 		}
 
 		// need to switch to the next physical_entry?
-		if (index &lt; 0 || (addr_t)table[index].address != physicalAddress - table[index].size) {
+		if (index &lt; 0 || (addr_t)table[index].address
+				!= physicalAddress - table[index].size) {
 			if (++index + 1 &gt; numEntries) {
 				// table to small
 				status = B_BUFFER_OVERFLOW;
@@ -3604,12 +3729,15 @@
 		// We need to check if all areas of this cache can be resized
 
 		for (current = cacheRef-&gt;areas; current; current = current-&gt;cache_next) {
-			if (current-&gt;address_space_next &amp;&amp; current-&gt;address_space_next-&gt;base &lt;= (current-&gt;base + newSize)) {
+			if (current-&gt;address_space_next
+				&amp;&amp; current-&gt;address_space_next-&gt;base &lt;= (current-&gt;base
+					+ newSize)) {
 				// if the area was created inside a reserved area, it can also be
 				// resized in that area
 				// ToDo: if there is free space after the reserved area, it could be used as well...
 				vm_area *next = current-&gt;address_space_next;
-				if (next-&gt;id == RESERVED_AREA_ID &amp;&amp; next-&gt;cache_offset &lt;= current-&gt;base
+				if (next-&gt;id == RESERVED_AREA_ID
+					&amp;&amp; next-&gt;cache_offset &lt;= current-&gt;base
 					&amp;&amp; next-&gt;base - 1 + next-&gt;size &gt;= current-&gt;base - 1 + newSize)
 					continue;
 
@@ -3622,9 +3750,11 @@
 	// Okay, looks good so far, so let's do it
 
 	for (current = cacheRef-&gt;areas; current; current = current-&gt;cache_next) {
-		if (current-&gt;address_space_next &amp;&amp; current-&gt;address_space_next-&gt;base &lt;= (current-&gt;base + newSize)) {
+		if (current-&gt;address_space_next
+			&amp;&amp; current-&gt;address_space_next-&gt;base &lt;= (current-&gt;base + newSize)) {
 			vm_area *next = current-&gt;address_space_next;
-			if (next-&gt;id == RESERVED_AREA_ID &amp;&amp; next-&gt;cache_offset &lt;= current-&gt;base
+			if (next-&gt;id == RESERVED_AREA_ID
+				&amp;&amp; next-&gt;cache_offset &lt;= current-&gt;base
 				&amp;&amp; next-&gt;base - 1 + next-&gt;size &gt;= current-&gt;base - 1 + newSize) {
 				// resize reserved area
 				addr_t offset = current-&gt;base + newSize - next-&gt;base;

Modified: haiku/trunk/src/system/kernel/vm/vm_page.c
===================================================================
--- haiku/trunk/src/system/kernel/vm/vm_page.c	2007-02-27 00:31:31 UTC (rev 20243)
+++ haiku/trunk/src/system/kernel/vm/vm_page.c	2007-02-27 19:26:40 UTC (rev 20244)
@@ -1,5 +1,5 @@
 /*
- * Copyright 2002-2006, Axel D&#246;rfler, <A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">axeld at pinc-software.de.</A>
+ * Copyright 2002-2007, Axel D&#246;rfler, <A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">axeld at pinc-software.de.</A>
  * Distributed under the terms of the MIT License.
  *
  * Copyright 2001-2002, Travis Geiselbrecht. All rights reserved.
@@ -65,8 +65,7 @@
 static int32 page_scrubber(void *);
 
 
-/**	Dequeues a page from the tail of the given queue */
-
+/*!	Dequeues a page from the tail of the given queue */
 static vm_page *
 dequeue_page(page_queue *q)
 {
@@ -87,8 +86,7 @@
 }
 
 
-/**	Enqueues a page to the head of the given queue */
-
+/*!	Enqueues a page to the head of the given queue */
 static void
 enqueue_page(page_queue *q, vm_page *page)
 {
@@ -165,10 +163,14 @@
 }
 
 
+/*!
+	You need to hold the vm_cache lock when calling this function.
+*/
 status_t
 vm_page_write_modified(vm_cache *cache, bool fsReenter)
 {
 	vm_page *page = cache-&gt;page_list;
+	vm_cache_ref *ref = cache-&gt;ref;
 
 	// ToDo: join adjacent pages into one vec list
 
@@ -202,7 +204,7 @@
 
 		pageOffset = (off_t)page-&gt;cache_offset &lt;&lt; PAGE_SHIFT;
 
-		for (area = page-&gt;cache-&gt;ref-&gt;areas; area; area = area-&gt;cache_next) {
+		for (area = ref-&gt;areas; area; area = area-&gt;cache_next) {
 			if (pageOffset &gt;= area-&gt;cache_offset
 				&amp;&amp; pageOffset &lt; area-&gt;cache_offset + area-&gt;size) {
 				vm_translation_map *map = &amp;area-&gt;address_space-&gt;translation_map;
@@ -231,10 +233,13 @@
 		if (!gotPage)
 			continue;
 
-		mutex_unlock(&amp;cache-&gt;ref-&gt;lock);
+		mutex_unlock(&amp;ref-&gt;lock);
+
 		status = write_page(page, fsReenter);
-		mutex_lock(&amp;cache-&gt;ref-&gt;lock);
 
+		mutex_lock(&amp;ref-&gt;lock);
+		cache = ref-&gt;cache;
+
 		if (status == B_OK) {
 			if (dequeuedPage) {
 				// put it into the active queue
@@ -386,8 +391,8 @@
 	page_active_queue.count = 0;
 
 	// map in the new free page table
-	sPages = (vm_page *)vm_alloc_from_kernel_args(args, sNumPages * sizeof(vm_page),
-		B_KERNEL_READ_AREA | B_KERNEL_WRITE_AREA);
+	sPages = (vm_page *)vm_allocate_early(args, sNumPages * sizeof(vm_page),
+		~0L, B_KERNEL_READ_AREA | B_KERNEL_WRITE_AREA);
 
 	TRACE((&quot;vm_init: putting free_page_table @ %p, # ents %d (size 0x%x)\n&quot;,
 		sPages, sNumPages, (unsigned int)(sNumPages * sizeof(vm_page))));
@@ -453,12 +458,12 @@
 }
 
 
-/**	This is a background thread that wakes up every now and then (every 100ms)
- *	and moves some pages from the free queue over to the clear queue.
- *	Given enough time, it will clear out all pages from the free queue - we
- *	could probably slow it down after having reached a certain threshold.
- */
-
+/*!
+	This is a background thread that wakes up every now and then (every 100ms)
+	and moves some pages from the free queue over to the clear queue.
+	Given enough time, it will clear out all pages from the free queue - we
+	could probably slow it down after having reached a certain threshold.
+*/
 static int32
 page_scrubber(void *unused)
 {
@@ -696,12 +701,12 @@
 }
 
 
-/**	Allocates a number of pages and puts their pointers into the provided
- *	array. All pages are marked busy.
- *	Returns B_OK on success, and B_NO_MEMORY when there aren't any free
- *	pages left to allocate.
- */
-
+/*!
+	Allocates a number of pages and puts their pointers into the provided
+	array. All pages are marked busy.
+	Returns B_OK on success, and B_NO_MEMORY when there aren't any free
+	pages left to allocate.
+*/
 status_t
 vm_page_allocate_pages(int pageState, vm_page **pages, uint32 numPages)
 {
@@ -1048,125 +1053,3 @@
 }
 #endif
 
-
-addr_t
-vm_alloc_virtual_from_kernel_args(kernel_args *ka, size_t size)
-{
-	addr_t spot = 0;
-	uint32 i;
-	int last_valloc_entry = 0;
-
-	size = PAGE_ALIGN(size);
-	// find a slot in the virtual allocation addr range
-	for (i = 1; i &lt; ka-&gt;num_virtual_allocated_ranges; i++) {
-		addr_t previousRangeEnd = ka-&gt;virtual_allocated_range[i-1].start
-			+ ka-&gt;virtual_allocated_range[i-1].size;
-		last_valloc_entry = i;
-		// check to see if the space between this one and the last is big enough
-		if (previousRangeEnd &gt;= KERNEL_BASE
-			&amp;&amp; ka-&gt;virtual_allocated_range[i].start
-				- previousRangeEnd &gt;= size) {
-			spot = previousRangeEnd;
-			ka-&gt;virtual_allocated_range[i-1].size += size;
-			goto out;
-		}
-	}
-	if (spot == 0) {
-		// we hadn't found one between allocation ranges. this is ok.
-		// see if there's a gap after the last one
-		addr_t lastRangeEnd
-			= ka-&gt;virtual_allocated_range[last_valloc_entry].start 
-				+ ka-&gt;virtual_allocated_range[last_valloc_entry].size;
-		if (KERNEL_BASE + (KERNEL_SIZE - 1) - lastRangeEnd &gt;= size) {
-			spot = lastRangeEnd;
-			ka-&gt;virtual_allocated_range[last_valloc_entry].size += size;
-			goto out;
-		}
-		// see if there's a gap before the first one
-		if (ka-&gt;virtual_allocated_range[0].start &gt; KERNEL_BASE) {
-			if (ka-&gt;virtual_allocated_range[0].start - KERNEL_BASE &gt;= size) {
-				ka-&gt;virtual_allocated_range[0].start -= size;
-				spot = ka-&gt;virtual_allocated_range[0].start;
-				goto out;
-			}
-		}
-	}
-
-out:
-	return spot;
-}
-
-
-static bool
-is_page_in_phys_range(kernel_args *ka, addr_t paddr)
-{
-	// XXX horrible brute-force method of determining if the page can be allocated
-	unsigned int i;
-
-	for (i = 0; i &lt; ka-&gt;num_physical_memory_ranges; i++) {
-		if (paddr &gt;= ka-&gt;physical_memory_range[i].start 
-			&amp;&amp; paddr &lt; ka-&gt;physical_memory_range[i].start 
-						+ ka-&gt;physical_memory_range[i].size) {
-			return true;
-		}
-	}
-	return false;
-}
-
-
-static addr_t
-vm_alloc_physical_page_from_kernel_args(kernel_args *ka)
-{
-	uint32 i;
-
-	for (i = 0; i &lt; ka-&gt;num_physical_allocated_ranges; i++) {
-		addr_t next_page;
-
-		next_page = ka-&gt;physical_allocated_range[i].start
-			+ ka-&gt;physical_allocated_range[i].size;
-		// see if the page after the next allocated paddr run can be allocated
-		if (i + 1 &lt; ka-&gt;num_physical_allocated_ranges
-			&amp;&amp; ka-&gt;physical_allocated_range[i+1].size != 0) {
-			// see if the next page will collide with the next allocated range
-			if (next_page &gt;= ka-&gt;physical_allocated_range[i+1].start)
-				continue;
-		}
-		// see if the next physical page fits in the memory block
-		if (is_page_in_phys_range(ka, next_page)) {
-			// we got one!
-			ka-&gt;physical_allocated_range[i].size += B_PAGE_SIZE;
-			return (next_page / B_PAGE_SIZE);
-		}
-	}
-
-	return 0;	// could not allocate a block
-}
-
-
-/**	This one uses the kernel_args' physical and virtual memory ranges to
- *	allocate some pages before the VM is completely up.
- */
-
-addr_t
-vm_alloc_from_kernel_args(kernel_args *args, size_t size, uint32 lock)
-{
-	addr_t virtualBase, physicalAddress;
-	uint32 i;
-
-	// find the vaddr to allocate at
-	virtualBase = vm_alloc_virtual_from_kernel_args(args, size);
-	//dprintf(&quot;alloc_from_ka_struct: vaddr 0x%lx\n&quot;, virtualAddress);
-
-	// map the pages
-	for (i = 0; i &lt; PAGE_ALIGN(size) / B_PAGE_SIZE; i++) {
-		physicalAddress = vm_alloc_physical_page_from_kernel_args(args);
-		//dprintf(&quot;alloc_from_ka_struct: paddr 0x%lx\n&quot;, physicalAddress);
-
-		if (physicalAddress == 0)
-			panic(&quot;error allocating page from ka_struct!\n&quot;);
-		arch_vm_translation_map_early_map(args, virtualBase + i * B_PAGE_SIZE,
-			physicalAddress * B_PAGE_SIZE, lock, &amp;vm_alloc_physical_page_from_kernel_args);
-	}
-
-	return virtualBase;
-}


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000934.html">[Haiku-commits] r20243 - haiku/trunk/src/preferences/backgrounds
</A></li>
	<LI>Next message: <A HREF="000947.html">[Haiku-commits] r20244 - in haiku/trunk: headers/private/kernel	src/system/kernel/arch/generic src/system/kernel/arch/x86	src/system/kernel/vm
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#936">[ date ]</a>
              <a href="thread.html#936">[ thread ]</a>
              <a href="subject.html#936">[ subject ]</a>
              <a href="author.html#936">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/haiku-commits">More information about the Haiku-commits
mailing list</a><br>
</body></html>
