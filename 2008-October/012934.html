<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Haiku-commits] r28273 - haiku/trunk/src/system/kernel/arch/x86
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/haiku-commits/2008-October/index.html" >
   <LINK REL="made" HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r28273%20-%20haiku/trunk/src/system/kernel/arch/x86&In-Reply-To=%3C200810212148.m9LLmuvG020610%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="012933.html">
   <LINK REL="Next"  HREF="012935.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Haiku-commits] r28273 - haiku/trunk/src/system/kernel/arch/x86</H1>
    <B>bonefish at BerliOS</B> 
    <A HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r28273%20-%20haiku/trunk/src/system/kernel/arch/x86&In-Reply-To=%3C200810212148.m9LLmuvG020610%40sheep.berlios.de%3E"
       TITLE="[Haiku-commits] r28273 - haiku/trunk/src/system/kernel/arch/x86">bonefish at mail.berlios.de
       </A><BR>
    <I>Tue Oct 21 23:48:56 CEST 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="012933.html">[Haiku-commits] r28272 - haiku/trunk/src/apps/magnify
</A></li>
        <LI>Next message: <A HREF="012935.html">[Haiku-commits] r28274 - haiku/trunk/src/kits/tracker
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#12934">[ date ]</a>
              <a href="thread.html#12934">[ thread ]</a>
              <a href="subject.html#12934">[ subject ]</a>
              <a href="author.html#12934">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: bonefish
Date: 2008-10-21 23:48:56 +0200 (Tue, 21 Oct 2008)
New Revision: 28273
ViewCVS: <A HREF="http://svn.berlios.de/viewcvs/haiku?rev=28273&amp;view=rev">http://svn.berlios.de/viewcvs/haiku?rev=28273&amp;view=rev</A>

Modified:
   haiku/trunk/src/system/kernel/arch/x86/arch_vm_translation_map.cpp
Log:
Style changes:
* Renamed static variables.
* Enforced 80 columns limit.


Modified: haiku/trunk/src/system/kernel/arch/x86/arch_vm_translation_map.cpp
===================================================================
--- haiku/trunk/src/system/kernel/arch/x86/arch_vm_translation_map.cpp	2008-10-21 21:37:31 UTC (rev 28272)
+++ haiku/trunk/src/system/kernel/arch/x86/arch_vm_translation_map.cpp	2008-10-21 21:48:56 UTC (rev 28273)
@@ -36,21 +36,23 @@
 #	define TRACE(x) ;
 #endif
 
-static page_table_entry *page_hole = NULL;
-static page_directory_entry *page_hole_pgdir = NULL;
+static page_table_entry *sPageHole = NULL;
+static page_directory_entry *sPageHolePageDir = NULL;
 static page_directory_entry *sKernelPhysicalPageDirectory = NULL;
 static page_directory_entry *sKernelVirtualPageDirectory = NULL;
 
-static vm_translation_map *tmap_list;
-static spinlock tmap_list_lock;
+static vm_translation_map *sTMapList;
+static spinlock sTMapListLock;
 
 #define CHATTY_TMAP 0
 
 #define FIRST_USER_PGDIR_ENT    (VADDR_TO_PDENT(USER_BASE))
-#define NUM_USER_PGDIR_ENTS     (VADDR_TO_PDENT(ROUNDUP(USER_SIZE, B_PAGE_SIZE * 1024)))
+#define NUM_USER_PGDIR_ENTS     (VADDR_TO_PDENT(ROUNDUP(USER_SIZE, \
+									B_PAGE_SIZE * 1024)))
 #define FIRST_KERNEL_PGDIR_ENT  (VADDR_TO_PDENT(KERNEL_BASE))
 #define NUM_KERNEL_PGDIR_ENTS   (VADDR_TO_PDENT(KERNEL_SIZE))
-#define IS_KERNEL_MAP(map)		(map-&gt;arch_data-&gt;pgdir_phys == sKernelPhysicalPageDirectory)
+#define IS_KERNEL_MAP(map)		(map-&gt;arch_data-&gt;pgdir_phys \
+									== sKernelPhysicalPageDirectory)
 
 static status_t early_query(addr_t va, addr_t *out_physical);
 
@@ -70,12 +72,12 @@
 	vm_translation_map *entry;
 	unsigned int state = disable_interrupts();
 
-	acquire_spinlock(&amp;tmap_list_lock);
+	acquire_spinlock(&amp;sTMapListLock);
 
-	for(entry = tmap_list; entry != NULL; entry = entry-&gt;next)
+	for(entry = sTMapList; entry != NULL; entry = entry-&gt;next)
 		entry-&gt;arch_data-&gt;pgdir_virt[index] = e;
 
-	release_spinlock(&amp;tmap_list_lock);
+	release_spinlock(&amp;sTMapListLock);
 	restore_interrupts(state);
 }
 
@@ -87,12 +89,12 @@
 {
 	page_table_entry *pentry;
 
-	if (page_hole_pgdir[VADDR_TO_PDENT(va)].present == 0) {
+	if (sPageHolePageDir[VADDR_TO_PDENT(va)].present == 0) {
 		// no pagetable here
 		return B_ERROR;
 	}
 
-	pentry = page_hole + va / B_PAGE_SIZE;
+	pentry = sPageHole + va / B_PAGE_SIZE;
 	if (pentry-&gt;present == 0) {
 		// page mapping not valid
 		return B_ERROR;
@@ -154,16 +156,16 @@
 
 	// remove it from the tmap list
 	state = disable_interrupts();
-	acquire_spinlock(&amp;tmap_list_lock);
+	acquire_spinlock(&amp;sTMapListLock);
 
 	// TODO: How about using a doubly linked list?
-	entry = tmap_list;
+	entry = sTMapList;
 	while (entry != NULL) {
 		if (entry == map) {
 			if (last != NULL)
 				last-&gt;next = entry-&gt;next;
 			else
-				tmap_list = entry-&gt;next;
+				sTMapList = entry-&gt;next;
 
 			break;
 		}
@@ -171,7 +173,7 @@
 		entry = entry-&gt;next;
 	}
 
-	release_spinlock(&amp;tmap_list_lock);
+	release_spinlock(&amp;sTMapListLock);
 	restore_interrupts(state);
 
 	if (map-&gt;arch_data-&gt;page_mapper != NULL)
@@ -179,7 +181,8 @@
 
 	if (map-&gt;arch_data-&gt;pgdir_virt != NULL) {
 		// cycle through and free all of the user space pgtables
-		for (i = VADDR_TO_PDENT(USER_BASE); i &lt;= VADDR_TO_PDENT(USER_BASE + (USER_SIZE - 1)); i++) {
+		for (i = VADDR_TO_PDENT(USER_BASE);
+				i &lt;= VADDR_TO_PDENT(USER_BASE + (USER_SIZE - 1)); i++) {
 			addr_t pgtable_addr;
 			vm_page *page;
 
@@ -299,7 +302,8 @@
 
 		// put it in the pgdir
 		x86_put_pgtable_in_pgdir(&amp;pd[index], pgtable, attributes
-			| (attributes &amp; B_USER_PROTECTION ? B_WRITE_AREA : B_KERNEL_WRITE_AREA));
+			| ((attributes &amp; B_USER_PROTECTION) != 0
+					? B_WRITE_AREA : B_KERNEL_WRITE_AREA));
 
 		// update any other page directories, if it maps kernel space
 		if (index &gt;= FIRST_KERNEL_PGDIR_ENT
@@ -322,8 +326,10 @@
 
 	pinner.Unlock();
 
-	if (map-&gt;arch_data-&gt;num_invalidate_pages &lt; PAGE_INVALIDATE_CACHE_SIZE)
-		map-&gt;arch_data-&gt;pages_to_invalidate[map-&gt;arch_data-&gt;num_invalidate_pages] = va;
+	if (map-&gt;arch_data-&gt;num_invalidate_pages &lt; PAGE_INVALIDATE_CACHE_SIZE) {
+		map-&gt;arch_data-&gt;pages_to_invalidate[
+			map-&gt;arch_data-&gt;num_invalidate_pages] = va;
+	}
 
 	map-&gt;arch_data-&gt;num_invalidate_pages++;
 
@@ -374,8 +380,10 @@
 		pt[index].present = 0;
 		map-&gt;map_count--;
 
-		if (map-&gt;arch_data-&gt;num_invalidate_pages &lt; PAGE_INVALIDATE_CACHE_SIZE)
-			map-&gt;arch_data-&gt;pages_to_invalidate[map-&gt;arch_data-&gt;num_invalidate_pages] = start;
+		if (map-&gt;arch_data-&gt;num_invalidate_pages &lt; PAGE_INVALIDATE_CACHE_SIZE) {
+			map-&gt;arch_data-&gt;pages_to_invalidate[
+				map-&gt;arch_data-&gt;num_invalidate_pages] = start;
+		}
 
 		map-&gt;arch_data-&gt;num_invalidate_pages++;
 	}
@@ -420,7 +428,8 @@
 
 
 static status_t
-query_tmap(vm_translation_map *map, addr_t va, addr_t *_physical, uint32 *_flags)
+query_tmap(vm_translation_map *map, addr_t va, addr_t *_physical,
+	uint32 *_flags)
 {
 	page_table_entry *pt;
 	page_directory_entry *pd = map-&gt;arch_data-&gt;pgdir_virt;
@@ -470,7 +479,8 @@
 
 
 static status_t
-protect_tmap(vm_translation_map *map, addr_t start, addr_t end, uint32 attributes)
+protect_tmap(vm_translation_map *map, addr_t start, addr_t end,
+	uint32 attributes)
 {
 	page_table_entry *pt;
 	page_directory_entry *pd = map-&gt;arch_data-&gt;pgdir_virt;
@@ -479,7 +489,8 @@
 	start = ROUNDOWN(start, B_PAGE_SIZE);
 	end = ROUNDUP(end, B_PAGE_SIZE);
 
-	TRACE((&quot;protect_tmap: pages 0x%lx to 0x%lx, attributes %lx\n&quot;, start, end, attributes));
+	TRACE((&quot;protect_tmap: pages 0x%lx to 0x%lx, attributes %lx\n&quot;, start, end,
+		attributes));
 
 restart:
 	if (start &gt;= end)
@@ -498,7 +509,8 @@
 	pt = map-&gt;arch_data-&gt;page_mapper-&gt;GetPageTableAt(
 		ADDR_REVERSE_SHIFT(pd[index].addr));
 
-	for (index = VADDR_TO_PTENT(start); index &lt; 1024 &amp;&amp; start &lt; end; index++, start += B_PAGE_SIZE) {
+	for (index = VADDR_TO_PTENT(start); index &lt; 1024 &amp;&amp; start &lt; end;
+			index++, start += B_PAGE_SIZE) {
 		if (pt[index].present == 0) {
 			// page mapping not valid
 			continue;
@@ -512,8 +524,10 @@
 		else
 			pt[index].rw = (attributes &amp; B_KERNEL_WRITE_AREA) != 0;
 
-		if (map-&gt;arch_data-&gt;num_invalidate_pages &lt; PAGE_INVALIDATE_CACHE_SIZE)
-			map-&gt;arch_data-&gt;pages_to_invalidate[map-&gt;arch_data-&gt;num_invalidate_pages] = start;
+		if (map-&gt;arch_data-&gt;num_invalidate_pages &lt; PAGE_INVALIDATE_CACHE_SIZE) {
+			map-&gt;arch_data-&gt;pages_to_invalidate[
+				map-&gt;arch_data-&gt;num_invalidate_pages] = start;
+		}
 
 		map-&gt;arch_data-&gt;num_invalidate_pages++;
 	}
@@ -558,8 +572,10 @@
 	pinner.Unlock();
 
 	if (tlb_flush) {
-		if (map-&gt;arch_data-&gt;num_invalidate_pages &lt; PAGE_INVALIDATE_CACHE_SIZE)
-			map-&gt;arch_data-&gt;pages_to_invalidate[map-&gt;arch_data-&gt;num_invalidate_pages] = va;
+		if (map-&gt;arch_data-&gt;num_invalidate_pages &lt; PAGE_INVALIDATE_CACHE_SIZE) {
+			map-&gt;arch_data-&gt;pages_to_invalidate[
+				map-&gt;arch_data-&gt;num_invalidate_pages] = va;
+		}
 
 		map-&gt;arch_data-&gt;num_invalidate_pages++;
 	}
@@ -665,7 +681,7 @@
 				i++, virtualTable += B_PAGE_SIZE) {
 			addr_t physicalTable;
 			early_query(virtualTable, &amp;physicalTable);
-			page_directory_entry* entry = &amp;page_hole_pgdir[
+			page_directory_entry* entry = &amp;sPageHolePageDir[
 				(address / (B_PAGE_SIZE * 1024)) + i];
 			x86_put_pgtable_in_pgdir(entry, physicalTable,
 				B_KERNEL_READ_AREA | B_KERNEL_WRITE_AREA);
@@ -694,7 +710,8 @@
 	CObjectDeleter&lt;recursive_lock&gt; lockDeleter(&amp;map-&gt;lock,
 		&amp;recursive_lock_destroy);
 
-	map-&gt;arch_data = (vm_translation_map_arch_info *)malloc(sizeof(vm_translation_map_arch_info));
+	map-&gt;arch_data = (vm_translation_map_arch_info*)
+		malloc(sizeof(vm_translation_map_arch_info));
 	if (map-&gt;arch_data == NULL)
 		return B_NO_MEMORY;
 	MemoryDeleter archInfoDeleter(map-&gt;arch_data);
@@ -720,7 +737,8 @@
 			return B_NO_MEMORY;
 		}
 		vm_get_page_mapping(vm_kernel_address_space_id(),
-			(addr_t)map-&gt;arch_data-&gt;pgdir_virt, (addr_t *)&amp;map-&gt;arch_data-&gt;pgdir_phys);
+			(addr_t)map-&gt;arch_data-&gt;pgdir_virt,
+			(addr_t*)&amp;map-&gt;arch_data-&gt;pgdir_phys);
 	} else {
 		// kernel
 		// get the physical page mapper
@@ -738,17 +756,17 @@
 	// insert this new map into the map list
 	{
 		int state = disable_interrupts();
-		acquire_spinlock(&amp;tmap_list_lock);
+		acquire_spinlock(&amp;sTMapListLock);
 
 		// copy the top portion of the pgdir from the current one
 		memcpy(map-&gt;arch_data-&gt;pgdir_virt + FIRST_KERNEL_PGDIR_ENT,
 			sKernelVirtualPageDirectory + FIRST_KERNEL_PGDIR_ENT,
 			NUM_KERNEL_PGDIR_ENTS * sizeof(page_directory_entry));
 
-		map-&gt;next = tmap_list;
-		tmap_list = map;
+		map-&gt;next = sTMapList;
+		sTMapList = map;
 
-		release_spinlock(&amp;tmap_list_lock);
+		release_spinlock(&amp;sTMapListLock);
 		restore_interrupts(state);
 	}
 
@@ -772,24 +790,30 @@
 	TRACE((&quot;vm_translation_map_init: entry\n&quot;));
 
 	// page hole set up in stage2
-	page_hole = (page_table_entry *)args-&gt;arch_args.page_hole;
+	sPageHole = (page_table_entry *)args-&gt;arch_args.page_hole;
 	// calculate where the pgdir would be
-	page_hole_pgdir = (page_directory_entry *)(((unsigned int)args-&gt;arch_args.page_hole) + (B_PAGE_SIZE * 1024 - B_PAGE_SIZE));
+	sPageHolePageDir = (page_directory_entry*)
+		(((addr_t)args-&gt;arch_args.page_hole)
+			+ (B_PAGE_SIZE * 1024 - B_PAGE_SIZE));
 	// clear out the bottom 2 GB, unmap everything
-	memset(page_hole_pgdir + FIRST_USER_PGDIR_ENT, 0, sizeof(page_directory_entry) * NUM_USER_PGDIR_ENTS);
+	memset(sPageHolePageDir + FIRST_USER_PGDIR_ENT, 0,
+		sizeof(page_directory_entry) * NUM_USER_PGDIR_ENTS);
 
-	sKernelPhysicalPageDirectory = (page_directory_entry *)args-&gt;arch_args.phys_pgdir;
-	sKernelVirtualPageDirectory = (page_directory_entry *)args-&gt;arch_args.vir_pgdir;
+	sKernelPhysicalPageDirectory = (page_directory_entry*)
+		args-&gt;arch_args.phys_pgdir;
+	sKernelVirtualPageDirectory = (page_directory_entry*)
+		args-&gt;arch_args.vir_pgdir;
 
-	B_INITIALIZE_SPINLOCK(&amp;tmap_list_lock);
-	tmap_list = NULL;
+	B_INITIALIZE_SPINLOCK(&amp;sTMapListLock);
+	sTMapList = NULL;
 
 // TODO: Select the best page mapper!
 	large_memory_physical_page_ops_init(args, &amp;tmap_ops);
 
 	// enable global page feature if available
 	if (x86_check_feature(IA32_FEATURE_PGE, FEATURE_COMMON)) {
-		// this prevents kernel pages from being flushed from TLB on context-switch
+		// this prevents kernel pages from being flushed from TLB on
+		// context-switch
 		x86_write_cr4(x86_read_cr4() | IA32_CR4_GLOBAL_PAGES);
 	}
 
@@ -819,8 +843,8 @@
 
 	// unmap the page hole hack we were using before
 	sKernelVirtualPageDirectory[1023].present = 0;
-	page_hole_pgdir = NULL;
-	page_hole = NULL;
+	sPageHolePageDir = NULL;
+	sPageHole = NULL;
 
 	temp = (void *)sKernelVirtualPageDirectory;
 	area = create_area(&quot;kernel_pgdir&quot;, &amp;temp, B_EXACT_ADDRESS, B_PAGE_SIZE,
@@ -837,11 +861,13 @@
 }
 
 
-// XXX horrible back door to map a page quickly regardless of translation map object, etc.
+// XXX horrible back door to map a page quickly regardless of translation map
+// object, etc.
 // used only during VM setup.
-// uses a 'page hole' set up in the stage 2 bootloader. The page hole is created by pointing one of
-// the pgdir entries back at itself, effectively mapping the contents of all of the 4MB of pagetables
-// into a 4 MB region. It's only used here, and is later unmapped.
+// uses a 'page hole' set up in the stage 2 bootloader. The page hole is created
+// by pointing one of the pgdir entries back at itself, effectively mapping the
+// contents of all of the 4MB of pagetables into a 4 MB region. It's only used
+// here, and is later unmapped.
 
 status_t
 arch_vm_translation_map_early_map(kernel_args *args, addr_t va, addr_t pa,
@@ -853,7 +879,7 @@
 
 	// check to see if a page table exists for this range
 	index = VADDR_TO_PDENT(va);
-	if (page_hole_pgdir[index].present == 0) {
+	if (sPageHolePageDir[index].present == 0) {
 		addr_t pgtable;
 		page_directory_entry *e;
 		// we need to allocate a pgtable
@@ -864,16 +890,17 @@
 		TRACE((&quot;early_map: asked for free page for pgtable. 0x%lx\n&quot;, pgtable));
 
 		// put it in the pgdir
-		e = &amp;page_hole_pgdir[index];
+		e = &amp;sPageHolePageDir[index];
 		x86_put_pgtable_in_pgdir(e, pgtable, attributes);
 
 		// zero it out in it's new mapping
-		memset((unsigned int *)((unsigned int)page_hole + (va / B_PAGE_SIZE / 1024) * B_PAGE_SIZE), 0, B_PAGE_SIZE);
+		memset((unsigned int*)((addr_t)sPageHole
+			+ (va / B_PAGE_SIZE / 1024) * B_PAGE_SIZE), 0, B_PAGE_SIZE);
 	}
 
 	// now, fill in the pentry
-	put_page_table_entry_in_pgtable(page_hole + va / B_PAGE_SIZE, pa, attributes,
-		IS_KERNEL_ADDRESS(va));
+	put_page_table_entry_in_pgtable(sPageHole + va / B_PAGE_SIZE, pa,
+		attributes, IS_KERNEL_ADDRESS(va));
 
 	arch_cpu_invalidate_TLB_range(va, va);
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="012933.html">[Haiku-commits] r28272 - haiku/trunk/src/apps/magnify
</A></li>
	<LI>Next message: <A HREF="012935.html">[Haiku-commits] r28274 - haiku/trunk/src/kits/tracker
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#12934">[ date ]</a>
              <a href="thread.html#12934">[ thread ]</a>
              <a href="subject.html#12934">[ subject ]</a>
              <a href="author.html#12934">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/haiku-commits">More information about the Haiku-commits
mailing list</a><br>
</body></html>
