<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Haiku-commits] r28253 - in haiku/trunk/src/system/kernel: .	scheduler
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/haiku-commits/2008-October/index.html" >
   <LINK REL="made" HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r28253%20-%20in%20haiku/trunk/src/system/kernel%3A%20.%0A%09scheduler&In-Reply-To=%3C200810202257.m9KMvfue000179%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="012907.html">
   <LINK REL="Next"  HREF="012904.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Haiku-commits] r28253 - in haiku/trunk/src/system/kernel: .	scheduler</H1>
    <B>bonefish at BerliOS</B> 
    <A HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r28253%20-%20in%20haiku/trunk/src/system/kernel%3A%20.%0A%09scheduler&In-Reply-To=%3C200810202257.m9KMvfue000179%40sheep.berlios.de%3E"
       TITLE="[Haiku-commits] r28253 - in haiku/trunk/src/system/kernel: .	scheduler">bonefish at mail.berlios.de
       </A><BR>
    <I>Tue Oct 21 00:57:41 CEST 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="012907.html">[Haiku-commits] r28252 - haiku/trunk/src/tools/vmdkimage
</A></li>
        <LI>Next message: <A HREF="012904.html">[Haiku-commits] r28254 - haiku/trunk/src/system/kernel/scheduler
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#12903">[ date ]</a>
              <a href="thread.html#12903">[ thread ]</a>
              <a href="subject.html#12903">[ subject ]</a>
              <a href="author.html#12903">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: bonefish
Date: 2008-10-21 00:57:40 +0200 (Tue, 21 Oct 2008)
New Revision: 28253
ViewCVS: <A HREF="http://svn.berlios.de/viewcvs/haiku?rev=28253&amp;view=rev">http://svn.berlios.de/viewcvs/haiku?rev=28253&amp;view=rev</A>

Added:
   haiku/trunk/src/system/kernel/scheduler/
   haiku/trunk/src/system/kernel/scheduler/scheduler.cpp
   haiku/trunk/src/system/kernel/scheduler/scheduler_tracing.cpp
   haiku/trunk/src/system/kernel/scheduler/scheduler_tracing.h
   haiku/trunk/src/system/kernel/scheduler/scheduling_analysis.cpp
Removed:
   haiku/trunk/src/system/kernel/scheduler.cpp
Modified:
   haiku/trunk/src/system/kernel/Jamfile
Log:
Moved scheduler.cpp into new subdirectory scheduler/. Moved the
scheduler tracing and scheduler analysis code into separate source
files.


Modified: haiku/trunk/src/system/kernel/Jamfile
===================================================================
--- haiku/trunk/src/system/kernel/Jamfile	2008-10-20 21:41:30 UTC (rev 28252)
+++ haiku/trunk/src/system/kernel/Jamfile	2008-10-20 22:57:40 UTC (rev 28253)
@@ -10,6 +10,8 @@
 	SubDirC++Flags $(defines) ;
 }
 
+SEARCH_SOURCE += [ FDirName $(SUBDIR) scheduler ] ;
+
 UsePrivateHeaders libroot ;
 UsePrivateHeaders shared ;
 UsePrivateHeaders runtime_loader ;
@@ -35,7 +37,6 @@
 	Notifications.cpp
 	port.cpp
 	real_time_clock.c
-	scheduler.cpp
 	sem.cpp
 	shutdown.c
 	signal.cpp
@@ -48,6 +49,11 @@
 	usergroup.cpp
 	wait_for_objects.cpp
 
+	# scheduler
+	scheduler.cpp
+	scheduler_tracing.cpp
+	scheduling_analysis.cpp
+
 	: $(TARGET_KERNEL_PIC_CCFLAGS)
 ;
 

Copied: haiku/trunk/src/system/kernel/scheduler/scheduler.cpp (from rev 28242, haiku/trunk/src/system/kernel/scheduler.cpp)
===================================================================
--- haiku/trunk/src/system/kernel/scheduler.cpp	2008-10-19 15:59:23 UTC (rev 28242)
+++ haiku/trunk/src/system/kernel/scheduler/scheduler.cpp	2008-10-20 22:57:40 UTC (rev 28253)
@@ -0,0 +1,393 @@
+/*
+ * Copyright 2008, Ingo Weinhold, <A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">ingo_weinhold at gmx.de.</A>
+ * Copyright 2002-2007, Axel D&#246;rfler, <A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">axeld at pinc-software.de.</A>
+ * Copyright 2002, Angelo Mottola, <A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">a.mottola at libero.it.</A>
+ * Distributed under the terms of the MIT License.
+ *
+ * Copyright 2001-2002, Travis Geiselbrecht. All rights reserved.
+ * Distributed under the terms of the NewOS License.
+ */
+
+/*! The thread scheduler */
+
+
+#include &lt;OS.h&gt;
+
+#include &lt;cpu.h&gt;
+#include &lt;int.h&gt;
+#include &lt;kernel.h&gt;
+#include &lt;kscheduler.h&gt;
+#include &lt;scheduler_defs.h&gt;
+#include &lt;smp.h&gt;
+#include &lt;thread.h&gt;
+#include &lt;timer.h&gt;
+#include &lt;user_debugger.h&gt;
+
+#include &quot;scheduler_tracing.h&quot;
+
+
+//#define TRACE_SCHEDULER
+#ifdef TRACE_SCHEDULER
+#	define TRACE(x) dprintf x
+#else
+#	define TRACE(x) ;
+#endif
+
+
+// The run queue. Holds the threads ready to run ordered by priority.
+static struct thread *sRunQueue = NULL;
+static cpu_mask_t sIdleCPUs = 0;
+
+
+static int
+_rand(void)
+{
+	static int next = 0;
+
+	if (next == 0)
+		next = system_time();
+
+	next = next * 1103515245 + 12345;
+	return (next &gt;&gt; 16) &amp; 0x7FFF;
+}
+
+
+static int
+dump_run_queue(int argc, char **argv)
+{
+	struct thread *thread;
+
+	thread = sRunQueue;
+	if (!thread)
+		kprintf(&quot;Run queue is empty!\n&quot;);
+	else {
+		kprintf(&quot;thread    id      priority name\n&quot;);
+		while (thread) {
+			kprintf(&quot;%p  %-7ld %-8ld %s\n&quot;, thread, thread-&gt;id,
+				thread-&gt;priority, thread-&gt;name);
+			thread = thread-&gt;queue_next;
+		}
+	}
+
+	return 0;
+}
+
+
+/*!	Enqueues the thread into the run queue.
+	Note: thread lock must be held when entering this function
+*/
+void
+scheduler_enqueue_in_run_queue(struct thread *thread)
+{
+	if (thread-&gt;state == B_THREAD_RUNNING) {
+		// The thread is currently running (on another CPU) and we cannot
+		// insert it into the run queue. Set the next state to ready so the
+		// thread is inserted into the run queue on the next reschedule.
+		thread-&gt;next_state = B_THREAD_READY;
+		return;
+	}
+
+	thread-&gt;state = thread-&gt;next_state = B_THREAD_READY;
+
+	struct thread *curr, *prev;
+	for (curr = sRunQueue, prev = NULL; curr
+			&amp;&amp; curr-&gt;priority &gt;= thread-&gt;next_priority;
+			curr = curr-&gt;queue_next) {
+		if (prev)
+			prev = prev-&gt;queue_next;
+		else
+			prev = sRunQueue;
+	}
+
+	T(EnqueueThread(thread, prev, curr));
+
+	thread-&gt;queue_next = curr;
+	if (prev)
+		prev-&gt;queue_next = thread;
+	else
+		sRunQueue = thread;
+
+	thread-&gt;next_priority = thread-&gt;priority;
+
+	if (thread-&gt;priority != B_IDLE_PRIORITY) {
+		int32 currentCPU = smp_get_current_cpu();
+		if (sIdleCPUs != 0) {
+			if (thread-&gt;pinned_to_cpu &gt; 0) {
+				// thread is pinned to a CPU -- notify it, if it is idle
+				int32 targetCPU = thread-&gt;previous_cpu-&gt;cpu_num;
+				if ((sIdleCPUs &amp; (1 &lt;&lt; targetCPU)) != 0) {
+					sIdleCPUs &amp;= ~(1 &lt;&lt; targetCPU);
+					smp_send_ici(targetCPU, SMP_MSG_RESCHEDULE_IF_IDLE, 0, 0,
+						0, NULL, SMP_MSG_FLAG_ASYNC);
+				}
+			} else {
+				// Thread is not pinned to any CPU -- take it ourselves, if we
+				// are idle, otherwise notify the next idle CPU. In either case
+				// we clear the idle bit of the chosen CPU, so that the
+				// scheduler_enqueue_in_run_queue() won't try to bother the
+				// same CPU again, if invoked before it handled the interrupt.
+				cpu_mask_t idleCPUs = CLEAR_BIT(sIdleCPUs, currentCPU);
+				if ((sIdleCPUs &amp; (1 &lt;&lt; currentCPU)) != 0) {
+					sIdleCPUs = idleCPUs;
+				} else {
+					int32 targetCPU = 0;
+					for (; targetCPU &lt; B_MAX_CPU_COUNT; targetCPU++) {
+						cpu_mask_t mask = 1 &lt;&lt; targetCPU;
+						if ((idleCPUs &amp; mask) != 0) {
+							sIdleCPUs &amp;= ~mask;
+							break;
+						}
+					}
+
+					smp_send_ici(targetCPU, SMP_MSG_RESCHEDULE_IF_IDLE, 0, 0,
+						0, NULL, SMP_MSG_FLAG_ASYNC);
+				}
+			}
+		}
+	}
+}
+
+
+/*!	Removes a thread from the run queue.
+	Note: thread lock must be held when entering this function
+*/
+void
+scheduler_remove_from_run_queue(struct thread *thread)
+{
+	struct thread *item, *prev;
+
+	T(RemoveThread(thread));
+
+	// find thread in run queue
+	for (item = sRunQueue, prev = NULL; item &amp;&amp; item != thread;
+			item = item-&gt;queue_next) {
+		if (prev)
+			prev = prev-&gt;queue_next;
+		else
+			prev = sRunQueue;
+	}
+
+	ASSERT(item == thread);
+
+	if (prev)
+		prev-&gt;queue_next = item-&gt;queue_next;
+	else
+		sRunQueue = item-&gt;queue_next;
+}
+
+
+static void
+context_switch(struct thread *fromThread, struct thread *toThread)
+{
+	if ((fromThread-&gt;flags &amp; THREAD_FLAGS_DEBUGGER_INSTALLED) != 0)
+		user_debug_thread_unscheduled(fromThread);
+
+	toThread-&gt;previous_cpu = toThread-&gt;cpu = fromThread-&gt;cpu;
+	fromThread-&gt;cpu = NULL;
+
+	arch_thread_set_current_thread(toThread);
+	arch_thread_context_switch(fromThread, toThread);
+
+	// Looks weird, but is correct. fromThread had been unscheduled earlier,
+	// but is back now. The notification for a thread scheduled the first time
+	// happens in thread.cpp:thread_kthread_entry().
+	if ((fromThread-&gt;flags &amp; THREAD_FLAGS_DEBUGGER_INSTALLED) != 0)
+		user_debug_thread_scheduled(fromThread);
+}
+
+
+static int32
+reschedule_event(timer *unused)
+{
+	if (thread_get_current_thread()-&gt;keep_scheduled &gt; 0)
+		return B_HANDLED_INTERRUPT;
+
+	// this function is called as a result of the timer event set by the
+	// scheduler returning this causes a reschedule on the timer event
+	thread_get_current_thread()-&gt;cpu-&gt;preempted = 1;
+	return B_INVOKE_SCHEDULER;
+}
+
+
+/*!	Runs the scheduler.
+	Note: expects thread spinlock to be held
+*/
+void
+scheduler_reschedule(void)
+{
+	struct thread *oldThread = thread_get_current_thread();
+	struct thread *nextThread, *prevThread;
+
+	TRACE((&quot;reschedule(): cpu %d, cur_thread = %ld\n&quot;, smp_get_current_cpu(), thread_get_current_thread()-&gt;id));
+
+	oldThread-&gt;cpu-&gt;invoke_scheduler = false;
+
+	oldThread-&gt;state = oldThread-&gt;next_state;
+	switch (oldThread-&gt;next_state) {
+		case B_THREAD_RUNNING:
+		case B_THREAD_READY:
+			TRACE((&quot;enqueueing thread %ld into run q. pri = %ld\n&quot;, oldThread-&gt;id, oldThread-&gt;priority));
+			scheduler_enqueue_in_run_queue(oldThread);
+			break;
+		case B_THREAD_SUSPENDED:
+			TRACE((&quot;reschedule(): suspending thread %ld\n&quot;, oldThread-&gt;id));
+			break;
+		case THREAD_STATE_FREE_ON_RESCHED:
+			break;
+		default:
+			TRACE((&quot;not enqueueing thread %ld into run q. next_state = %ld\n&quot;, oldThread-&gt;id, oldThread-&gt;next_state));
+			break;
+	}
+
+	nextThread = sRunQueue;
+	prevThread = NULL;
+
+	if (oldThread-&gt;cpu-&gt;disabled) {
+		// CPU is disabled - just select an idle thread
+		while (nextThread &amp;&amp; nextThread-&gt;priority &gt; B_IDLE_PRIORITY) {
+			prevThread = nextThread;
+			nextThread = nextThread-&gt;queue_next;
+		}
+	} else {
+		while (nextThread) {
+			// select next thread from the run queue
+			while (nextThread &amp;&amp; nextThread-&gt;priority &gt; B_IDLE_PRIORITY) {
+#if 0
+				if (oldThread == nextThread &amp;&amp; nextThread-&gt;was_yielded) {
+					// ignore threads that called thread_yield() once
+					nextThread-&gt;was_yielded = false;
+					prevThread = nextThread;
+					nextThread = nextThread-&gt;queue_next;
+				}
+#endif
+
+				// skip thread, if it doesn't want to run on this CPU
+				if (nextThread-&gt;pinned_to_cpu &gt; 0
+					&amp;&amp; nextThread-&gt;previous_cpu != oldThread-&gt;cpu) {
+					prevThread = nextThread;
+					nextThread = nextThread-&gt;queue_next;
+					continue;
+				}
+
+				// always extract real time threads
+				if (nextThread-&gt;priority &gt;= B_FIRST_REAL_TIME_PRIORITY)
+					break;
+
+				// never skip last non-idle normal thread
+				if (nextThread-&gt;queue_next &amp;&amp; nextThread-&gt;queue_next-&gt;priority == B_IDLE_PRIORITY)
+					break;
+
+				// skip normal threads sometimes (roughly 20%)
+				if (_rand() &gt; 0x1a00)
+					break;
+
+				// skip until next lower priority
+				int32 priority = nextThread-&gt;priority;
+				do {
+					prevThread = nextThread;
+					nextThread = nextThread-&gt;queue_next;
+				} while (nextThread-&gt;queue_next != NULL
+					&amp;&amp; priority == nextThread-&gt;queue_next-&gt;priority
+					&amp;&amp; nextThread-&gt;queue_next-&gt;priority &gt; B_IDLE_PRIORITY);
+			}
+
+			if (nextThread-&gt;cpu
+				&amp;&amp; nextThread-&gt;cpu-&gt;cpu_num != oldThread-&gt;cpu-&gt;cpu_num) {
+				panic(&quot;thread in run queue that's still running on another CPU!\n&quot;);
+				// ToDo: remove this check completely when we're sure that this
+				// cannot happen anymore.
+				prevThread = nextThread;
+				nextThread = nextThread-&gt;queue_next;
+				continue;
+			}
+
+			break;
+		}
+	}
+
+	if (!nextThread)
+		panic(&quot;reschedule(): run queue is empty!\n&quot;);
+
+	// extract selected thread from the run queue
+	if (prevThread)
+		prevThread-&gt;queue_next = nextThread-&gt;queue_next;
+	else
+		sRunQueue = nextThread-&gt;queue_next;
+
+	T(ScheduleThread(nextThread, oldThread));
+
+	nextThread-&gt;state = B_THREAD_RUNNING;
+	nextThread-&gt;next_state = B_THREAD_READY;
+	oldThread-&gt;was_yielded = false;
+
+	// track kernel time (user time is tracked in thread_at_kernel_entry())
+	bigtime_t now = system_time();
+	oldThread-&gt;kernel_time += now - oldThread-&gt;last_time;
+	nextThread-&gt;last_time = now;
+
+	// track CPU activity
+	if (!thread_is_idle_thread(oldThread)) {
+		oldThread-&gt;cpu-&gt;active_time +=
+			(oldThread-&gt;kernel_time - oldThread-&gt;cpu-&gt;last_kernel_time)
+			+ (oldThread-&gt;user_time - oldThread-&gt;cpu-&gt;last_user_time);
+	}
+
+	if (!thread_is_idle_thread(nextThread)) {
+		oldThread-&gt;cpu-&gt;last_kernel_time = nextThread-&gt;kernel_time;
+		oldThread-&gt;cpu-&gt;last_user_time = nextThread-&gt;user_time;
+	}
+
+	if (nextThread != oldThread || oldThread-&gt;cpu-&gt;preempted) {
+		bigtime_t quantum = 3000;	// ToDo: calculate quantum!
+		timer *quantumTimer = &amp;oldThread-&gt;cpu-&gt;quantum_timer;
+
+		if (!oldThread-&gt;cpu-&gt;preempted)
+			cancel_timer(quantumTimer);
+
+		oldThread-&gt;cpu-&gt;preempted = 0;
+		add_timer(quantumTimer, &amp;reschedule_event, quantum,
+			B_ONE_SHOT_RELATIVE_TIMER | B_TIMER_ACQUIRE_THREAD_LOCK);
+
+		// update the idle bit for this CPU in the CPU mask
+		int32 cpuNum = smp_get_current_cpu();
+		if (nextThread-&gt;priority == B_IDLE_PRIORITY)
+			sIdleCPUs = SET_BIT(sIdleCPUs, cpuNum);
+		else
+			sIdleCPUs = CLEAR_BIT(sIdleCPUs, cpuNum);
+
+		if (nextThread != oldThread)
+			context_switch(oldThread, nextThread);
+	}
+}
+
+
+void
+scheduler_init(void)
+{
+	add_debugger_command_etc(&quot;run_queue&quot;, &amp;dump_run_queue,
+		&quot;List threads in run queue&quot;, &quot;\nLists threads in run queue&quot;, 0);
+
+#if SCHEDULER_TRACING
+	add_debugger_command_etc(&quot;scheduler&quot;, &amp;cmd_scheduler,
+		&quot;Analyze scheduler tracing information&quot;,
+		&quot;&lt;thread&gt;\n&quot;
+		&quot;Analyzes scheduler tracing information for a given thread.\n&quot;
+		&quot;  &lt;thread&gt;  - ID of the thread.\n&quot;, 0);
+#endif
+}
+
+
+/*!	This starts the scheduler. Must be run under the context of
+	the initial idle thread.
+*/
+void
+scheduler_start(void)
+{
+	cpu_status state = disable_interrupts();
+	GRAB_THREAD_LOCK();
+
+	scheduler_reschedule();
+
+	RELEASE_THREAD_LOCK();
+	restore_interrupts(state);
+}

Added: haiku/trunk/src/system/kernel/scheduler/scheduler_tracing.cpp
===================================================================
--- haiku/trunk/src/system/kernel/scheduler/scheduler_tracing.cpp	2008-10-20 21:41:30 UTC (rev 28252)
+++ haiku/trunk/src/system/kernel/scheduler/scheduler_tracing.cpp	2008-10-20 22:57:40 UTC (rev 28253)
@@ -0,0 +1,304 @@
+/*
+ * Copyright 2008, Ingo Weinhold, <A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">ingo_weinhold at gmx.de.</A>
+ * Copyright 2002-2007, Axel D&#246;rfler, <A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">axeld at pinc-software.de.</A>
+ * Distributed under the terms of the MIT License.
+ */
+
+#include &quot;scheduler_tracing.h&quot;
+
+#include &lt;debug.h&gt;
+
+
+#if SCHEDULER_TRACING
+
+namespace SchedulerTracing {
+
+// #pragma mark - EnqueueThread
+
+
+void
+EnqueueThread::AddDump(TraceOutput&amp; out)
+{
+	out.Print(&quot;scheduler enqueue %ld \&quot;%s\&quot;, priority %d (previous %ld, &quot;
+		&quot;next %ld)&quot;, fID, fName, fPriority, fPreviousID, fNextID);
+}
+
+
+const char*
+EnqueueThread::Name() const
+{
+	return fName;
+}
+
+
+// #pragma mark - RemoveThread
+
+
+void
+RemoveThread::AddDump(TraceOutput&amp; out)
+{
+	out.Print(&quot;scheduler remove %ld, priority %d&quot;, fID, fPriority);
+}
+
+const char*
+RemoveThread::Name() const
+{
+	return NULL;
+}
+
+
+// #pragma mark - ScheduleThread
+
+
+void
+ScheduleThread::AddDump(TraceOutput&amp; out)
+{
+	out.Print(&quot;schedule %ld \&quot;%s\&quot;, priority %d, CPU %ld, &quot;
+		&quot;previous thread: %ld (&quot;, fID, fName, fPriority, fCPU, fPreviousID);
+	if (fPreviousState == B_THREAD_WAITING) {
+		switch (fPreviousWaitObjectType) {
+			case THREAD_BLOCK_TYPE_SEMAPHORE:
+				out.Print(&quot;sem %ld&quot;, (sem_id)(addr_t)fPreviousWaitObject);
+				break;
+			case THREAD_BLOCK_TYPE_CONDITION_VARIABLE:
+				out.Print(&quot;cvar %p&quot;, fPreviousWaitObject);
+				break;
+			case THREAD_BLOCK_TYPE_SNOOZE:
+				out.Print(&quot;snooze()&quot;);
+				break;
+			case THREAD_BLOCK_TYPE_SIGNAL:
+				out.Print(&quot;signal&quot;);
+				break;
+			case THREAD_BLOCK_TYPE_MUTEX:
+				out.Print(&quot;mutex %p&quot;, fPreviousWaitObject);
+				break;
+			case THREAD_BLOCK_TYPE_RW_LOCK:
+				out.Print(&quot;rwlock %p&quot;, fPreviousWaitObject);
+				break;
+			case THREAD_BLOCK_TYPE_OTHER:
+				out.Print(&quot;other (%p)&quot;, fPreviousWaitObject);
+					// We could print the string, but it might come from a
+					// kernel module that has already been unloaded.
+				break;
+			default:
+				out.Print(&quot;unknown (%p)&quot;, fPreviousWaitObject);
+				break;
+		}
+#if SCHEDULER_TRACING &gt;= 2
+	} else if (fPreviousState == B_THREAD_READY) {
+		out.Print(&quot;ready at %p&quot;, fPreviousPC);
+#endif
+	} else
+		out.Print(&quot;%s&quot;, thread_state_to_text(NULL, fPreviousState));
+
+	out.Print(&quot;)&quot;);
+}
+
+
+const char*
+ScheduleThread::Name() const
+{
+	return fName;
+}
+
+}	// namespace SchedulerTracing
+
+
+// #pragma mark -
+
+
+int
+cmd_scheduler(int argc, char** argv)
+{
+	using namespace SchedulerTracing;
+
+	int64 threadID;
+	if (argc != 2
+		|| !evaluate_debug_expression(argv[1], (uint64*)&amp;threadID, true)) {
+		print_debugger_command_usage(argv[0]);
+		return 0;
+	}
+
+	if (threadID &lt;= 0) {
+		kprintf(&quot;Invalid thread ID: %lld\n&quot;, threadID);
+		return 0;
+	}
+
+	ScheduleState state = UNKNOWN;
+	bigtime_t lastTime = 0;
+
+	int64 runs = 0;
+	bigtime_t totalRunTime = 0;
+	bigtime_t minRunTime = -1;
+	bigtime_t maxRunTime = -1;
+
+	int64 latencies = 0;
+	bigtime_t totalLatency = 0;
+	bigtime_t minLatency = -1;
+	bigtime_t maxLatency = -1;
+	int32 maxLatencyEntry = -1;
+
+	int64 reruns = 0;
+	bigtime_t totalRerunTime = 0;
+	bigtime_t minRerunTime = -1;
+	bigtime_t maxRerunTime = -1;
+	int32 maxRerunEntry = -1;
+
+	int64 preemptions = 0;
+
+	TraceEntryIterator iterator;
+	while (TraceEntry* _entry = iterator.Next()) {
+		if (dynamic_cast&lt;SchedulerTraceEntry*&gt;(_entry) == NULL)
+			continue;
+
+		if (ScheduleThread* entry = dynamic_cast&lt;ScheduleThread*&gt;(_entry)) {
+			if (entry-&gt;ThreadID() == threadID) {
+				// thread scheduled
+				bigtime_t diffTime = entry-&gt;Time() - lastTime;
+
+				if (state == READY) {
+					// thread scheduled after having been woken up
+					latencies++;
+					totalLatency += diffTime;
+					if (minLatency &lt; 0 || diffTime &lt; minLatency)
+						minLatency = diffTime;
+					if (diffTime &gt; maxLatency) {
+						maxLatency = diffTime;
+						maxLatencyEntry = iterator.Index();
+					}
+				} else if (state == PREEMPTED) {
+					// thread scheduled after having been preempted before
+					reruns++;
+					totalRerunTime += diffTime;
+					if (minRerunTime &lt; 0 || diffTime &lt; minRerunTime)
+						minRerunTime = diffTime;
+					if (diffTime &gt; maxRerunTime) {
+						maxRerunTime = diffTime;
+						maxRerunEntry = iterator.Index();
+					}
+				}
+
+				if (state == STILL_RUNNING) {
+					// Thread was running and continues to run.
+					state = RUNNING;
+				}
+
+				if (state != RUNNING) {
+					lastTime = entry-&gt;Time();
+					state = RUNNING;
+				}
+			} else if (entry-&gt;PreviousThreadID() == threadID) {
+				// thread unscheduled
+				bigtime_t diffTime = entry-&gt;Time() - lastTime;
+
+				if (state == STILL_RUNNING) {
+					// thread preempted
+					state = PREEMPTED;
+
+					runs++;
+					preemptions++;
+					totalRunTime += diffTime;
+					if (minRunTime &lt; 0 || diffTime &lt; minRunTime)
+						minRunTime = diffTime;
+					if (diffTime &gt; maxRunTime)
+						maxRunTime = diffTime;
+				} else if (state == RUNNING) {
+					// thread starts waiting (it hadn't been added to the run
+					// queue before being unscheduled)
+					bigtime_t diffTime = entry-&gt;Time() - lastTime;
+					runs++;
+					totalRunTime += diffTime;
+					if (minRunTime &lt; 0 || diffTime &lt; minRunTime)
+						minRunTime = diffTime;
+					if (diffTime &gt; maxRunTime)
+						maxRunTime = diffTime;
+
+					state = WAITING;
+				}
+			}
+		} else if (EnqueueThread* entry
+				= dynamic_cast&lt;EnqueueThread*&gt;(_entry)) {
+			if (entry-&gt;ThreadID() != threadID)
+				continue;
+
+			// thread enqueued in run queue
+
+			if (state == RUNNING || state == STILL_RUNNING) {
+				// Thread was running and is reentered into the run queue. This
+				// is done by the scheduler, if the thread remains ready.
+				state = STILL_RUNNING;
+			} else {
+				// Thread was waiting and is ready now.
+				lastTime = entry-&gt;Time();
+				state = READY;
+			}
+		} else if (RemoveThread* entry = dynamic_cast&lt;RemoveThread*&gt;(_entry)) {
+			if (entry-&gt;ThreadID() != threadID)
+				continue;
+
+			// thread removed from run queue
+
+			// This really only happens when the thread priority is changed
+			// while the thread is ready.
+
+			if (state == RUNNING) {
+				// This should never happen.
+				bigtime_t diffTime = entry-&gt;Time() - lastTime;
+				runs++;
+				totalRunTime += diffTime;
+				if (minRunTime &lt; 0 || diffTime &lt; minRunTime)
+					minRunTime = diffTime;
+				if (diffTime &gt; maxRunTime)
+					maxRunTime = diffTime;
+			}
+
+			state = WAITING;
+		}
+	}
+
+	// print results
+	if (runs == 0) {
+		kprintf(&quot;thread %lld never ran.\n&quot;, threadID);
+		return 0;
+	}
+
+	kprintf(&quot;scheduling statistics for thread %lld:\n&quot;, threadID);
+	kprintf(&quot;runs:\n&quot;);
+	kprintf(&quot;  total #: %lld\n&quot;, runs);
+	kprintf(&quot;  total:   %lld us\n&quot;, totalRunTime);
+	kprintf(&quot;  average: %#.2f us\n&quot;, (double)totalRunTime / runs);
+	kprintf(&quot;  min:     %lld us\n&quot;, minRunTime);
+	kprintf(&quot;  max:     %lld us\n&quot;, maxRunTime);
+
+	if (latencies &gt; 0) {
+		kprintf(&quot;scheduling latency after wake up:\n&quot;);
+		kprintf(&quot;  total #: %lld\n&quot;, latencies);
+		kprintf(&quot;  total:   %lld us\n&quot;, totalLatency);
+		kprintf(&quot;  average: %#.2f us\n&quot;, (double)totalLatency / latencies);
+		kprintf(&quot;  min:     %lld us\n&quot;, minLatency);
+		kprintf(&quot;  max:     %lld us\n&quot;, maxLatency);
+		kprintf(&quot;  max:     %lld us (at tracing entry %ld)\n&quot;, maxLatency,
+			maxLatencyEntry);
+	} else
+		kprintf(&quot;thread was never run after having been woken up\n&quot;);
+
+	if (reruns &gt; 0) {
+		kprintf(&quot;scheduling latency after preemption:\n&quot;);
+		kprintf(&quot;  total #: %lld\n&quot;, reruns);
+		kprintf(&quot;  total:   %lld us\n&quot;, totalRerunTime);
+		kprintf(&quot;  average: %#.2f us\n&quot;, (double)totalRerunTime / reruns);
+		kprintf(&quot;  min:     %lld us\n&quot;, minRerunTime);
+		kprintf(&quot;  max:     %lld us (at tracing entry %ld)\n&quot;, maxRerunTime,
+			maxRerunEntry);
+	} else
+		kprintf(&quot;thread was never rerun after preemption\n&quot;);
+
+	if (preemptions &gt; 0)
+		kprintf(&quot;thread was preempted %lld times\n&quot;, preemptions);
+	else
+		kprintf(&quot;thread was never preempted\n&quot;);
+
+	return 0;
+}
+
+#endif	// SCHEDULER_TRACING

Added: haiku/trunk/src/system/kernel/scheduler/scheduler_tracing.h
===================================================================
--- haiku/trunk/src/system/kernel/scheduler/scheduler_tracing.h	2008-10-20 21:41:30 UTC (rev 28252)
+++ haiku/trunk/src/system/kernel/scheduler/scheduler_tracing.h	2008-10-20 22:57:40 UTC (rev 28253)
@@ -0,0 +1,159 @@
+/*
+ * Copyright 2008, Ingo Weinhold, <A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">ingo_weinhold at gmx.de.</A>
+ * Copyright 2002-2007, Axel D&#246;rfler, <A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">axeld at pinc-software.de.</A>
+ * Distributed under the terms of the MIT License.
+ */
+#ifndef KERNEL_SCHEDULER_TRACING_H
+#define KERNEL_SCHEDULER_TRACING_H
+
+#include &lt;arch/debug.h&gt;
+#include &lt;cpu.h&gt;
+#include &lt;thread.h&gt;
+#include &lt;tracing.h&gt;
+
+
+#if SCHEDULER_TRACING
+
+namespace SchedulerTracing {
+
+class SchedulerTraceEntry : public AbstractTraceEntry {
+public:
+	SchedulerTraceEntry(struct thread* thread)
+		:
+		fID(thread-&gt;id)
+	{
+	}
+
+	thread_id ThreadID() const	{ return fID; }
+
+	virtual const char* Name() const = 0;
+
+protected:
+	thread_id			fID;
+};
+
+
+class EnqueueThread : public SchedulerTraceEntry {
+public:
+	EnqueueThread(struct thread* thread, struct thread* previous,
+			struct thread* next)
+		:
+		SchedulerTraceEntry(thread),
+		fPreviousID(-1),
+		fNextID(-1),
+		fPriority(thread-&gt;priority)
+	{
+		if (previous != NULL)
+			fPreviousID = previous-&gt;id;
+		if (next != NULL)
+			fNextID = next-&gt;id;
+		fName = alloc_tracing_buffer_strcpy(thread-&gt;name, B_OS_NAME_LENGTH,
+			false);
+		Initialized();
+	}
+
+	virtual void AddDump(TraceOutput&amp; out);
+
+	virtual const char* Name() const;
+
+private:
+	thread_id			fPreviousID;
+	thread_id			fNextID;
+	char*				fName;
+	uint8				fPriority;
+};
+
+
+class RemoveThread : public SchedulerTraceEntry {
+public:
+	RemoveThread(struct thread* thread)
+		:
+		SchedulerTraceEntry(thread),
+		fPriority(thread-&gt;priority)
+	{
+		Initialized();
+	}
+
+	virtual void AddDump(TraceOutput&amp; out);
+
+	virtual const char* Name() const;
+
+private:
+	uint8				fPriority;
+};
+
+
+class ScheduleThread : public SchedulerTraceEntry {
+public:
+	ScheduleThread(struct thread* thread, struct thread* previous)
+		:
+		SchedulerTraceEntry(thread),
+		fPreviousID(previous-&gt;id),
+		fCPU(previous-&gt;cpu-&gt;cpu_num),
+		fPriority(thread-&gt;priority),
+		fPreviousState(previous-&gt;state),
+		fPreviousWaitObjectType(previous-&gt;wait.type)
+	{
+		fName = alloc_tracing_buffer_strcpy(thread-&gt;name, B_OS_NAME_LENGTH,
+			false);
+
+#if SCHEDULER_TRACING &gt;= 2
+		if (fPreviousState == B_THREAD_READY)
+			fPreviousPC = arch_debug_get_interrupt_pc();
+		else
+#endif
+			fPreviousWaitObject = previous-&gt;wait.object;
+
+		Initialized();
+	}
+
+	virtual void AddDump(TraceOutput&amp; out);
+
+	virtual const char* Name() const;
+
+	thread_id PreviousThreadID() const		{ return fPreviousID; }
+	uint8 PreviousState() const				{ return fPreviousState; }
+	uint16 PreviousWaitObjectType() const	{ return fPreviousWaitObjectType; }
+	const void* PreviousWaitObject() const	{ return fPreviousWaitObject; }
+
+private:
+	thread_id			fPreviousID;
+	int32				fCPU;
+	char*				fName;
+	uint8				fPriority;
+	uint8				fPreviousState;
+	uint16				fPreviousWaitObjectType;
+	union {
+		const void*		fPreviousWaitObject;
+		void*			fPreviousPC;
+	};
+};
+
+}	// namespace SchedulerTracing
+
+#	define T(x) new(std::nothrow) SchedulerTracing::x;
+#else
+#	define T(x) ;
+#endif
+
+
+#if SCHEDULER_TRACING
+
+namespace SchedulerTracing {
+
+enum ScheduleState {
+	RUNNING,
+	STILL_RUNNING,
+	PREEMPTED,
+	READY,
+	WAITING,
+	UNKNOWN
+};
+
+}
+
+int cmd_scheduler(int argc, char** argv);
+
+#endif	// SCHEDULER_TRACING
+
+#endif	// KERNEL_SCHEDULER_TRACING_H

Added: haiku/trunk/src/system/kernel/scheduler/scheduling_analysis.cpp
===================================================================
--- haiku/trunk/src/system/kernel/scheduler/scheduling_analysis.cpp	2008-10-20 21:41:30 UTC (rev 28252)
+++ haiku/trunk/src/system/kernel/scheduler/scheduling_analysis.cpp	2008-10-20 22:57:40 UTC (rev 28253)
@@ -0,0 +1,868 @@
+/*
+ * Copyright 2008, Ingo Weinhold, <A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">ingo_weinhold at gmx.de.</A>
+ * Distributed under the terms of the MIT License.
+ */
+
+#include &lt;scheduling_analysis.h&gt;
+
+#include &lt;elf.h&gt;
+#include &lt;kernel.h&gt;
+#include &lt;scheduler_defs.h&gt;
+#include &lt;tracing.h&gt;
+#include &lt;util/AutoLock.h&gt;
+#include &lt;util/khash.h&gt;
+
+#include &quot;scheduler_tracing.h&quot;
+
+
+#if SCHEDULER_TRACING
+
+namespace SchedulingAnalysis {
+
+using namespace SchedulerTracing;
+
+#if SCHEDULING_ANALYSIS_TRACING
+using namespace SchedulingAnalysisTracing;
+#endif
+
+struct ThreadWaitObject;
+
+struct HashObjectKey {
+	virtual ~HashObjectKey()
+	{
+	}
+
+	virtual uint32 HashKey() const = 0;
+};
+
+
+struct HashObject {
+	HashObject*	next;
+
+	virtual ~HashObject()
+	{
+	}
+
+	virtual uint32 HashKey() const = 0;
+	virtual bool Equals(const HashObjectKey* key) const = 0;
+};
+
+
+struct ThreadKey : HashObjectKey {
+	thread_id	id;
+
+	ThreadKey(thread_id id)
+		:
+		id(id)
+	{
+	}
+
+	virtual uint32 HashKey() const
+	{
+		return id;
+	}
+};
+
+
+struct Thread : HashObject, scheduling_analysis_thread {
+	ScheduleState state;
+	bigtime_t lastTime;
+
+	ThreadWaitObject* waitObject;
+
+	Thread(thread_id id)
+		:
+		state(UNKNOWN),
+		lastTime(0),
+
+		waitObject(NULL)
+	{
+		this-&gt;id = id;
+		name[0] = '\0';
+
+		runs = 0;
+		total_run_time = 0;
+		min_run_time = 1;
+		max_run_time = -1;
+

[... truncated: 784 lines follow ...]

</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="012907.html">[Haiku-commits] r28252 - haiku/trunk/src/tools/vmdkimage
</A></li>
	<LI>Next message: <A HREF="012904.html">[Haiku-commits] r28254 - haiku/trunk/src/system/kernel/scheduler
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#12903">[ date ]</a>
              <a href="thread.html#12903">[ thread ]</a>
              <a href="subject.html#12903">[ subject ]</a>
              <a href="author.html#12903">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/haiku-commits">More information about the Haiku-commits
mailing list</a><br>
</body></html>
