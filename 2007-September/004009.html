<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Haiku-commits] r22326 - in haiku/trunk: headers/private/kernel	src/system/kernel/arch/x86 src/system/kernel/vm
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/haiku-commits/2007-September/index.html" >
   <LINK REL="made" HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r22326%20-%20in%20haiku/trunk%3A%20headers/private/kernel%0A%09src/system/kernel/arch/x86%20src/system/kernel/vm&In-Reply-To=%3C200709271029.l8RAT6cH025863%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="004008.html">
   <LINK REL="Next"  HREF="004010.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Haiku-commits] r22326 - in haiku/trunk: headers/private/kernel	src/system/kernel/arch/x86 src/system/kernel/vm</H1>
    <B>axeld at BerliOS</B> 
    <A HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r22326%20-%20in%20haiku/trunk%3A%20headers/private/kernel%0A%09src/system/kernel/arch/x86%20src/system/kernel/vm&In-Reply-To=%3C200709271029.l8RAT6cH025863%40sheep.berlios.de%3E"
       TITLE="[Haiku-commits] r22326 - in haiku/trunk: headers/private/kernel	src/system/kernel/arch/x86 src/system/kernel/vm">axeld at mail.berlios.de
       </A><BR>
    <I>Thu Sep 27 12:29:06 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="004008.html">[Haiku-commits] r22325 - in haiku/trunk: headers/os/interface	src/kits/interface
</A></li>
        <LI>Next message: <A HREF="004010.html">[Haiku-commits] r22327 - haiku/trunk/src/system/kernel/vm
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#4009">[ date ]</a>
              <a href="thread.html#4009">[ thread ]</a>
              <a href="subject.html#4009">[ subject ]</a>
              <a href="author.html#4009">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: axeld
Date: 2007-09-27 12:29:05 +0200 (Thu, 27 Sep 2007)
New Revision: 22326
ViewCVS: <A HREF="http://svn.berlios.de/viewcvs/haiku?rev=22326&amp;view=rev">http://svn.berlios.de/viewcvs/haiku?rev=22326&amp;view=rev</A>

Added:
   haiku/trunk/src/system/kernel/arch/x86/arch_debug.cpp
   haiku/trunk/src/system/kernel/arch/x86/arch_vm.cpp
   haiku/trunk/src/system/kernel/arch/x86/arch_vm_translation_map.cpp
   haiku/trunk/src/system/kernel/vm/vm_address_space.cpp
   haiku/trunk/src/system/kernel/vm/vm_store_anonymous_noswap.cpp
Removed:
   haiku/trunk/src/system/kernel/arch/x86/arch_debug.c
   haiku/trunk/src/system/kernel/arch/x86/arch_vm.c
   haiku/trunk/src/system/kernel/arch/x86/arch_vm_translation_map.c
Modified:
   haiku/trunk/headers/private/kernel/vm_types.h
   haiku/trunk/src/system/kernel/arch/x86/Jamfile
   haiku/trunk/src/system/kernel/vm/Jamfile
Log:
* Cleaned up vm_types.h a bit, and made vm_page, vm_cache, and vm_area
  opaque types for C.
* As a result, I've renamed some more source files to .cpp, and fixed
  all warnings caused by that.


Modified: haiku/trunk/headers/private/kernel/vm_types.h
===================================================================
--- haiku/trunk/headers/private/kernel/vm_types.h	2007-09-27 08:26:15 UTC (rev 22325)
+++ haiku/trunk/headers/private/kernel/vm_types.h	2007-09-27 10:29:05 UTC (rev 22326)
@@ -9,9 +9,10 @@
 #define _KERNEL_VM_TYPES_H
 
 
-#include &lt;kernel.h&gt;
 #include &lt;arch/vm_types.h&gt;
 #include &lt;arch/vm_translation_map.h&gt;
+#include &lt;condition_variable.h&gt;
+#include &lt;kernel.h&gt;
 #include &lt;util/DoublyLinkedQueue.h&gt;
 
 #include &lt;sys/uio.h&gt;
@@ -31,9 +32,6 @@
 #ifdef __cplusplus
 struct vm_page_mapping;
 typedef DoublyLinkedListLink&lt;vm_page_mapping&gt; vm_page_mapping_link;
-#else
-typedef struct { void *previous; void *next; } vm_page_mapping_link;
-#endif
 
 typedef struct vm_page_mapping {
 	vm_page_mapping_link page_link;
@@ -42,7 +40,6 @@
 	struct vm_area *area;
 } vm_page_mapping;
 
-#ifdef __cplusplus
 class DoublyLinkedPageLink {
 	public:
 		inline vm_page_mapping_link *operator()(vm_page_mapping *element) const
@@ -71,10 +68,6 @@
 
 typedef class DoublyLinkedQueue&lt;vm_page_mapping, DoublyLinkedPageLink&gt; vm_page_mappings;
 typedef class DoublyLinkedQueue&lt;vm_page_mapping, DoublyLinkedAreaLink&gt; vm_area_mappings;
-#else	// !__cplusplus
-typedef struct vm_page_mapping *vm_page_mappings;
-typedef struct vm_page_mapping *vm_area_mappings;
-#endif
 
 // vm page
 typedef struct vm_page {
@@ -137,16 +130,10 @@
 	CACHE_TYPE_NULL
 };
 
-#ifdef __cplusplus
-
-#include &lt;condition_variable.h&gt;
-
 struct vm_dummy_page : vm_page {
 	ConditionVariable&lt;vm_page&gt;	busy_condition;
 };
 
-#endif	// __cplusplus
-
 // vm_cache
 typedef struct vm_cache {
 	mutex				lock;
@@ -201,6 +188,13 @@
 	struct vm_area		*hash_next;
 } vm_area;
 
+#else	// !__cplusplus
+// these are just opaque types in C
+typedef struct vm_page vm_page;
+typedef struct vm_cache vm_cache;
+typedef struct vm_area vm_area;
+#endif
+
 enum {
 	VM_ASPACE_STATE_NORMAL = 0,
 	VM_ASPACE_STATE_DELETION

Modified: haiku/trunk/src/system/kernel/arch/x86/Jamfile
===================================================================
--- haiku/trunk/src/system/kernel/arch/x86/Jamfile	2007-09-27 08:26:15 UTC (rev 22325)
+++ haiku/trunk/src/system/kernel/arch/x86/Jamfile	2007-09-27 10:29:05 UTC (rev 22326)
@@ -12,7 +12,7 @@
 
 KernelStaticLibrary libx86 :
 	arch_cpu.c
-	arch_debug.c
+	arch_debug.cpp
 	arch_debug_console.c
 	arch_elf.c
 	arch_int.c
@@ -22,8 +22,8 @@
 	arch_smp.c
 	arch_thread.c
 	arch_timer.c
-	arch_vm.c
-	arch_vm_translation_map.c
+	arch_vm.cpp
+	arch_vm_translation_map.cpp
 	arch_x86.S
 	arch_interrupts.S
 	arch_system_info.c

Deleted: haiku/trunk/src/system/kernel/arch/x86/arch_debug.c

Copied: haiku/trunk/src/system/kernel/arch/x86/arch_debug.cpp (from rev 22318, haiku/trunk/src/system/kernel/arch/x86/arch_debug.c)

Deleted: haiku/trunk/src/system/kernel/arch/x86/arch_vm.c

Copied: haiku/trunk/src/system/kernel/arch/x86/arch_vm.cpp (from rev 22318, haiku/trunk/src/system/kernel/arch/x86/arch_vm.c)
===================================================================
--- haiku/trunk/src/system/kernel/arch/x86/arch_vm.c	2007-09-26 14:01:28 UTC (rev 22318)
+++ haiku/trunk/src/system/kernel/arch/x86/arch_vm.cpp	2007-09-27 10:29:05 UTC (rev 22326)
@@ -0,0 +1,276 @@
+/*
+ * Copyright 2002-2007, Axel D&#246;rfler, <A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">axeld at pinc-software.de.</A>
+ * Distributed under the terms of the MIT License.
+ *
+ * Copyright 2001, Travis Geiselbrecht. All rights reserved.
+ * Distributed under the terms of the NewOS License.
+ */
+
+
+#include &lt;KernelExport.h&gt;
+#include &lt;smp.h&gt;
+#include &lt;util/AutoLock.h&gt;
+#include &lt;vm.h&gt;
+#include &lt;vm_page.h&gt;
+#include &lt;vm_priv.h&gt;
+
+#include &lt;arch/vm.h&gt;
+#include &lt;arch/int.h&gt;
+#include &lt;arch/cpu.h&gt;
+
+#include &lt;arch/x86/bios.h&gt;
+
+#include &lt;stdlib.h&gt;
+#include &lt;string.h&gt;
+
+
+//#define TRACE_ARCH_VM
+#ifdef TRACE_ARCH_VM
+#	define TRACE(x) dprintf x
+#else
+#	define TRACE(x) ;
+#endif
+
+
+#define kMaxMemoryTypeRegisters 32
+
+void *gDmaAddress;
+
+static uint32 sMemoryTypeBitmap;
+static int32 sMemoryTypeIDs[kMaxMemoryTypeRegisters];
+static uint32 sMemoryTypeRegisterCount;
+static spinlock sMemoryTypeLock;
+
+
+static int32
+allocate_mtrr(void)
+{
+	InterruptsSpinLocker _(&amp;sMemoryTypeLock);
+
+	// find free bit
+
+	for (uint32 index = 0; index &lt; sMemoryTypeRegisterCount; index++) {
+		if (sMemoryTypeBitmap &amp; (1UL &lt;&lt; index))
+			continue;
+
+		sMemoryTypeBitmap |= 1UL &lt;&lt; index;
+		return index;
+	}
+
+	return -1;
+}
+
+
+static void
+free_mtrr(int32 index)
+{
+	InterruptsSpinLocker _(&amp;sMemoryTypeLock);
+
+	sMemoryTypeBitmap &amp;= ~(1UL &lt;&lt; index);
+}
+
+
+static uint64
+nearest_power(addr_t value)
+{
+	uint64 power = 1UL &lt;&lt; 12;
+		// 12 bits is the smallest supported alignment/length
+
+	while (value &gt; power)
+		power &lt;&lt;= 1;
+
+	return power;
+}
+
+
+static status_t
+set_memory_type(int32 id, uint64 base, uint64 length, uint32 type)
+{
+	int32 index;
+
+	if (type == 0)
+		return B_OK;
+
+	switch (type) {
+		case B_MTR_UC:
+			type = IA32_MTR_UNCACHED;
+			break;
+		case B_MTR_WC:
+			type = IA32_MTR_WRITE_COMBINING;
+			break;
+		case B_MTR_WT:
+			type = IA32_MTR_WRITE_THROUGH;
+			break;
+		case B_MTR_WP:
+			type = IA32_MTR_WRITE_PROTECTED;
+			break;
+		case B_MTR_WB:
+			type = IA32_MTR_WRITE_BACK;
+			break;
+
+		default:
+			return B_BAD_VALUE;
+	}
+
+	if (sMemoryTypeRegisterCount == 0)
+		return B_NOT_SUPPORTED;
+
+	// length must be a power of 2; just round it up to the next value
+	length = nearest_power(length);
+	if (length + base &lt;= base) {
+		// 4GB overflow
+		return B_BAD_VALUE;
+	}
+
+	// base must be aligned to the length
+	if (base &amp; (length - 1))
+		return B_BAD_VALUE;
+
+	index = allocate_mtrr();
+	if (index &lt; 0)
+		return B_ERROR;
+
+	TRACE((&quot;allocate MTRR slot %ld, base = %Lx, length = %Lx\n&quot;, index,
+		base, length));
+
+	sMemoryTypeIDs[index] = id;
+	x86_set_mtrr(index, base, length, type);
+
+	return B_OK;
+}
+
+
+//	#pragma mark -
+
+
+status_t
+arch_vm_init(kernel_args *args)
+{
+	TRACE((&quot;arch_vm_init: entry\n&quot;));
+	return 0;
+}
+
+
+/*!	Marks DMA region as in-use, and maps it into the kernel space */
+status_t
+arch_vm_init_post_area(kernel_args *args)
+{
+	area_id id;
+
+	TRACE((&quot;arch_vm_init_post_area: entry\n&quot;));
+
+	// account for DMA area and mark the pages unusable
+	vm_mark_page_range_inuse(0x0, 0xa0000 / B_PAGE_SIZE);
+
+	// map 0 - 0xa0000 directly
+	id = map_physical_memory(&quot;dma_region&quot;, (void *)0x0, 0xa0000,
+		B_ANY_KERNEL_ADDRESS, B_KERNEL_READ_AREA | B_KERNEL_WRITE_AREA,
+		&amp;gDmaAddress);
+	if (id &lt; 0) {
+		panic(&quot;arch_vm_init_post_area: unable to map dma region\n&quot;);
+		return B_NO_MEMORY;
+	}
+
+	return bios_init();
+}
+
+
+/*!	Gets rid of all yet unmapped (and therefore now unused) page tables */
+status_t
+arch_vm_init_end(kernel_args *args)
+{
+	TRACE((&quot;arch_vm_init_endvm: entry\n&quot;));
+
+	// throw away anything in the kernel_args.pgtable[] that's not yet mapped
+	vm_free_unused_boot_loader_range(KERNEL_BASE,
+		0x400000 * args-&gt;arch_args.num_pgtables);
+
+	return B_OK;
+}
+
+
+status_t
+arch_vm_init_post_modules(kernel_args *args)
+{
+//	void *cookie;
+
+	// the x86 CPU modules are now accessible
+
+	sMemoryTypeRegisterCount = x86_count_mtrrs();
+	if (sMemoryTypeRegisterCount == 0)
+		return B_OK;
+
+	// not very likely, but play safe here
+	if (sMemoryTypeRegisterCount &gt; kMaxMemoryTypeRegisters)
+		sMemoryTypeRegisterCount = kMaxMemoryTypeRegisters;
+
+	// init memory type ID table
+
+	for (uint32 i = 0; i &lt; sMemoryTypeRegisterCount; i++) {
+		sMemoryTypeIDs[i] = -1;
+	}
+
+	// set the physical memory ranges to write-back mode
+
+	for (uint32 i = 0; i &lt; args-&gt;num_physical_memory_ranges; i++) {
+		set_memory_type(-1, args-&gt;physical_memory_range[i].start,
+			args-&gt;physical_memory_range[i].size, B_MTR_WB);
+	}
+
+	return B_OK;
+}
+
+
+void
+arch_vm_aspace_swap(vm_address_space *aspace)
+{
+	i386_swap_pgdir((addr_t)i386_translation_map_get_pgdir(
+		&amp;aspace-&gt;translation_map));
+}
+
+
+bool
+arch_vm_supports_protection(uint32 protection)
+{
+	// x86 always has the same read/write properties for userland and the
+	// kernel.
+	// That's why we do not support user-read/kernel-write access. While the
+	// other way around is not supported either, we don't care in this case
+	// and give the kernel full access.
+	if ((protection &amp; (B_READ_AREA | B_WRITE_AREA)) == B_READ_AREA
+		&amp;&amp; protection &amp; B_KERNEL_WRITE_AREA)
+		return false;
+
+	return true;
+}
+
+
+void
+arch_vm_unset_memory_type(struct vm_area *area)
+{
+	uint32 index;
+
+	if (area-&gt;memory_type == 0)
+		return;
+
+	// find index for area ID
+
+	for (index = 0; index &lt; sMemoryTypeRegisterCount; index++) {
+		if (sMemoryTypeIDs[index] == area-&gt;id) {
+			x86_set_mtrr(index, 0, 0, 0);
+
+			sMemoryTypeIDs[index] = -1;
+			free_mtrr(index);
+			break;
+		}
+	}
+}
+
+
+status_t
+arch_vm_set_memory_type(struct vm_area *area, addr_t physicalBase,
+	uint32 type)
+{
+	area-&gt;memory_type = type &gt;&gt; MEMORY_TYPE_SHIFT;
+	return set_memory_type(area-&gt;id, physicalBase, area-&gt;size, type);
+}

Deleted: haiku/trunk/src/system/kernel/arch/x86/arch_vm_translation_map.c

Copied: haiku/trunk/src/system/kernel/arch/x86/arch_vm_translation_map.cpp (from rev 22319, haiku/trunk/src/system/kernel/arch/x86/arch_vm_translation_map.c)
===================================================================
--- haiku/trunk/src/system/kernel/arch/x86/arch_vm_translation_map.c	2007-09-26 17:42:25 UTC (rev 22319)
+++ haiku/trunk/src/system/kernel/arch/x86/arch_vm_translation_map.cpp	2007-09-27 10:29:05 UTC (rev 22326)
@@ -0,0 +1,1027 @@
+/*
+ * Copyright 2002-2007, Axel D&#246;rfler, <A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">axeld at pinc-software.de.</A> All rights reserved.
+ * Distributed under the terms of the MIT License.
+ *
+ * Copyright 2001-2002, Travis Geiselbrecht. All rights reserved.
+ * Distributed under the terms of the NewOS License.
+ */
+
+
+#include &lt;vm_address_space.h&gt;
+#include &lt;vm_page.h&gt;
+#include &lt;vm_priv.h&gt;
+#include &lt;smp.h&gt;
+#include &lt;util/queue.h&gt;
+#include &lt;memheap.h&gt;
+#include &lt;arch_system_info.h&gt;
+#include &lt;arch/vm_translation_map.h&gt;
+
+#include &lt;string.h&gt;
+#include &lt;stdlib.h&gt;
+
+#include &quot;generic_vm_physical_page_mapper.h&quot;
+
+//#define TRACE_VM_TMAP
+#ifdef TRACE_VM_TMAP
+#	define TRACE(x) dprintf x
+#else
+#	define TRACE(x) ;
+#endif
+
+// 256 MB of iospace
+#define IOSPACE_SIZE (256*1024*1024)
+// 4 MB chunks, to optimize for 4 MB pages
+#define IOSPACE_CHUNK_SIZE (4*1024*1024)
+
+typedef struct page_table_entry {
+	uint32	present:1;
+	uint32	rw:1;
+	uint32	user:1;
+	uint32	write_through:1;
+	uint32	cache_disabled:1;
+	uint32	accessed:1;
+	uint32	dirty:1;
+	uint32	reserved:1;
+	uint32	global:1;
+	uint32	avail:3;
+	uint32	addr:20;
+} page_table_entry;
+
+typedef struct page_directory_entry {
+	uint32	present:1;
+	uint32	rw:1;
+	uint32	user:1;
+	uint32	write_through:1;
+	uint32	cache_disabled:1;
+	uint32	accessed:1;
+	uint32	reserved:1;
+	uint32	page_size:1;
+	uint32	global:1;
+	uint32	avail:3;
+	uint32	addr:20;
+} page_directory_entry;
+
+static page_table_entry *iospace_pgtables = NULL;
+
+#define PAGE_INVALIDATE_CACHE_SIZE 64
+
+// vm_translation object stuff
+typedef struct vm_translation_map_arch_info {
+	page_directory_entry *pgdir_virt;
+	page_directory_entry *pgdir_phys;
+	int num_invalidate_pages;
+	addr_t pages_to_invalidate[PAGE_INVALIDATE_CACHE_SIZE];
+} vm_translation_map_arch_info;
+
+
+static page_table_entry *page_hole = NULL;
+static page_directory_entry *page_hole_pgdir = NULL;
+static page_directory_entry *sKernelPhysicalPageDirectory = NULL;
+static page_directory_entry *sKernelVirtualPageDirectory = NULL;
+static addr_t sQueryPages;
+static page_table_entry *sQueryPageTable;
+
+static vm_translation_map *tmap_list;
+static spinlock tmap_list_lock;
+
+static addr_t sIOSpaceBase;
+
+#define CHATTY_TMAP 0
+
+#define ADDR_SHIFT(x) ((x)&gt;&gt;12)
+#define ADDR_REVERSE_SHIFT(x) ((x)&lt;&lt;12)
+
+#define VADDR_TO_PDENT(va) (((va) / B_PAGE_SIZE) / 1024)
+#define VADDR_TO_PTENT(va) (((va) / B_PAGE_SIZE) % 1024)
+
+#define FIRST_USER_PGDIR_ENT    (VADDR_TO_PDENT(USER_BASE))
+#define NUM_USER_PGDIR_ENTS     (VADDR_TO_PDENT(ROUNDUP(USER_SIZE, B_PAGE_SIZE * 1024)))
+#define FIRST_KERNEL_PGDIR_ENT  (VADDR_TO_PDENT(KERNEL_BASE))
+#define NUM_KERNEL_PGDIR_ENTS   (VADDR_TO_PDENT(KERNEL_SIZE))
+#define IS_KERNEL_MAP(map)		(map-&gt;arch_data-&gt;pgdir_phys == sKernelPhysicalPageDirectory)
+
+static status_t early_query(addr_t va, addr_t *out_physical);
+static status_t get_physical_page_tmap(addr_t pa, addr_t *va, uint32 flags);
+static status_t put_physical_page_tmap(addr_t va);
+
+static void flush_tmap(vm_translation_map *map);
+
+
+void *
+i386_translation_map_get_pgdir(vm_translation_map *map)
+{
+	return map-&gt;arch_data-&gt;pgdir_phys;
+}
+
+
+static inline void
+init_page_directory_entry(page_directory_entry *entry)
+{
+	*(uint32 *)entry = 0;
+}
+
+
+static inline void
+update_page_directory_entry(page_directory_entry *entry, page_directory_entry *with)
+{
+	// update page directory entry atomically
+	*(uint32 *)entry = *(uint32 *)with;
+}
+
+
+static inline void
+init_page_table_entry(page_table_entry *entry)
+{
+	*(uint32 *)entry = 0;
+}
+
+
+static inline void
+update_page_table_entry(page_table_entry *entry, page_table_entry *with)
+{
+	// update page table entry atomically
+	*(uint32 *)entry = *(uint32 *)with;
+}
+
+
+static void
+_update_all_pgdirs(int index, page_directory_entry e)
+{
+	vm_translation_map *entry;
+	unsigned int state = disable_interrupts();
+
+	acquire_spinlock(&amp;tmap_list_lock);
+
+	for(entry = tmap_list; entry != NULL; entry = entry-&gt;next)
+		entry-&gt;arch_data-&gt;pgdir_virt[index] = e;
+
+	release_spinlock(&amp;tmap_list_lock);
+	restore_interrupts(state);
+}
+
+
+// XXX currently assumes this translation map is active
+
+static status_t
+early_query(addr_t va, addr_t *_physicalAddress)
+{
+	page_table_entry *pentry;
+
+	if (page_hole_pgdir[VADDR_TO_PDENT(va)].present == 0) {
+		// no pagetable here
+		return B_ERROR;
+	}
+
+	pentry = page_hole + va / B_PAGE_SIZE;
+	if (pentry-&gt;present == 0) {
+		// page mapping not valid
+		return B_ERROR;
+	}
+
+	*_physicalAddress = pentry-&gt;addr &lt;&lt; 12;
+	return B_OK;
+}
+
+
+/*!	Acquires the map's recursive lock, and resets the invalidate pages counter
+	in case it's the first locking recursion.
+*/
+static status_t
+lock_tmap(vm_translation_map *map)
+{
+	TRACE((&quot;lock_tmap: map %p\n&quot;, map));
+
+	recursive_lock_lock(&amp;map-&gt;lock);
+	if (recursive_lock_get_recursion(&amp;map-&gt;lock) == 1) {
+		// we were the first one to grab the lock
+		TRACE((&quot;clearing invalidated page count\n&quot;));
+		map-&gt;arch_data-&gt;num_invalidate_pages = 0;
+	}
+
+	return B_OK;
+}
+
+
+/*!	Unlocks the map, and, if we'll actually losing the recursive lock,
+	flush all pending changes of this map (ie. flush TLB caches as
+	needed).
+*/
+static status_t
+unlock_tmap(vm_translation_map *map)
+{
+	TRACE((&quot;unlock_tmap: map %p\n&quot;, map));
+
+	if (recursive_lock_get_recursion(&amp;map-&gt;lock) == 1) {
+		// we're about to release it for the last time
+		flush_tmap(map);
+	}
+
+	recursive_lock_unlock(&amp;map-&gt;lock);
+	return B_OK;
+}
+
+
+static void
+destroy_tmap(vm_translation_map *map)
+{
+	int state;
+	vm_translation_map *entry;
+	vm_translation_map *last = NULL;
+	unsigned int i;
+
+	if (map == NULL)
+		return;
+
+	// remove it from the tmap list
+	state = disable_interrupts();
+	acquire_spinlock(&amp;tmap_list_lock);
+
+	entry = tmap_list;
+	while (entry != NULL) {
+		if (entry == map) {
+			if (last != NULL)
+				last-&gt;next = entry-&gt;next;
+			else
+				tmap_list = entry-&gt;next;
+
+			break;
+		}
+		last = entry;
+		entry = entry-&gt;next;
+	}
+
+	release_spinlock(&amp;tmap_list_lock);
+	restore_interrupts(state);
+
+	if (map-&gt;arch_data-&gt;pgdir_virt != NULL) {
+		// cycle through and free all of the user space pgtables
+		for (i = VADDR_TO_PDENT(USER_BASE); i &lt;= VADDR_TO_PDENT(USER_BASE + (USER_SIZE - 1)); i++) {
+			addr_t pgtable_addr;
+			vm_page *page;
+
+			if (map-&gt;arch_data-&gt;pgdir_virt[i].present == 1) {
+				pgtable_addr = map-&gt;arch_data-&gt;pgdir_virt[i].addr;
+				page = vm_lookup_page(pgtable_addr);
+				if (!page)
+					panic(&quot;destroy_tmap: didn't find pgtable page\n&quot;);
+				vm_page_set_state(page, PAGE_STATE_FREE);
+			}
+		}
+		free(map-&gt;arch_data-&gt;pgdir_virt);
+	}
+
+	free(map-&gt;arch_data);
+	recursive_lock_destroy(&amp;map-&gt;lock);
+}
+
+
+static void
+put_pgtable_in_pgdir(page_directory_entry *entry,
+	addr_t pgtable_phys, uint32 attributes)
+{
+	page_directory_entry table;
+	// put it in the pgdir
+	init_page_directory_entry(&amp;table);
+	table.addr = ADDR_SHIFT(pgtable_phys);
+
+	// ToDo: we ignore the attributes of the page table - for compatibility
+	//	with BeOS we allow having user accessible areas in the kernel address
+	//	space. This is currently being used by some drivers, mainly for the
+	//	frame buffer. Our current real time data implementation makes use of
+	//	this fact, too.
+	//	We might want to get rid of this possibility one day, especially if
+	//	we intend to port it to a platform that does not support this.
+	table.user = 1;
+	table.rw = 1;
+	table.present = 1;
+	update_page_directory_entry(entry, &amp;table);
+}
+
+
+static void
+put_page_table_entry_in_pgtable(page_table_entry *entry,
+	addr_t physicalAddress, uint32 attributes, bool globalPage)
+{
+	page_table_entry page;
+	init_page_table_entry(&amp;page);
+
+	page.addr = ADDR_SHIFT(physicalAddress);
+
+	// if the page is user accessible, it's automatically
+	// accessible in kernel space, too (but with the same
+	// protection)
+	page.user = (attributes &amp; B_USER_PROTECTION) != 0;
+	if (page.user)
+		page.rw = (attributes &amp; B_WRITE_AREA) != 0;
+	else
+		page.rw = (attributes &amp; B_KERNEL_WRITE_AREA) != 0;
+	page.present = 1;
+
+	if (globalPage)
+		page.global = 1;
+
+	// put it in the page table
+	update_page_table_entry(entry, &amp;page);
+}
+
+
+static status_t
+map_tmap(vm_translation_map *map, addr_t va, addr_t pa, uint32 attributes)
+{
+	page_directory_entry *pd;
+	page_table_entry *pt;
+	unsigned int index;
+	int err;
+
+	TRACE((&quot;map_tmap: entry pa 0x%lx va 0x%lx\n&quot;, pa, va));
+
+/*
+	dprintf(&quot;pgdir at 0x%x\n&quot;, pgdir);
+	dprintf(&quot;index is %d\n&quot;, va / B_PAGE_SIZE / 1024);
+	dprintf(&quot;final at 0x%x\n&quot;, &amp;pgdir[va / B_PAGE_SIZE / 1024]);
+	dprintf(&quot;value is 0x%x\n&quot;, *(int *)&amp;pgdir[va / B_PAGE_SIZE / 1024]);
+	dprintf(&quot;present bit is %d\n&quot;, pgdir[va / B_PAGE_SIZE / 1024].present);
+	dprintf(&quot;addr is %d\n&quot;, pgdir[va / B_PAGE_SIZE / 1024].addr);
+*/
+	pd = map-&gt;arch_data-&gt;pgdir_virt;
+
+	// check to see if a page table exists for this range
+	index = VADDR_TO_PDENT(va);
+	if (pd[index].present == 0) {
+		addr_t pgtable;
+		vm_page *page;
+
+		// we need to allocate a pgtable
+		page = vm_page_allocate_page(PAGE_STATE_CLEAR, false);
+
+		// mark the page WIRED
+		vm_page_set_state(page, PAGE_STATE_WIRED);
+
+		pgtable = page-&gt;physical_page_number * B_PAGE_SIZE;
+
+		TRACE((&quot;map_tmap: asked for free page for pgtable. 0x%lx\n&quot;, pgtable));
+
+		// put it in the pgdir
+		put_pgtable_in_pgdir(&amp;pd[index], pgtable, attributes
+			| (attributes &amp; B_USER_PROTECTION ? B_WRITE_AREA : B_KERNEL_WRITE_AREA));
+
+		// update any other page directories, if it maps kernel space
+		if (index &gt;= FIRST_KERNEL_PGDIR_ENT
+			&amp;&amp; index &lt; (FIRST_KERNEL_PGDIR_ENT + NUM_KERNEL_PGDIR_ENTS))
+			_update_all_pgdirs(index, pd[index]);
+
+		map-&gt;map_count++;
+	}
+
+	// now, fill in the pentry
+	do {
+		err = get_physical_page_tmap(ADDR_REVERSE_SHIFT(pd[index].addr),
+				(addr_t *)&amp;pt, PHYSICAL_PAGE_NO_WAIT);
+	} while (err &lt; 0);
+	index = VADDR_TO_PTENT(va);
+
+	put_page_table_entry_in_pgtable(&amp;pt[index], pa, attributes,
+		IS_KERNEL_MAP(map));
+
+	put_physical_page_tmap((addr_t)pt);
+
+	if (map-&gt;arch_data-&gt;num_invalidate_pages &lt; PAGE_INVALIDATE_CACHE_SIZE)
+		map-&gt;arch_data-&gt;pages_to_invalidate[map-&gt;arch_data-&gt;num_invalidate_pages] = va;
+
+	map-&gt;arch_data-&gt;num_invalidate_pages++;
+
+	map-&gt;map_count++;
+
+	return 0;
+}
+
+
+static status_t
+unmap_tmap(vm_translation_map *map, addr_t start, addr_t end)
+{
+	page_table_entry *pt;
+	page_directory_entry *pd = map-&gt;arch_data-&gt;pgdir_virt;
+	status_t status;
+	int index;
+
+	start = ROUNDOWN(start, B_PAGE_SIZE);
+	end = ROUNDUP(end, B_PAGE_SIZE);
+
+	TRACE((&quot;unmap_tmap: asked to free pages 0x%lx to 0x%lx\n&quot;, start, end));
+
+restart:
+	if (start &gt;= end)
+		return B_OK;
+
+	index = VADDR_TO_PDENT(start);
+	if (pd[index].present == 0) {
+		// no pagetable here, move the start up to access the next page table
+		start = ROUNDUP(start + 1, B_PAGE_SIZE);
+		goto restart;
+	}
+
+	do {
+		status = get_physical_page_tmap(ADDR_REVERSE_SHIFT(pd[index].addr),
+			(addr_t *)&amp;pt, PHYSICAL_PAGE_NO_WAIT);
+	} while (status &lt; B_OK);
+
+	for (index = VADDR_TO_PTENT(start); (index &lt; 1024) &amp;&amp; (start &lt; end);
+			index++, start += B_PAGE_SIZE) {
+		if (pt[index].present == 0) {
+			// page mapping not valid
+			continue;
+		}
+
+		TRACE((&quot;unmap_tmap: removing page 0x%lx\n&quot;, start));
+
+		pt[index].present = 0;
+		map-&gt;map_count--;
+
+		if (map-&gt;arch_data-&gt;num_invalidate_pages &lt; PAGE_INVALIDATE_CACHE_SIZE)
+			map-&gt;arch_data-&gt;pages_to_invalidate[map-&gt;arch_data-&gt;num_invalidate_pages] = start;
+
+		map-&gt;arch_data-&gt;num_invalidate_pages++;
+	}
+
+	put_physical_page_tmap((addr_t)pt);
+
+	goto restart;
+}
+
+
+static status_t
+query_tmap_interrupt(vm_translation_map *map, addr_t va, addr_t *_physical,
+	uint32 *_flags)
+{
+	page_directory_entry *pd = map-&gt;arch_data-&gt;pgdir_virt;
+	page_table_entry *pt;
+	addr_t physicalPageTable;
+	int32 cpu = smp_get_current_cpu();
+	int32 index;
+
+	*_physical = 0;
+
+	index = VADDR_TO_PDENT(va);
+	if (pd[index].present == 0) {
+		// no pagetable here
+		return B_ERROR;
+	}
+
+	// map page table entry using our per CPU mapping page
+
+	physicalPageTable = ADDR_REVERSE_SHIFT(pd[index].addr);
+	pt = (page_table_entry *)(sQueryPages + cpu * B_PAGE_SIZE);
+	index = VADDR_TO_PDENT((addr_t)pt);
+	if (pd[index].present == 0) {
+		// no page table here
+		return B_ERROR;
+	}
+
+	index = VADDR_TO_PTENT((addr_t)pt);
+	put_page_table_entry_in_pgtable(&amp;sQueryPageTable[index], physicalPageTable,
+		B_KERNEL_READ_AREA, false);
+	invalidate_TLB(pt);
+
+	index = VADDR_TO_PTENT(va);
+	*_physical = ADDR_REVERSE_SHIFT(pt[index].addr);
+
+	*_flags |= ((pt[index].rw ? B_KERNEL_WRITE_AREA : 0) | B_KERNEL_READ_AREA)
+		| (pt[index].dirty ? PAGE_MODIFIED : 0)
+		| (pt[index].accessed ? PAGE_ACCESSED : 0)
+		| (pt[index].present ? PAGE_PRESENT : 0);
+
+	return B_OK;
+}
+
+
+static status_t
+query_tmap(vm_translation_map *map, addr_t va, addr_t *_physical, uint32 *_flags)
+{
+	page_table_entry *pt;
+	page_directory_entry *pd = map-&gt;arch_data-&gt;pgdir_virt;
+	status_t status;
+	int32 index;
+
+	// default the flags to not present
+	*_flags = 0;
+	*_physical = 0;
+
+	index = VADDR_TO_PDENT(va);
+	if (pd[index].present == 0) {
+		// no pagetable here
+		return B_NO_ERROR;
+	}
+
+	do {
+		status = get_physical_page_tmap(ADDR_REVERSE_SHIFT(pd[index].addr),
+			(addr_t *)&amp;pt, PHYSICAL_PAGE_NO_WAIT);
+	} while (status &lt; B_OK);
+	index = VADDR_TO_PTENT(va);
+
+	*_physical = ADDR_REVERSE_SHIFT(pt[index].addr);
+
+	// read in the page state flags, clearing the modified and accessed flags in the process
+	if (pt[index].user)
+		*_flags |= (pt[index].rw ? B_WRITE_AREA : 0) | B_READ_AREA;
+
+	*_flags |= ((pt[index].rw ? B_KERNEL_WRITE_AREA : 0) | B_KERNEL_READ_AREA)
+		| (pt[index].dirty ? PAGE_MODIFIED : 0)
+		| (pt[index].accessed ? PAGE_ACCESSED : 0)
+		| (pt[index].present ? PAGE_PRESENT : 0);
+
+	put_physical_page_tmap((addr_t)pt);
+
+	TRACE((&quot;query_tmap: returning pa 0x%lx for va 0x%lx\n&quot;, *_physical, va));
+
+	return B_OK;
+}
+
+
+static addr_t
+get_mapped_size_tmap(vm_translation_map *map)
+{
+	return map-&gt;map_count;
+}
+
+
+static status_t
+protect_tmap(vm_translation_map *map, addr_t start, addr_t end, uint32 attributes)
+{
+	page_table_entry *pt;
+	page_directory_entry *pd = map-&gt;arch_data-&gt;pgdir_virt;
+	status_t status;
+	int index;
+
+	start = ROUNDOWN(start, B_PAGE_SIZE);
+	end = ROUNDUP(end, B_PAGE_SIZE);
+
+	TRACE((&quot;protect_tmap: pages 0x%lx to 0x%lx, attributes %lx\n&quot;, start, end, attributes));
+
+restart:
+	if (start &gt;= end)
+		return B_OK;
+
+	index = VADDR_TO_PDENT(start);
+	if (pd[index].present == 0) {
+		// no pagetable here, move the start up to access the next page table
+		start = ROUNDUP(start + 1, B_PAGE_SIZE);
+		goto restart;
+	}
+
+	do {
+		status = get_physical_page_tmap(ADDR_REVERSE_SHIFT(pd[index].addr),
+				(addr_t *)&amp;pt, PHYSICAL_PAGE_NO_WAIT);
+	} while (status &lt; B_OK);
+
+	for (index = VADDR_TO_PTENT(start); index &lt; 1024 &amp;&amp; start &lt; end; index++, start += B_PAGE_SIZE) {
+		if (pt[index].present == 0) {
+			// page mapping not valid
+			continue;
+		}
+
+		TRACE((&quot;protect_tmap: protect page 0x%lx\n&quot;, start));
+
+		pt[index].user = (attributes &amp; B_USER_PROTECTION) != 0;
+		if ((attributes &amp; B_USER_PROTECTION) != 0)
+			pt[index].rw = (attributes &amp; B_WRITE_AREA) != 0;
+		else
+			pt[index].rw = (attributes &amp; B_KERNEL_WRITE_AREA) != 0;
+
+		if (map-&gt;arch_data-&gt;num_invalidate_pages &lt; PAGE_INVALIDATE_CACHE_SIZE)
+			map-&gt;arch_data-&gt;pages_to_invalidate[map-&gt;arch_data-&gt;num_invalidate_pages] = start;
+
+		map-&gt;arch_data-&gt;num_invalidate_pages++;
+	}
+
+	put_physical_page_tmap((addr_t)pt);
+
+	goto restart;
+}
+
+
+static status_t
+clear_flags_tmap(vm_translation_map *map, addr_t va, uint32 flags)

[... truncated: 1008 lines follow ...]

</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="004008.html">[Haiku-commits] r22325 - in haiku/trunk: headers/os/interface	src/kits/interface
</A></li>
	<LI>Next message: <A HREF="004010.html">[Haiku-commits] r22327 - haiku/trunk/src/system/kernel/vm
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#4009">[ date ]</a>
              <a href="thread.html#4009">[ thread ]</a>
              <a href="subject.html#4009">[ subject ]</a>
              <a href="author.html#4009">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/haiku-commits">More information about the Haiku-commits
mailing list</a><br>
</body></html>
