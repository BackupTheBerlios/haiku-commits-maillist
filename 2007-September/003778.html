<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Haiku-commits] r22152 - in haiku/trunk: headers/private/kernel	src/system/kernel/vm
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/haiku-commits/2007-September/index.html" >
   <LINK REL="made" HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r22152%20-%20in%20haiku/trunk%3A%20headers/private/kernel%0A%09src/system/kernel/vm&In-Reply-To=%3C200709022255.l82MtPb1017032%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="003777.html">
   <LINK REL="Next"  HREF="003779.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Haiku-commits] r22152 - in haiku/trunk: headers/private/kernel	src/system/kernel/vm</H1>
    <B>bonefish at BerliOS</B> 
    <A HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r22152%20-%20in%20haiku/trunk%3A%20headers/private/kernel%0A%09src/system/kernel/vm&In-Reply-To=%3C200709022255.l82MtPb1017032%40sheep.berlios.de%3E"
       TITLE="[Haiku-commits] r22152 - in haiku/trunk: headers/private/kernel	src/system/kernel/vm">bonefish at mail.berlios.de
       </A><BR>
    <I>Mon Sep  3 00:55:25 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="003777.html">[Haiku-commits] r22151 - in haiku/trunk: headers/private/kernel	src/system/kernel
</A></li>
        <LI>Next message: <A HREF="003779.html">[Haiku-commits] r22153 - haiku/trunk/src/system/kernel/arch/x86
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#3778">[ date ]</a>
              <a href="thread.html#3778">[ thread ]</a>
              <a href="subject.html#3778">[ subject ]</a>
              <a href="author.html#3778">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: bonefish
Date: 2007-09-03 00:55:23 +0200 (Mon, 03 Sep 2007)
New Revision: 22152
ViewCVS: <A HREF="http://svn.berlios.de/viewcvs/haiku?rev=22152&amp;view=rev">http://svn.berlios.de/viewcvs/haiku?rev=22152&amp;view=rev</A>

Modified:
   haiku/trunk/headers/private/kernel/vm_types.h
   haiku/trunk/src/system/kernel/vm/Jamfile
   haiku/trunk/src/system/kernel/vm/vm.cpp
   haiku/trunk/src/system/kernel/vm/vm_cache.cpp
Log:
* Added &quot;caches&quot; debugger command (to be enable by defining
  DEBUG_CACHE_LIST) that prints an unspectacular list of pointers to all
  existing caches. Feel free to extend.
* Enhanced MultiAddressSpaceLocker:
  - It supports choosing between read and write lock per address space,
    now.
  - Added AddAreaCacheAndLock(), which adds the address spaces of all
    areas that are attached to a given area's cache, locks them, and
    locks the cache. It makes sure that the area list didn't change in
    the meantime and optionally also that all areas have their
    no_cache_change flags cleared.
* Changed vm_copy_on_write_area() to take a cache instead of an area,
  requiring it to be locked and all address spaces of affected areas to
  be read-locked, plus all areas' no_cache_change flags to be cleared.
  Callers simply use MultiAddressSpaceLocker:: AddAreaCacheAndLock() to
  do that. This resolves an open TODO, that the areas' base, size, and
  protection fields were accessed without their address spaces being
  locked.
* vm_copy_area() does now always insert a cache for the target area. Not
  doing that would cause source and target area being attached to
  the same cache in case the target protection was read-only. This
  would make them behave like cloned areas, which would lead to trouble
  when one of the areas would be changed to writable later.
* Fixed the !writable -&gt; writable case in vm_set_area_protection(). It
  would simply change the protection of all mapped pages for this area,
  including ones from lower caches, thus causing later writes to the
  area to be seen by areas that shouldn't see them. This fixes a problem
  with software breakpoints in gdb. They could cause other programs to
  be dropped into the debugger.
* resize_area() uses MultiAddressSpaceLocker::AddAreaCacheAndLock() now,
  too, and could be compacted quite a bit.
 


Modified: haiku/trunk/headers/private/kernel/vm_types.h
===================================================================
--- haiku/trunk/headers/private/kernel/vm_types.h	2007-09-02 22:21:26 UTC (rev 22151)
+++ haiku/trunk/headers/private/kernel/vm_types.h	2007-09-02 22:55:23 UTC (rev 22152)
@@ -25,6 +25,9 @@
 // between caches.
 //#define DEBUG_PAGE_CACHE_TRANSITIONS	1
 
+// Enables a global list of all vm_cache structures.
+//#define DEBUG_CACHE_LIST 1
+
 #ifdef __cplusplus
 struct vm_page_mapping;
 typedef DoublyLinkedListLink&lt;vm_page_mapping&gt; vm_page_mapping_link;
@@ -91,14 +94,14 @@
 
 	vm_page_mappings	mappings;
 
-	#ifdef DEBUG_PAGE_QUEUE
+#ifdef DEBUG_PAGE_QUEUE
 	void*				queue;
-	#endif
+#endif
 
-	#ifdef DEBUG_PAGE_CACHE_TRANSITIONS
+#ifdef DEBUG_PAGE_CACHE_TRANSITIONS
 	uint32	debug_flags;
 	struct vm_page *collided_page;
-	#endif
+#endif
 
 	uint8				type : 2;
 	uint8				state : 3;
@@ -162,8 +165,18 @@
 	uint32				scan_skip : 1;
 	uint32				busy : 1;
 	uint32				type : 5;
+
+#if DEBUG_CACHE_LIST
+	struct vm_cache*	debug_previous;
+	struct vm_cache*	debug_next;
+#endif
 } vm_cache;
 
+
+#if DEBUG_CACHE_LIST
+extern vm_cache* gDebugCacheList;
+#endif
+
 // vm area
 typedef struct vm_area {
 	char				*name;

Modified: haiku/trunk/src/system/kernel/vm/Jamfile
===================================================================
--- haiku/trunk/src/system/kernel/vm/Jamfile	2007-09-02 22:21:26 UTC (rev 22151)
+++ haiku/trunk/src/system/kernel/vm/Jamfile	2007-09-02 22:55:23 UTC (rev 22152)
@@ -1,5 +1,7 @@
 SubDir HAIKU_TOP src system kernel vm ;
 
+UsePrivateHeaders shared ;
+
 KernelMergeObject kernel_vm.o :
 	vm.cpp
 	vm_address_space.c

Modified: haiku/trunk/src/system/kernel/vm/vm.cpp
===================================================================
--- haiku/trunk/src/system/kernel/vm/vm.cpp	2007-09-02 22:21:26 UTC (rev 22151)
+++ haiku/trunk/src/system/kernel/vm/vm.cpp	2007-09-02 22:55:23 UTC (rev 22152)
@@ -7,14 +7,18 @@
  */
 
 
-#include &quot;vm_store_anonymous_noswap.h&quot;
-#include &quot;vm_store_device.h&quot;
-#include &quot;vm_store_null.h&quot;
+#include &lt;vm.h&gt;
 
+#include &lt;ctype.h&gt;
+#include &lt;stdlib.h&gt;
+#include &lt;stdio.h&gt;
+#include &lt;string.h&gt;
+
 #include &lt;OS.h&gt;
 #include &lt;KernelExport.h&gt;
 
-#include &lt;vm.h&gt;
+#include &lt;AutoDeleter.h&gt;
+
 #include &lt;vm_address_space.h&gt;
 #include &lt;vm_priv.h&gt;
 #include &lt;vm_page.h&gt;
@@ -38,11 +42,11 @@
 #include &lt;arch/cpu.h&gt;
 #include &lt;arch/vm.h&gt;
 
-#include &lt;string.h&gt;
-#include &lt;ctype.h&gt;
-#include &lt;stdlib.h&gt;
-#include &lt;stdio.h&gt;
+#include &quot;vm_store_anonymous_noswap.h&quot;
+#include &quot;vm_store_device.h&quot;
+#include &quot;vm_store_null.h&quot;
 
+
 //#define TRACE_VM
 //#define TRACE_FAULTS
 #ifdef TRACE_VM
@@ -114,9 +118,15 @@
 	MultiAddressSpaceLocker();
 	~MultiAddressSpaceLocker();
 
-	status_t AddTeam(team_id team, vm_address_space** _space = NULL);
-	status_t AddArea(area_id area, vm_address_space** _space = NULL);
+	inline status_t AddTeam(team_id team, bool writeLock,
+		vm_address_space** _space = NULL);
+	inline status_t AddArea(area_id area, bool writeLock,
+		vm_address_space** _space = NULL);
 
+	status_t AddAreaCacheAndLock(area_id areaID, bool writeLockThisOne,
+		bool writeLockOthers, vm_area*&amp; _area, vm_cache** _cache = NULL,
+		bool checkNoCacheChange = false);
+
 	status_t Lock();
 	void Unlock();
 	bool IsLocked() const { return fLocked; }
@@ -124,18 +134,59 @@
 	void Unset();
 
 private:
+	struct lock_item {
+		vm_address_space*	space;
+		bool				write_lock;
+	};
+
 	bool _ResizeIfNeeded();
-	vm_address_space*&amp; _CurrentItem() { return fSpaces[fCount]; }
-	bool _HasAddressSpace(vm_address_space* space) const;
+	int32 _IndexOfAddressSpace(vm_address_space* space) const;
+	status_t _AddAddressSpace(vm_address_space* space, bool writeLock,
+		vm_address_space** _space);
 
 	static int _CompareItems(const void* _a, const void* _b);
 
-	vm_address_space** fSpaces;
+	lock_item*	fItems;
 	int32		fCapacity;
 	int32		fCount;
 	bool		fLocked;
 };
 
+
+class AreaCacheLocking {
+public:
+	inline bool Lock(vm_cache* lockable)
+	{
+		return false;
+	}
+
+	inline void Unlock(vm_cache* lockable)
+	{
+		vm_area_put_locked_cache(lockable);
+	}
+};
+
+class AreaCacheLocker : public AutoLocker&lt;vm_cache, AreaCacheLocking&gt; {
+public:
+	inline AreaCacheLocker(vm_cache* cache = NULL)
+		: AutoLocker&lt;vm_cache, AreaCacheLocking&gt;(cache, true)
+	{
+	}
+
+	inline AreaCacheLocker(vm_area* area)
+		: AutoLocker&lt;vm_cache, AreaCacheLocking&gt;()
+	{
+		SetTo(area);
+	}
+
+	inline void SetTo(vm_area* area)
+	{
+		return AutoLocker&lt;vm_cache, AreaCacheLocking&gt;::SetTo(
+			area != NULL ? vm_area_get_locked_cache(area) : NULL, true, true);
+	}
+};
+
+
 #define REGION_HASH_TABLE_SIZE 1024
 static area_id sNextAreaID;
 static hash_table *sAreaHash;
@@ -396,7 +447,7 @@
 
 MultiAddressSpaceLocker::MultiAddressSpaceLocker()
 	:
-	fSpaces(NULL),
+	fItems(NULL),
 	fCapacity(0),
 	fCount(0),
 	fLocked(false)
@@ -407,16 +458,16 @@
 MultiAddressSpaceLocker::~MultiAddressSpaceLocker()
 {
 	Unset();
-	free(fSpaces);
+	free(fItems);
 }
 
 
 /*static*/ int
 MultiAddressSpaceLocker::_CompareItems(const void* _a, const void* _b)
 {
-	vm_address_space* a = *(vm_address_space **)_a;
-	vm_address_space* b = *(vm_address_space **)_b;
-	return a-&gt;id - b-&gt;id;
+	lock_item* a = (lock_item*)_a;
+	lock_item* b = (lock_item*)_b;
+	return a-&gt;space-&gt;id - b-&gt;space-&gt;id;
 }
 
 
@@ -424,53 +475,56 @@
 MultiAddressSpaceLocker::_ResizeIfNeeded()
 {
 	if (fCount == fCapacity) {
-		vm_address_space** items = (vm_address_space**)realloc(fSpaces,
-			(fCapacity + 4) * sizeof(void*));
+		lock_item* items = (lock_item*)realloc(fItems,
+			(fCapacity + 4) * sizeof(lock_item));
 		if (items == NULL)
 			return false;
 
 		fCapacity += 4;
-		fSpaces = items;
+		fItems = items;
 	}
 
 	return true;
 }
 
 
-bool
-MultiAddressSpaceLocker::_HasAddressSpace(vm_address_space* space) const
+int32
+MultiAddressSpaceLocker::_IndexOfAddressSpace(vm_address_space* space) const
 {
 	for (int32 i = 0; i &lt; fCount; i++) {
-		if (fSpaces[i] == space)
-			return true;
+		if (fItems[i].space == space)
+			return i;
 	}
 	
-	return false;
+	return -1;
 }
 
 
 status_t
-MultiAddressSpaceLocker::AddTeam(team_id team, vm_address_space** _space)
+MultiAddressSpaceLocker::_AddAddressSpace(vm_address_space* space,
+	bool writeLock, vm_address_space** _space)
 {
-	if (!_ResizeIfNeeded())
-		return B_NO_MEMORY;
-
-	vm_address_space*&amp; space = _CurrentItem();
-	space = vm_get_address_space_by_id(team);
-	if (space == NULL)
+	if (!space)
 		return B_BAD_VALUE;
 
-	// check if we have already added this address space
-	if (_HasAddressSpace(space)) {
-		// no need to add it again
+	int32 index = _IndexOfAddressSpace(space);
+	if (index &lt; 0) {
+		if (!_ResizeIfNeeded()) {
+			vm_put_address_space(space);
+			return B_NO_MEMORY;
+		}
+
+		lock_item&amp; item = fItems[fCount++];
+		item.space = space;
+		item.write_lock = writeLock;
+	} else {
+
+		// one reference is enough
 		vm_put_address_space(space);
-		if (_space != NULL)
-			*_space = space;
-		return B_OK;
+
+		fItems[index].write_lock |= writeLock;
 	}
 
-	fCount++;
-
 	if (_space != NULL)
 		*_space = space;
 
@@ -478,32 +532,21 @@
 }
 
 
-status_t
-MultiAddressSpaceLocker::AddArea(area_id area, vm_address_space** _space)
+inline status_t
+MultiAddressSpaceLocker::AddTeam(team_id team, bool writeLock,
+	vm_address_space** _space)
 {
-	if (!_ResizeIfNeeded())
-		return B_NO_MEMORY;
+	return _AddAddressSpace(vm_get_address_space_by_id(team), writeLock,
+		_space);
+}
 
-	vm_address_space*&amp; space = _CurrentItem();
-	space = get_address_space_by_area_id(area);
-	if (space == NULL)
-		return B_BAD_VALUE;
 
-	// check if we have already added this address space
-	if (_HasAddressSpace(space)) {
-		// no need to add it again
-		vm_put_address_space(space);
-		if (_space != NULL)
-			*_space = space;
-		return B_OK;
-	}
-
-	fCount++;
-
-	if (_space != NULL)
-		*_space = space;
-
-	return B_OK;
+inline status_t
+MultiAddressSpaceLocker::AddArea(area_id area, bool writeLock,
+	vm_address_space** _space)
+{
+	return _AddAddressSpace(get_address_space_by_area_id(area), writeLock,
+		_space);
 }
 
 
@@ -512,9 +555,8 @@
 {
 	Unlock();
 
-	for (int32 i = 0; i &lt; fCount; i++) {
-		vm_put_address_space(fSpaces[i]);
-	}
+	for (int32 i = 0; i &lt; fCount; i++)
+		vm_put_address_space(fItems[i].space);
 
 	fCount = 0;
 }
@@ -523,13 +565,17 @@
 status_t
 MultiAddressSpaceLocker::Lock()
 {
-	qsort(fSpaces, fCount, sizeof(void*), &amp;_CompareItems);
+	ASSERT(!fLocked);
 
+	qsort(fItems, fCount, sizeof(lock_item), &amp;_CompareItems);
+
 	for (int32 i = 0; i &lt; fCount; i++) {
-		status_t status = acquire_sem_etc(fSpaces[i]-&gt;sem, WRITE_COUNT, 0, 0);
+		status_t status = acquire_sem_etc(fItems[i].space-&gt;sem,
+			fItems[i].write_lock ? WRITE_COUNT : READ_COUNT, 0, 0);
 		if (status &lt; B_OK) {
 			while (--i &gt;= 0) {
-				release_sem_etc(fSpaces[i]-&gt;sem, WRITE_COUNT, 0);
+				release_sem_etc(fItems[i].space-&gt;sem,
+					fItems[i].write_lock ? WRITE_COUNT : READ_COUNT, 0);
 			}
 			return status;
 		}
@@ -547,13 +593,135 @@
 		return;
 
 	for (int32 i = 0; i &lt; fCount; i++) {
-		release_sem_etc(fSpaces[i]-&gt;sem, WRITE_COUNT, 0);
+		release_sem_etc(fItems[i].space-&gt;sem,
+			fItems[i].write_lock ? WRITE_COUNT : READ_COUNT, 0);
 	}
 
 	fLocked = false;
 }
 
 
+/*!	Adds all address spaces of the areas associated with the given area's cache,
+	locks them, and locks the cache (including a reference to it). It retries
+	until the situation is stable (i.e. the neither cache nor cache's areas
+	changed) or an error occurs. If \c checkNoCacheChange ist \c true it does
+	not return until all areas' \c no_cache_change flags is clear.
+*/
+status_t
+MultiAddressSpaceLocker::AddAreaCacheAndLock(area_id areaID,
+	bool writeLockThisOne, bool writeLockOthers, vm_area*&amp; _area,
+	vm_cache** _cache, bool checkNoCacheChange)
+{
+	// remember the original state
+	int originalCount = fCount;
+	lock_item* originalItems = NULL;
+	if (fCount &gt; 0) {
+		originalItems = new(nothrow) lock_item[fCount];
+		if (originalItems == NULL)
+			return B_NO_MEMORY;
+		memcpy(originalItems, fItems, fCount * sizeof(lock_item));
+	}
+	ArrayDeleter&lt;lock_item&gt; _(originalItems);
+
+	// get the cache
+	vm_cache* cache;
+	vm_area* area;
+	status_t error;
+	{
+		AddressSpaceReadLocker locker;
+		error = locker.SetFromArea(areaID, area);
+		if (error != B_OK)
+			return error;
+
+		cache = vm_area_get_locked_cache(area);
+	}
+
+	while (true) {
+		// add all areas
+		vm_area* firstArea = cache-&gt;areas;
+		for (vm_area* current = firstArea; current;
+				current = current-&gt;cache_next) {
+			error = AddArea(current-&gt;id,
+				current == area ? writeLockThisOne : writeLockOthers);
+			if (error != B_OK) {
+				vm_area_put_locked_cache(cache);
+				return error;
+			}
+		}
+
+		// unlock the cache and attempt to lock the address spaces
+		vm_area_put_locked_cache(cache);
+
+		error = Lock();
+		if (error != B_OK)
+			return error;
+
+		// lock the cache again and check whether anything has changed
+
+		// check whether the area is gone in the meantime
+		acquire_sem_etc(sAreaHashLock, READ_COUNT, 0, 0);
+		area = (vm_area *)hash_lookup(sAreaHash, &amp;areaID);
+		release_sem_etc(sAreaHashLock, READ_COUNT, 0);
+
+		if (area == NULL) {
+			Unlock();
+			return B_BAD_VALUE;
+		}
+
+		// lock the cache
+		vm_cache* oldCache = cache;
+		cache = vm_area_get_locked_cache(area);
+
+		// If neither the area's cache has changed nor its area list we're
+		// done...
+		bool done = (cache == oldCache || firstArea == cache-&gt;areas);
+
+		// ... unless we're supposed to check the areas' &quot;no_cache_change&quot; flag
+		bool yield = false;
+		if (done &amp;&amp; checkNoCacheChange) {
+			for (vm_area *tempArea = cache-&gt;areas; tempArea != NULL;
+					tempArea = tempArea-&gt;cache_next) {
+				if (tempArea-&gt;no_cache_change) {
+					done = false;
+					yield = true;
+					break;
+				}
+			}
+		}
+
+		// If everything looks dandy, return the values.
+		if (done) {
+			_area = area;
+			if (_cache != NULL)
+				*_cache = cache;
+			return B_OK;
+		}
+
+		// Restore the original state and try again.
+
+		// Unlock the address spaces, but keep the cache locked for the next
+		// iteration.
+		Unlock();
+
+		// Get an additional reference to the original address spaces.
+		for (int32 i = 0; i &lt; originalCount; i++)
+			atomic_add(&amp;originalItems[i].space-&gt;ref_count, 1);
+
+		// Release all references to the current address spaces.
+		for (int32 i = 0; i &lt; fCount; i++)
+			vm_put_address_space(fItems[i].space);
+
+		// Copy over the original state.
+		fCount = originalCount;
+		if (originalItems != NULL)
+			memcpy(fItems, originalItems, fCount * sizeof(lock_item));
+
+		if (yield)
+			thread_yield();
+	}
+}
+
+
 //	#pragma mark -
 
 
@@ -1738,12 +1906,12 @@
 
 	MultiAddressSpaceLocker locker;
 	vm_address_space *sourceAddressSpace;
-	status_t status = locker.AddArea(sourceID, &amp;sourceAddressSpace);
+	status_t status = locker.AddArea(sourceID, false, &amp;sourceAddressSpace);
 	if (status != B_OK)
 		return status;
 
 	vm_address_space *targetAddressSpace;
-	status = locker.AddTeam(team, &amp;targetAddressSpace);
+	status = locker.AddTeam(team, true, &amp;targetAddressSpace);
 	if (status != B_OK)
 		return status;
 
@@ -1900,11 +2068,19 @@
 }
 
 
+/*!	Creates a new cache on top of given cache, moves all areas from
+	the old cache to the new one, and changes the protection of all affected
+	areas' pages to read-only.
+	Preconditions:
+	- The given cache must be locked.
+	- All of the cache's areas' address spaces must be read locked.
+	- All of the cache's areas must have a clear \c no_cache_change flags.
+*/
 static status_t
-vm_copy_on_write_area(vm_area *area)
+vm_copy_on_write_area(vm_cache* lowerCache)
 {
 	vm_store *store;
-	vm_cache *upperCache, *lowerCache;
+	vm_cache *upperCache;
 	vm_page *page;
 	status_t status;
 
@@ -1913,33 +2089,15 @@
 	// We need to separate the cache from its areas. The cache goes one level
 	// deeper and we create a new cache inbetween.
 
-	bool noCacheChange;
-	do {
-		lowerCache = vm_area_get_locked_cache(area);
-		noCacheChange = false;
-
-		for (vm_area *tempArea = lowerCache-&gt;areas; tempArea != NULL;
-				tempArea = tempArea-&gt;cache_next) {
-			if (tempArea-&gt;no_cache_change) {
-				noCacheChange = true;
-				vm_area_put_locked_cache(lowerCache);
-				thread_yield();
-				break;
-			}
-		}
-	} while (noCacheChange);
-
 	// create an anonymous store object
 	store = vm_store_create_anonymous_noswap(false, 0, 0);
-	if (store == NULL) {
-		status = B_NO_MEMORY;
-		goto err1;
-	}
+	if (store == NULL)
+		return B_NO_MEMORY;
 
 	upperCache = vm_cache_create(store);
 	if (upperCache == NULL) {
-		status = B_NO_MEMORY;
-		goto err2;
+		store-&gt;ops-&gt;destroy(store);
+		return B_NO_MEMORY;
 	}
 
 	mutex_lock(&amp;upperCache-&gt;lock);
@@ -1958,6 +2116,8 @@
 
 	for (vm_area *tempArea = upperCache-&gt;areas; tempArea != NULL;
 			tempArea = tempArea-&gt;cache_next) {
+		ASSERT(!tempArea-&gt;no_cache_change);
+
 		tempArea-&gt;cache = upperCache;
 		atomic_add(&amp;upperCache-&gt;ref_count, 1);
 		atomic_add(&amp;lowerCache-&gt;ref_count, -1);
@@ -1972,7 +2132,6 @@
 
 	for (vm_area *tempArea = upperCache-&gt;areas; tempArea != NULL;
 			tempArea = tempArea-&gt;cache_next) {
-// TODO: Don't we have to lock the area's address space for accessing base, size, and protection?
 		// The area must be readable in the same way it was previously writable
 		uint32 protection = B_KERNEL_READ_AREA;
 		if (tempArea-&gt;protection &amp; B_READ_AREA)
@@ -1984,16 +2143,9 @@
 		map-&gt;ops-&gt;unlock(map);
 	}
 
-	vm_area_put_locked_cache(lowerCache);
 	vm_area_put_locked_cache(upperCache);
 
 	return B_OK;
-
-err2:
-	store-&gt;ops-&gt;destroy(store);
-err1:
-	vm_area_put_locked_cache(lowerCache);
-	return status;
 }
 
 
@@ -2010,23 +2162,22 @@
 			protection |= B_KERNEL_WRITE_AREA;
 	}
 
+	// Do the locking: target address space, all address spaces associated with
+	// the source cache, and the cache itself.
 	MultiAddressSpaceLocker locker;
-	vm_address_space *sourceAddressSpace = NULL;
 	vm_address_space *targetAddressSpace;
-	status_t status = locker.AddTeam(team, &amp;targetAddressSpace);
-	if (status == B_OK)
-		status = locker.AddArea(sourceID, &amp;sourceAddressSpace);
-	if (status == B_OK)
-		status = locker.Lock();
+	vm_cache *cache;
+	vm_area* source;
+	status_t status = locker.AddTeam(team, true, &amp;targetAddressSpace);
+	if (status == B_OK) {
+		status = locker.AddAreaCacheAndLock(sourceID, false, false, source,
+			&amp;cache, true);
+	}
 	if (status != B_OK)
 		return status;
 
-	vm_area* source = lookup_area(sourceAddressSpace, sourceID);
-	if (source == NULL)
-		return B_BAD_VALUE;
+	AreaCacheLocker cacheLocker(cache);	// already locked
 
-	vm_cache *cache = vm_area_get_locked_cache(source);
-
 	if (addressSpec == B_CLONE_ADDRESS) {
 		addressSpec = B_EXACT_ADDRESS;
 		*_address = (void *)source-&gt;base;
@@ -2034,31 +2185,19 @@
 
 	// First, create a cache on top of the source area
 
-	if (!writableCopy) {
-		// map_backing_store() cannot know it has to acquire a ref to
-		// the store for REGION_NO_PRIVATE_MAP
-		vm_cache_acquire_ref(cache);
-	}
-
 	vm_area *target;
 	status = map_backing_store(targetAddressSpace, cache, _address,
 		source-&gt;cache_offset, source-&gt;size, addressSpec, source-&gt;wiring,
-		protection, writableCopy ? REGION_PRIVATE_MAP : REGION_NO_PRIVATE_MAP,
-		&amp;target, name);
+		protection, REGION_PRIVATE_MAP, &amp;target, name);
 
-	vm_area_put_locked_cache(cache);
-
-	if (status &lt; B_OK) {
-		if (!writableCopy)
-			vm_cache_release_ref(cache);
+	if (status &lt; B_OK)
 		return status;
-	}
 
 	// If the source area is writable, we need to move it one layer up as well
 
 	if ((source-&gt;protection &amp; (B_KERNEL_WRITE_AREA | B_WRITE_AREA)) != 0) {
 		// ToDo: do something more useful if this fails!
-		if (vm_copy_on_write_area(source) &lt; B_OK)
+		if (vm_copy_on_write_area(cache) &lt; B_OK)
 			panic(&quot;vm_copy_on_write_area() failed!\n&quot;);
 	}
 
@@ -2093,12 +2232,17 @@
 	if (!arch_vm_supports_protection(newProtection))
 		return B_NOT_SUPPORTED;
 
-	AddressSpaceReadLocker locker;
+	// lock address spaces and cache
+	MultiAddressSpaceLocker locker;
+	vm_cache *cache;
 	vm_area* area;
-	status_t status = locker.SetFromArea(areaID, area);
-	if (status != B_OK)
-		return status;
+	status_t status = locker.AddAreaCacheAndLock(areaID, true, false, area,
+		&amp;cache, true);
+	AreaCacheLocker cacheLocker(cache);	// already locked
 
+	if (area-&gt;protection == newProtection)
+		return B_OK;
+
 	if (team != vm_kernel_address_space_id()
 		&amp;&amp; area-&gt;address_space-&gt;id != team) {
 		// unless you're the kernel, you are only allowed to set
@@ -2106,11 +2250,11 @@
 		return B_NOT_ALLOWED;
 	}
 
-	vm_cache *cache = vm_area_get_locked_cache(area);
+	bool changePageProtection = true;
 
 	if ((area-&gt;protection &amp; (B_WRITE_AREA | B_KERNEL_WRITE_AREA)) != 0
 		&amp;&amp; (newProtection &amp; (B_WRITE_AREA | B_KERNEL_WRITE_AREA)) == 0) {
-		// change from read/write to read-only
+		// writable -&gt; !writable
 
 		if (cache-&gt;source != NULL &amp;&amp; cache-&gt;temporary) {
 			if (count_writable_areas(cache, area) == 0) {
@@ -2134,43 +2278,61 @@
 		}
 	} else if ((area-&gt;protection &amp; (B_WRITE_AREA | B_KERNEL_WRITE_AREA)) == 0
 		&amp;&amp; (newProtection &amp; (B_WRITE_AREA | B_KERNEL_WRITE_AREA)) != 0) {
-		// change from read-only to read/write
+		// !writable -&gt; writable
 
-		// ToDo: if this is a shared cache, insert new cache (we only know about other
-		//	areas in this cache yet, though, not about child areas)
-		//	-&gt; use this call with care, it might currently have unwanted consequences
-		//	   because of this. It should always be safe though, if there are no other
-		//	   (child) areas referencing this area's cache (you just might not know).
-		if (count_writable_areas(cache, area) == 0
-			&amp;&amp; (cache-&gt;areas != area || area-&gt;cache_next)) {
-			// ToDo: child areas are not tested for yet
-			dprintf(&quot;set_area_protection(): warning, would need to insert a new cache (not yet implemented)!\n&quot;);
-			status = B_NOT_ALLOWED;
-		} else
-			dprintf(&quot;set_area_protection() may not work correctly yet in this direction!\n&quot;);
+		if (!list_is_empty(&amp;cache-&gt;consumers)) {
+			// There are consumers -- we have to insert a new cache. Fortunately
+			// vm_copy_on_write_area() does everything that's needed.
+			changePageProtection = false;
+			status = vm_copy_on_write_area(cache);
+		} else {
+			// No consumers, so we don't need to insert a new one.
+			if (cache-&gt;source != NULL &amp;&amp; cache-&gt;temporary) {
+				// the cache's commitment must contain all possible pages
+				status = cache-&gt;store-&gt;ops-&gt;commit(cache-&gt;store,
+					cache-&gt;virtual_size);
+			}
 
-		if (status == B_OK &amp;&amp; cache-&gt;source != NULL &amp;&amp; cache-&gt;temporary) {
-			// the cache's commitment must contain all possible pages
-			status = cache-&gt;store-&gt;ops-&gt;commit(cache-&gt;store,
-				cache-&gt;virtual_size);
+			if (status == B_OK &amp;&amp; cache-&gt;source != NULL) {
+				// There's a source cache, hence we can't just change all pages'
+				// protection or we might allow writing into pages belonging to
+				// a lower cache.
+				changePageProtection = false;
+
+				struct vm_translation_map *map
+					= &amp;area-&gt;address_space-&gt;translation_map;
+				map-&gt;ops-&gt;lock(map);
+
+				vm_page* page = cache-&gt;page_list;
+				while (page) {
+					addr_t address = area-&gt;base
+						+ (page-&gt;cache_offset &lt;&lt; PAGE_SHIFT);
+					map-&gt;ops-&gt;protect(map, area-&gt;base, area-&gt;base + area-&gt;size,
+						newProtection);
+					page = page-&gt;cache_next;
+				}
+
+				map-&gt;ops-&gt;unlock(map);
+			}
 		}
 	} else {
 		// we don't have anything special to do in all other cases
 	}
 
-	if (status == B_OK &amp;&amp; area-&gt;protection != newProtection) {
+	if (status == B_OK) {
 		// remap existing pages in this cache
-		struct vm_translation_map *map = &amp;locker.AddressSpace()-&gt;translation_map;
+		struct vm_translation_map *map = &amp;area-&gt;address_space-&gt;translation_map;
 
-		map-&gt;ops-&gt;lock(map);
-		map-&gt;ops-&gt;protect(map, area-&gt;base, area-&gt;base + area-&gt;size,
-			newProtection);
-		map-&gt;ops-&gt;unlock(map);
+		if (changePageProtection) {
+			map-&gt;ops-&gt;lock(map);
+			map-&gt;ops-&gt;protect(map, area-&gt;base, area-&gt;base + area-&gt;size,
+				newProtection);
+			map-&gt;ops-&gt;unlock(map);
+		}
 
 		area-&gt;protection = newProtection;
 	}
 
-	vm_area_put_locked_cache(cache);
 	return status;
 }
 
@@ -2610,6 +2772,27 @@
 }
 
 
+#if DEBUG_CACHE_LIST
+
+static int
+dump_caches(int argc, char **argv)
+{
+	kprintf(&quot;caches:&quot;);
+
+	vm_cache* cache = gDebugCacheList;
+	while (cache) {
+		kprintf(&quot; %p&quot;, cache);
+		cache = cache-&gt;debug_next;
+	}
+
+	kprintf(&quot;\n&quot;);
+
+	return 0;
+}
+
+#endif	// DEBUG_CACHE_LIST
+
+
 static const char *
 cache_type_to_string(int32 type)
 {
@@ -3293,6 +3476,9 @@
 	add_debugger_command(&quot;area&quot;, &amp;dump_area, &quot;Dump info about a particular area&quot;);
 	add_debugger_command(&quot;cache&quot;, &amp;dump_cache, &quot;Dump vm_cache&quot;);
 	add_debugger_command(&quot;cache_tree&quot;, &amp;dump_cache_tree, &quot;Dump vm_cache tree&quot;);
+#if DEBUG_CACHE_LIST
+	add_debugger_command(&quot;caches&quot;, &amp;dump_caches, &quot;List vm_cache structures&quot;);
+#endif
 	add_debugger_command(&quot;avail&quot;, &amp;dump_available_memory, &quot;Dump available memory&quot;);
 	add_debugger_command(&quot;dl&quot;, &amp;display_mem, &quot;dump memory long words (64-bit)&quot;);
 	add_debugger_command(&quot;dw&quot;, &amp;display_mem, &quot;dump memory words (32-bit)&quot;);
@@ -4604,73 +4790,29 @@
 status_t
 resize_area(area_id areaID, size_t newSize)
 {
-	vm_area* first;
-	vm_cache* cache;
-
 	// is newSize a multiple of B_PAGE_SIZE?
 	if (newSize &amp; (B_PAGE_SIZE - 1))
 		return B_BAD_VALUE;
 
+	// lock all affected address spaces and the cache
+	vm_area* area;
+	vm_cache* cache;
+
 	MultiAddressSpaceLocker locker;
-	vm_address_space* addressSpace;
-	status_t status = locker.AddArea(areaID, &amp;addressSpace);
+	status_t status = locker.AddAreaCacheAndLock(areaID, true, true, area,
+		&amp;cache);
 	if (status != B_OK)
 		return status;
+	AreaCacheLocker cacheLocker(cache);	// already locked
 
-	{
-		AddressSpaceReadLocker readLocker;
-		vm_area* area;
-		status = readLocker.SetFromArea(areaID, area);
-		if (status != B_OK)
-			return status;
-
-		// collect areas to lock their address spaces
-
-		vm_cache* cache = vm_area_get_locked_cache(area);
-		readLocker.Unlock();
-
-		first = cache-&gt;areas;
-		for (vm_area* current = first; current; current = current-&gt;cache_next) {
-			if (current == area)
-				continue;
-
-			status = locker.AddArea(current-&gt;id);
-			if (status != B_OK) {
-				vm_area_put_locked_cache(cache);
-				return status;
-			}
-		}
-
-		vm_area_put_locked_cache(cache);
-	}
-
-	status = locker.Lock();
-	if (status != B_OK)
-		return status;
-
-	vm_area* area = lookup_area(addressSpace, areaID);
-	if (area == NULL)
-		return B_BAD_VALUE;
-
-	cache = vm_area_get_locked_cache(area);
 	size_t oldSize = area-&gt;size;
+	if (newSize == oldSize)
+		return B_OK;
 
-	// Check if we have locked all address spaces we need to; since
-	// new entries are put at the top, we just have to check if the
-	// first area of the cache changed - we do not care if we have
-	// locked more address spaces, though.
-	if (cache-&gt;areas != first) {
-		// TODO: just try again in this case!
-		status = B_ERROR;
-		goto out;
-	}
-
 	// Resize all areas of this area's cache
 
-	if (cache-&gt;type != CACHE_TYPE_RAM) {
-		status = B_NOT_ALLOWED;
-		goto out;
-	}
+	if (cache-&gt;type != CACHE_TYPE_RAM)
+		return B_NOT_ALLOWED;
 
 	if (oldSize &lt; newSize) {
 		// We need to check if all areas of this cache can be resized
@@ -4689,8 +4831,7 @@
 					&amp;&amp; next-&gt;base - 1 + next-&gt;size &gt;= current-&gt;base - 1 + newSize)
 					continue;
 
-				status = B_ERROR;
-				goto out;
+				return B_ERROR;
 			}
 		}
 	}
@@ -4738,9 +4879,6 @@
 		}
 	}
 
-out:
-	vm_area_put_locked_cache(cache);
-
 	// ToDo: we must honour the lock restrictions of this area
 	return status;
 }

Modified: haiku/trunk/src/system/kernel/vm/vm_cache.cpp
===================================================================
--- haiku/trunk/src/system/kernel/vm/vm_cache.cpp	2007-09-02 22:21:26 UTC (rev 22151)
+++ haiku/trunk/src/system/kernel/vm/vm_cache.cpp	2007-09-02 22:55:23 UTC (rev 22152)
@@ -38,6 +38,12 @@
 static hash_table *sPageCacheTable;
 static spinlock sPageCacheTableLock;
 
+#if DEBUG_CACHE_LIST
+vm_cache* gDebugCacheList;
+static spinlock sDebugCacheListLock;
+#endif
+
+
 struct page_lookup_key {
 	uint32	offset;
 	vm_cache *cache;
@@ -120,6 +126,21 @@
 	cache-&gt;page_count = 0;
 	cache-&gt;busy = false;
 
+
+#if DEBUG_CACHE_LIST
+	int state = disable_interrupts();
+	acquire_spinlock(&amp;sDebugCacheListLock);
+
+	if (gDebugCacheList)
+		gDebugCacheList-&gt;debug_previous = cache;
+	cache-&gt;debug_previous = NULL;
+	cache-&gt;debug_next = gDebugCacheList;
+	gDebugCacheList = cache;
+
+	release_spinlock(&amp;sDebugCacheListLock);
+	restore_interrupts(state);
+#endif
+
 	// connect the store to its cache
 	cache-&gt;store = store;
 	store-&gt;cache = cache;
@@ -198,6 +219,21 @@
 	if (!list_is_empty(&amp;cache-&gt;consumers))

[... truncated: 21 lines follow ...]

</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="003777.html">[Haiku-commits] r22151 - in haiku/trunk: headers/private/kernel	src/system/kernel
</A></li>
	<LI>Next message: <A HREF="003779.html">[Haiku-commits] r22153 - haiku/trunk/src/system/kernel/arch/x86
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#3778">[ date ]</a>
              <a href="thread.html#3778">[ thread ]</a>
              <a href="subject.html#3778">[ subject ]</a>
              <a href="author.html#3778">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/haiku-commits">More information about the Haiku-commits
mailing list</a><br>
</body></html>
