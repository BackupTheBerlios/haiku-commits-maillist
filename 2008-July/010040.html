<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Haiku-commits] r26271 - in haiku/branches/developer/bonefish/vm:	headers/private/kernel src/system/kernel/cache	src/system/kernel/fs src/system/kernel/vm
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/haiku-commits/2008-July/index.html" >
   <LINK REL="made" HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r26271%20-%20in%20haiku/branches/developer/bonefish/vm%3A%0A%09headers/private/kernel%20src/system/kernel/cache%0A%09src/system/kernel/fs%20src/system/kernel/vm&In-Reply-To=%3C200807060107.m6617ivY006133%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="010039.html">
   <LINK REL="Next"  HREF="010041.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Haiku-commits] r26271 - in haiku/branches/developer/bonefish/vm:	headers/private/kernel src/system/kernel/cache	src/system/kernel/fs src/system/kernel/vm</H1>
    <B>bonefish at mail.berlios.de</B> 
    <A HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r26271%20-%20in%20haiku/branches/developer/bonefish/vm%3A%0A%09headers/private/kernel%20src/system/kernel/cache%0A%09src/system/kernel/fs%20src/system/kernel/vm&In-Reply-To=%3C200807060107.m6617ivY006133%40sheep.berlios.de%3E"
       TITLE="[Haiku-commits] r26271 - in haiku/branches/developer/bonefish/vm:	headers/private/kernel src/system/kernel/cache	src/system/kernel/fs src/system/kernel/vm">bonefish at mail.berlios.de
       </A><BR>
    <I>Sun Jul  6 03:07:44 CEST 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="010039.html">[Haiku-commits] r26270 -	haiku/branches/developer/bonefish/vm/src/system/kernel/vm
</A></li>
        <LI>Next message: <A HREF="010041.html">[Haiku-commits] r26272 - in haiku/trunk: build/jam	src/add-ons/kernel/bus_managers/firewire
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#10040">[ date ]</a>
              <a href="thread.html#10040">[ thread ]</a>
              <a href="subject.html#10040">[ subject ]</a>
              <a href="author.html#10040">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: bonefish
Date: 2008-07-06 03:07:31 +0200 (Sun, 06 Jul 2008)
New Revision: 26271
ViewCVS: <A HREF="http://svn.berlios.de/viewcvs/haiku?rev=26271&amp;view=rev">http://svn.berlios.de/viewcvs/haiku?rev=26271&amp;view=rev</A>

Modified:
   haiku/branches/developer/bonefish/vm/headers/private/kernel/vm_cache.h
   haiku/branches/developer/bonefish/vm/headers/private/kernel/vm_types.h
   haiku/branches/developer/bonefish/vm/src/system/kernel/cache/file_cache.cpp
   haiku/branches/developer/bonefish/vm/src/system/kernel/fs/vfs.cpp
   haiku/branches/developer/bonefish/vm/src/system/kernel/vm/vm.cpp
   haiku/branches/developer/bonefish/vm/src/system/kernel/vm/vm_cache.cpp
   haiku/branches/developer/bonefish/vm/src/system/kernel/vm/vm_daemons.cpp
   haiku/branches/developer/bonefish/vm/src/system/kernel/vm/vm_page.cpp
Log:
* Turned most of the vm_cache_*() functions into VMCache methods.
* Changed the VMCache ref count and locking concept. References can only
  be acquired or released with the cache locked. A cache is destroyed
  when it has no more references when being unlocked. Unlocking is also
  the point when a cache is merged with its only consumer. It must have
  ref count 1 in this case.
* Removed VMCache::busy. It is no longer needed due to the ref
  count/locking changes. Merging only happens when no one else has
  reference to the cache. This also simplifies fault_find_page().


Modified: haiku/branches/developer/bonefish/vm/headers/private/kernel/vm_cache.h
===================================================================
--- haiku/branches/developer/bonefish/vm/headers/private/kernel/vm_cache.h	2008-07-06 00:44:17 UTC (rev 26270)
+++ haiku/branches/developer/bonefish/vm/headers/private/kernel/vm_cache.h	2008-07-06 01:07:31 UTC (rev 26271)
@@ -15,30 +15,14 @@
 
 struct kernel_args;
 
-//typedef struct vm_store vm_store;
 
-
 #ifdef __cplusplus
 extern &quot;C&quot; {
 #endif
 
 status_t vm_cache_init(struct kernel_args *args);
-void vm_cache_acquire_ref(struct VMCache *cache);
-void vm_cache_release_ref(struct VMCache *cache);
-struct VMCache *vm_cache_acquire_page_cache_ref(struct vm_page *page);
-struct vm_page *vm_cache_lookup_page(struct VMCache *cache, off_t page);
-void vm_cache_insert_page(struct VMCache *cache, struct vm_page *page,
-	off_t offset);
-void vm_cache_remove_page(struct VMCache *cache, struct vm_page *page);
-void vm_cache_remove_consumer(struct VMCache *cache, struct VMCache *consumer);
-void vm_cache_add_consumer_locked(struct VMCache *cache,
-	struct VMCache *consumer);
-status_t vm_cache_write_modified(struct VMCache *cache, bool fsReenter);
-status_t vm_cache_set_minimal_commitment_locked(struct VMCache *cache,
-	off_t commitment);
-status_t vm_cache_resize(struct VMCache *cache, off_t newSize);
-status_t vm_cache_insert_area_locked(struct VMCache *cache, vm_area *area);
-status_t vm_cache_remove_area(struct VMCache *cache, struct vm_area *area);
+struct VMCache *vm_cache_acquire_locked_page_cache(struct vm_page *page,
+	bool dontWait);
 
 #ifdef __cplusplus
 }

Modified: haiku/branches/developer/bonefish/vm/headers/private/kernel/vm_types.h
===================================================================
--- haiku/branches/developer/bonefish/vm/headers/private/kernel/vm_types.h	2008-07-06 00:44:17 UTC (rev 26270)
+++ haiku/branches/developer/bonefish/vm/headers/private/kernel/vm_types.h	2008-07-06 01:07:31 UTC (rev 26271)
@@ -13,6 +13,7 @@
 #include &lt;arch/vm_translation_map.h&gt;
 #include &lt;condition_variable.h&gt;
 #include &lt;kernel.h&gt;
+#include &lt;lock.h&gt;
 #include &lt;util/DoublyLinkedQueue.h&gt;
 
 #include &lt;sys/uio.h&gt;
@@ -175,6 +176,42 @@
 
 	virtual	void		Delete();
 
+			bool		Lock()
+							{ return mutex_lock(&amp;fLock) == B_OK; }
+			bool		TryLock()
+							{ return mutex_trylock(&amp;fLock) == B_OK; }
+			bool		SwitchLock(mutex* from)
+							{ return mutex_switch_lock(from, &amp;fLock) == B_OK; }
+			void		Unlock();
+			void		AssertLocked()
+							{ ASSERT_LOCKED_MUTEX(&amp;fLock); }
+
+			void		AcquireRefLocked();
+			void		AcquireRef();
+			void		ReleaseRefLocked();
+			void		ReleaseRef();
+			void		ReleaseRefAndUnlock()
+							{ ReleaseRefLocked(); Unlock(); }
+
+			vm_page*	LookupPage(off_t offset);
+			void		InsertPage(vm_page* page, off_t offset);
+			void		RemovePage(vm_page* page);
+
+			void		AddConsumer(VMCache* consumer);
+
+			status_t	InsertAreaLocked(vm_area* area);
+			status_t	RemoveArea(vm_area* area);
+
+			status_t	WriteModified(bool fsReenter);
+			status_t	SetMinimalCommitment(off_t commitment);
+			status_t	Resize(off_t newSize);
+
+			// for debugging only
+			mutex*		GetLock()
+							{ return &fLock; }
+			int32		RefCount() const
+							{ return fRefCount; }
+
 	// backing store operations
 	virtual	status_t	Commit(off_t size);
 	virtual	bool		HasPage(off_t offset);
@@ -192,10 +229,15 @@
 	virtual	void		AcquireStoreRef();
 	virtual	void		ReleaseStoreRef();
 
+private:
+	inline	bool		_IsMergeable() const;
+
+			void		_MergeWithOnlyConsumer();
+			void		_RemoveConsumer(VMCache* consumer);
+
+
 public:
-	mutex				lock;
 	struct vm_area		*areas;
-	vint32				ref_count;
 	struct list_link	consumer_link;
 	struct list			consumers;
 		// list of caches that use this cache as a source
@@ -208,13 +250,16 @@
 	uint32				page_count;
 	uint32				temporary : 1;
 	uint32				scan_skip : 1;
-	uint32				busy : 1;
-	uint32				type : 5;
+	uint32				type : 6;
 
 #if DEBUG_CACHE_LIST
 	struct VMCache*		debug_previous;
 	struct VMCache*		debug_next;
 #endif
+
+private:
+	int32				fRefCount;
+	mutex				fLock;
 };
 
 typedef VMCache vm_cache;

Modified: haiku/branches/developer/bonefish/vm/src/system/kernel/cache/file_cache.cpp
===================================================================
--- haiku/branches/developer/bonefish/vm/src/system/kernel/cache/file_cache.cpp	2008-07-06 00:44:17 UTC (rev 26270)
+++ haiku/branches/developer/bonefish/vm/src/system/kernel/cache/file_cache.cpp	2008-07-06 01:07:31 UTC (rev 26271)
@@ -119,7 +119,7 @@
 {
 	if (vm_low_memory_state() != B_NO_LOW_MEMORY) {
 		vm_cache *cache = ref-&gt;cache;
-		mutex_lock(&amp;cache-&gt;lock);
+		cache-&gt;Lock();
 
 		if (list_is_empty(&amp;cache-&gt;consumers) &amp;&amp; cache-&gt;areas == NULL
 			&amp;&amp; access_is_sequential(ref)) {
@@ -144,14 +144,14 @@
 						(page = it.Next()) != NULL &amp;&amp; left &gt; 0;) {
 					if (page-&gt;state != PAGE_STATE_MODIFIED
 						&amp;&amp; page-&gt;state != PAGE_STATE_BUSY) {
-						vm_cache_remove_page(cache, page);
+						cache-&gt;RemovePage(page);
 						vm_page_set_state(page, PAGE_STATE_FREE);
 						left--;
 					}
 				}
 			}
 		}
-		mutex_unlock(&amp;cache-&gt;lock);
+		cache-&gt;Unlock();
 	}
 
 	vm_page_reserve_pages(reservePages);
@@ -194,7 +194,7 @@
 
 		busyConditions[pageIndex - 1].Publish(page, &quot;page&quot;);
 
-		vm_cache_insert_page(cache, page, offset + pos);
+		cache-&gt;InsertPage(page, offset + pos);
 
 		addr_t virtualAddress;
 		if (vm_get_physical_page(page-&gt;physical_page_number * B_PAGE_SIZE,
@@ -206,7 +206,7 @@
 	}
 
 	push_access(ref, offset, bufferSize, false);
-	mutex_unlock(&amp;cache-&gt;lock);
+	cache-&gt;Unlock();
 	vm_page_unreserve_pages(lastReservedPages);
 
 	// read file into reserved pages
@@ -227,11 +227,11 @@
 			}
 		}
 
-		mutex_lock(&amp;cache-&gt;lock);
+		cache-&gt;Lock();
 
 		for (int32 i = 0; i &lt; pageIndex; i++) {
 			busyConditions[i].Unpublish();
-			vm_cache_remove_page(cache, pages[i]);
+			cache-&gt;RemovePage(pages[i]);
 			vm_page_set_state(pages[i], PAGE_STATE_FREE);
 		}
 
@@ -261,7 +261,7 @@
 	}
 
 	reserve_pages(ref, reservePages, false);
-	mutex_lock(&amp;cache-&gt;lock);
+	cache-&gt;Lock();
 
 	// make the pages accessible in the cache
 	for (int32 i = pageIndex; i-- &gt; 0;) {
@@ -290,7 +290,7 @@
 	vec.iov_len = bufferSize;
 
 	push_access(ref, offset, bufferSize, false);
-	mutex_unlock(&amp;ref-&gt;cache-&gt;lock);
+	ref-&gt;cache-&gt;Unlock();
 	vm_page_unreserve_pages(lastReservedPages);
 
 	status_t status = vfs_read_pages(ref-&gt;vnode, cookie, offset + pageOffset,
@@ -298,7 +298,7 @@
 	if (status == B_OK)
 		reserve_pages(ref, reservePages, false);
 
-	mutex_lock(&amp;ref-&gt;cache-&gt;lock);
+	ref-&gt;cache-&gt;Lock();
 
 	return status;
 }
@@ -338,7 +338,7 @@
 			PAGE_STATE_FREE, true);
 		busyConditions[pageIndex - 1].Publish(page, &quot;page&quot;);
 
-		vm_cache_insert_page(ref-&gt;cache, page, offset + pos);
+		ref-&gt;cache-&gt;InsertPage(page, offset + pos);
 
 		addr_t virtualAddress;
 		vm_get_physical_page(page-&gt;physical_page_number * B_PAGE_SIZE,
@@ -349,7 +349,7 @@
 	}
 
 	push_access(ref, offset, bufferSize, true);
-	mutex_unlock(&amp;ref-&gt;cache-&gt;lock);
+	ref-&gt;cache-&gt;Unlock();
 	vm_page_unreserve_pages(lastReservedPages);
 
 	// copy contents (and read in partially written pages first)
@@ -431,7 +431,7 @@
 	if (status == B_OK)
 		reserve_pages(ref, reservePages, true);
 
-	mutex_lock(&amp;ref-&gt;cache-&gt;lock);
+	ref-&gt;cache-&gt;Lock();
 
 	// unmap the pages again
 
@@ -480,7 +480,7 @@
 	vec.iov_len = bufferSize;
 
 	push_access(ref, offset, bufferSize, true);
-	mutex_unlock(&amp;ref-&gt;cache-&gt;lock);
+	ref-&gt;cache-&gt;Unlock();
 	vm_page_unreserve_pages(lastReservedPages);
 
 	status_t status = B_OK;
@@ -508,7 +508,7 @@
 	if (status == B_OK)
 		reserve_pages(ref, reservePages, true);
 
-	mutex_lock(&amp;ref-&gt;cache-&gt;lock);
+	ref-&gt;cache-&gt;Lock();
 
 	return status;
 }
@@ -604,11 +604,11 @@
 	size_t reservePages = 0;
 
 	reserve_pages(ref, lastReservedPages, doWrite);
-	MutexLocker locker(cache-&gt;lock);
+	AutoLocker&lt;VMCache&gt; locker(cache);
 
 	while (bytesLeft &gt; 0) {
 		// check if this page is already in memory
-		vm_page *page = vm_cache_lookup_page(cache, offset);
+		vm_page *page = cache-&gt;LookupPage(offset);
 		if (page != NULL) {
 			// The page may be busy - since we need to unlock the cache sometime
 			// in the near future, we need to satisfy the request of the pages
@@ -969,7 +969,7 @@
 
 	TRACE((&quot;file_cache_delete(ref = %p)\n&quot;, ref));
 
-	vm_cache_release_ref(ref-&gt;cache);
+	ref-&gt;cache-&gt;ReleaseRef();
 	delete ref;
 }
 
@@ -984,7 +984,7 @@
 	if (ref == NULL)
 		return B_OK;
 
-	MutexLocker _(ref-&gt;cache-&gt;lock);
+	AutoLocker&lt;VMCache&gt; _(ref-&gt;cache);
 
 	off_t offset = ref-&gt;cache-&gt;virtual_end;
 	off_t size = newSize;
@@ -994,7 +994,7 @@
 	} else
 		size = newSize - offset;
 
-	return vm_cache_resize(ref-&gt;cache, newSize);
+	return ref-&gt;cache-&gt;Resize(newSize);
 }
 
 
@@ -1005,7 +1005,7 @@
 	if (ref == NULL)
 		return B_BAD_VALUE;
 
-	return vm_cache_write_modified(ref-&gt;cache, true);
+	return ref-&gt;cache-&gt;WriteModified(true);
 }
 
 

Modified: haiku/branches/developer/bonefish/vm/src/system/kernel/fs/vfs.cpp
===================================================================
--- haiku/branches/developer/bonefish/vm/src/system/kernel/fs/vfs.cpp	2008-07-06 00:44:17 UTC (rev 26270)
+++ haiku/branches/developer/bonefish/vm/src/system/kernel/fs/vfs.cpp	2008-07-06 01:07:31 UTC (rev 26271)
@@ -770,7 +770,7 @@
 
 	// if we have a vm_cache attached, remove it
 	if (vnode-&gt;cache)
-		vm_cache_release_ref(vnode-&gt;cache);
+		vnode-&gt;cache-&gt;ReleaseRef();
 
 	vnode-&gt;cache = NULL;
 
@@ -1059,7 +1059,7 @@
 		mutex_unlock(&amp;sVnodeMutex);
 
 		if (vnode-&gt;cache != NULL)
-			vm_cache_write_modified(vnode-&gt;cache, false);
+			vnode-&gt;cache-&gt;WriteModified(false);
 
 		dec_vnode_ref_count(vnode, true, false);
 			// this should free the vnode when it's still unused
@@ -3920,7 +3920,7 @@
 vfs_get_vnode_cache(struct vnode *vnode, vm_cache **_cache, bool allocate)
 {
 	if (vnode-&gt;cache != NULL) {
-		vm_cache_acquire_ref(vnode-&gt;cache);
+		vnode-&gt;cache-&gt;AcquireRef();
 		*_cache = vnode-&gt;cache;
 		return B_OK;
 	}
@@ -3946,12 +3946,13 @@
 			status = B_BAD_VALUE;
 	}
 
+	mutex_unlock(&amp;sVnodeMutex);
+
 	if (status == B_OK) {
-		vm_cache_acquire_ref(vnode-&gt;cache);
+		vnode-&gt;cache-&gt;AcquireRef();
 		*_cache = vnode-&gt;cache;
 	}
 
-	mutex_unlock(&amp;sVnodeMutex);
 	return status;
 }
 
@@ -6705,7 +6706,7 @@
 				put_vnode(previousVnode);
 
 			if (vnode-&gt;cache != NULL)
-				vm_cache_write_modified(vnode-&gt;cache, false);
+				vnode-&gt;cache-&gt;WriteModified(false);
 
 			// the next vnode might change until we lock the vnode list again,
 			// but this vnode won't go away since we keep a reference to it.

Modified: haiku/branches/developer/bonefish/vm/src/system/kernel/vm/vm.cpp
===================================================================
--- haiku/branches/developer/bonefish/vm/src/system/kernel/vm/vm.cpp	2008-07-06 00:44:17 UTC (rev 26270)
+++ haiku/branches/developer/bonefish/vm/src/system/kernel/vm/vm.cpp	2008-07-06 01:07:31 UTC (rev 26271)
@@ -1239,7 +1239,7 @@
 		// If no one else uses the area's cache, we can resize it, too.
 		if (cache-&gt;areas == area &amp;&amp; area-&gt;cache_next == NULL
 			&amp;&amp; list_is_empty(&amp;cache-&gt;consumers)) {
-			status_t error = vm_cache_resize(cache, newSize);
+			status_t error = cache-&gt;Resize(newSize);
 			if (error != B_OK)
 				return error;
 		}
@@ -1298,7 +1298,7 @@
 	}
 
 	// We need a cache reference for the new area.
-	vm_cache_acquire_ref(cache);
+	cache-&gt;AcquireRefLocked();
 
 	if (_secondArea != NULL)
 		*_secondArea = secondArea;
@@ -1372,7 +1372,7 @@
 	TRACE((&quot;map_backing_store: aspace %p, cache %p, *vaddr %p, offset 0x%Lx, size %lu, addressSpec %ld, wiring %d, protection %d, _area %p, area_name '%s'\n&quot;,
 		addressSpace, cache, *_virtualAddress, offset, size, addressSpec,
 		wiring, protection, _area, areaName));
-	ASSERT_LOCKED_MUTEX(&amp;cache-&gt;lock);
+	cache-&gt;AssertLocked();
 
 	vm_area *area = create_area_struct(addressSpace, areaName, wiring,
 		protection);
@@ -1393,18 +1393,18 @@
 		if (status != B_OK)
 			goto err1;
 
-		mutex_lock(&amp;newCache-&gt;lock);
+		newCache-&gt;Lock();
 		newCache-&gt;temporary = 1;
 		newCache-&gt;scan_skip = cache-&gt;scan_skip;
 		newCache-&gt;virtual_base = offset;
 		newCache-&gt;virtual_end = offset + size;
 
-		vm_cache_add_consumer_locked(cache, newCache);
+		cache-&gt;AddConsumer(newCache);
 
 		cache = newCache;
 	}
 
-	status = vm_cache_set_minimal_commitment_locked(cache, size);
+	status = cache-&gt;SetMinimalCommitment(size);
 	if (status != B_OK)
 		goto err2;
 
@@ -1432,9 +1432,9 @@
 	area-&gt;cache_offset = offset;
 
 	// point the cache back to the area
-	vm_cache_insert_area_locked(cache, area);
+	cache-&gt;InsertAreaLocked(area);
 	if (mapping == REGION_PRIVATE_MAP)
-		mutex_unlock(&amp;cache-&gt;lock);
+		cache-&gt;Unlock();
 
 	// insert the area in the global area hash table
 	acquire_sem_etc(sAreaHashLock, WRITE_COUNT, 0 ,0);
@@ -1451,11 +1451,10 @@
 	if (mapping == REGION_PRIVATE_MAP) {
 		// We created this cache, so we must delete it again. Note, that we
 		// need to temporarily unlock the source cache or we'll otherwise
-		// deadlock, since vm_cache_remove_consumer will try to lock it too.
-		mutex_unlock(&amp;cache-&gt;lock);
-		mutex_unlock(&amp;sourceCache-&gt;lock);
-		vm_cache_release_ref(cache);
-		mutex_lock(&amp;sourceCache-&gt;lock);
+		// deadlock, since VMCache::_RemoveConsumer() will try to lock it, too.
+		sourceCache-&gt;Unlock();
+		cache-&gt;ReleaseRefAndUnlock();
+		sourceCache-&gt;Lock();
 	}
 err1:
 	free(area-&gt;name);
@@ -1641,19 +1640,19 @@
 			break;
 	}
 
-	mutex_lock(&amp;cache-&gt;lock);
+	cache-&gt;Lock();
 
 	status = map_backing_store(addressSpace, cache, address, 0, size,
 		addressSpec, wiring, protection, REGION_NO_PRIVATE_MAP, &amp;area, name,
 		unmapAddressRange, kernel);
 
-	mutex_unlock(&amp;cache-&gt;lock);
-
 	if (status &lt; B_OK) {
-		vm_cache_release_ref(cache);
+		cache-&gt;ReleaseRefAndUnlock();
 		goto err1;
 	}
 
+	cache-&gt;Unlock();
+
 	locker.DegradeToReadLock();
 
 	switch (wiring) {
@@ -1670,7 +1669,7 @@
 			vm_page_reserve_pages(reservePages);
 
 			// Allocate and map all pages for this area
-			mutex_lock(&amp;cache-&gt;lock);
+			cache-&gt;Lock();
 
 			off_t offset = 0;
 			for (addr_t address = area-&gt;base; address &lt; area-&gt;base + (area-&gt;size - 1);
@@ -1691,11 +1690,11 @@
 					panic(&quot;couldn't fulfill B_FULL lock!&quot;);
 				}
 
-				vm_cache_insert_page(cache, page, offset);
+				cache-&gt;InsertPage(page, offset);
 				vm_map_page(area, page, address, protection);
 			}
 
-			mutex_unlock(&amp;cache-&gt;lock);
+			cache-&gt;Unlock();
 			vm_page_unreserve_pages(reservePages);
 			break;
 		}
@@ -1711,7 +1710,7 @@
 			if (!kernel_startup)
 				panic(&quot;ALREADY_WIRED flag used outside kernel startup\n&quot;);
 
-			mutex_lock(&amp;cache-&gt;lock);
+			cache-&gt;Lock();
 			map-&gt;ops-&gt;lock(map);
 
 			for (addr_t virtualAddress = area-&gt;base; virtualAddress &lt; area-&gt;base
@@ -1734,11 +1733,11 @@
 				page-&gt;wired_count++;
 					// TODO: needs to be atomic on all platforms!
 				vm_page_set_state(page, PAGE_STATE_WIRED);
-				vm_cache_insert_page(cache, page, offset);
+				cache-&gt;InsertPage(page, offset);
 			}
 
 			map-&gt;ops-&gt;unlock(map);
-			mutex_unlock(&amp;cache-&gt;lock);
+			cache-&gt;Unlock();
 			break;
 		}
 
@@ -1754,7 +1753,7 @@
 			off_t offset = 0;
 
 			vm_page_reserve_pages(reservePages);
-			mutex_lock(&amp;cache-&gt;lock);
+			cache-&gt;Lock();
 			map-&gt;ops-&gt;lock(map);
 
 			for (virtualAddress = area-&gt;base; virtualAddress &lt; area-&gt;base
@@ -1772,11 +1771,11 @@
 				page-&gt;wired_count++;
 					// TODO: needs to be atomic on all platforms!
 				vm_page_set_state(page, PAGE_STATE_WIRED);
-				vm_cache_insert_page(cache, page, offset);
+				cache-&gt;InsertPage(page, offset);
 			}
 
 			map-&gt;ops-&gt;unlock(map);
-			mutex_unlock(&amp;cache-&gt;lock);
+			cache-&gt;Unlock();
 			vm_page_unreserve_pages(reservePages);
 			break;
 		}
@@ -1844,17 +1843,17 @@
 	cache-&gt;scan_skip = 1;
 	cache-&gt;virtual_end = size;
 
-	mutex_lock(&amp;cache-&gt;lock);
+	cache-&gt;Lock();
 
 	status = map_backing_store(locker.AddressSpace(), cache, _address,
 		0, size, addressSpec &amp; ~B_MTR_MASK, B_FULL_LOCK, protection,
 		REGION_NO_PRIVATE_MAP, &amp;area, name, false, true);
 
-	mutex_unlock(&amp;cache-&gt;lock);
-
 	if (status &lt; B_OK)
-		vm_cache_release_ref(cache);
+		cache-&gt;ReleaseRefLocked();
 
+	cache-&gt;Unlock();
+
 	if (status &gt;= B_OK &amp;&amp; (addressSpec &amp; B_MTR_MASK) != 0) {
 		// set requested memory type
 		status = arch_vm_set_memory_type(area, physicalAddress,
@@ -1917,19 +1916,19 @@
 	cache-&gt;scan_skip = 1;
 	cache-&gt;virtual_end = size;
 
-	mutex_lock(&amp;cache-&gt;lock);
+	cache-&gt;Lock();
 
 	status = map_backing_store(locker.AddressSpace(), cache, address, 0, size,
 		addressSpec, 0, B_KERNEL_READ_AREA, REGION_NO_PRIVATE_MAP, &amp;area, name,
 		false, true);
 
-	mutex_unlock(&amp;cache-&gt;lock);
-
 	if (status &lt; B_OK) {
-		vm_cache_release_ref(cache);
+		cache-&gt;ReleaseRefAndUnlock();
 		return status;
 	}
 
+	cache-&gt;Unlock();
+
 	area-&gt;cache_type = CACHE_TYPE_NULL;
 	return area-&gt;id;
 }
@@ -2005,19 +2004,20 @@
 	if (status &lt; B_OK)
 		return status;
 
-	mutex_lock(&amp;cache-&gt;lock);
+	cache-&gt;Lock();
 
 	vm_area *area;
 	status = map_backing_store(locker.AddressSpace(), cache, _address,
 		offset, size, addressSpec, 0, protection, mapping, &amp;area, name,
 		addressSpec == B_EXACT_ADDRESS, kernel);
 
-	mutex_unlock(&amp;cache-&gt;lock);
-
 	if (status &lt; B_OK || mapping == REGION_PRIVATE_MAP) {
 		// map_backing_store() cannot know we no longer need the ref
-		vm_cache_release_ref(cache);
+		cache-&gt;ReleaseRefLocked();
 	}
+
+	cache-&gt;Unlock();
+
 	if (status &lt; B_OK)
 		return status;
 
@@ -2041,21 +2041,27 @@
 vm_cache *
 vm_area_get_locked_cache(vm_area *area)
 {
-	MutexLocker locker(sAreaCacheLock);
+	mutex_lock(&amp;sAreaCacheLock);
+
 	while (true) {
 		vm_cache* cache = area-&gt;cache;
-		vm_cache_acquire_ref(cache);
-		locker.Unlock();
 
-		mutex_lock(&amp;cache-&gt;lock);
+		if (!cache-&gt;SwitchLock(&amp;sAreaCacheLock)) {
+			// cache has been deleted
+			mutex_lock(&amp;sAreaCacheLock);
+			continue;
+		}
 
-		locker.Lock();
-		if (cache == area-&gt;cache)
+		mutex_lock(&amp;sAreaCacheLock);
+
+		if (cache == area-&gt;cache) {
+			cache-&gt;AcquireRefLocked();
+			mutex_unlock(&amp;sAreaCacheLock);
 			return cache;
+		}
 
 		// the cache changed in the meantime
-		mutex_unlock(&amp;cache-&gt;lock);
-		vm_cache_release_ref(cache);
+		cache-&gt;Unlock();
 	}
 }
 
@@ -2063,8 +2069,7 @@
 void
 vm_area_put_locked_cache(vm_cache *cache)
 {
-	mutex_unlock(&amp;cache-&gt;lock);
-	vm_cache_release_ref(cache);
+	cache-&gt;ReleaseRefAndUnlock();
 }
 
 
@@ -2121,10 +2126,10 @@
 	}
 	if (status == B_OK &amp;&amp; mapping != REGION_PRIVATE_MAP) {
 		// If the mapping is REGION_PRIVATE_MAP, map_backing_store() needed
-		// to create a new ref, and has therefore already acquired a reference
+		// to create a new cache, and has therefore already acquired a reference
 		// to the source cache - but otherwise it has no idea that we need
 		// one.
-		vm_cache_acquire_ref(cache);
+		cache-&gt;AcquireRefLocked();
 	}
 	if (status == B_OK &amp;&amp; newArea-&gt;wiring == B_FULL_LOCK) {
 		// we need to map in everything at this point
@@ -2226,14 +2231,14 @@
 	vm_unmap_pages(area, area-&gt;base, area-&gt;size, !area-&gt;cache-&gt;temporary);
 
 	if (!area-&gt;cache-&gt;temporary)
-		vm_cache_write_modified(area-&gt;cache, false);
+		area-&gt;cache-&gt;WriteModified(false);
 
 	arch_vm_unset_memory_type(area);
 	remove_area_from_address_space(addressSpace, area);
 	vm_put_address_space(addressSpace);
 
-	vm_cache_remove_area(area-&gt;cache, area);
-	vm_cache_release_ref(area-&gt;cache);
+	area-&gt;cache-&gt;RemoveArea(area);
+	area-&gt;cache-&gt;ReleaseRef();
 
 	free(area-&gt;name);
 	free(area);
@@ -2284,7 +2289,7 @@
 	if (status != B_OK)
 		return status;
 
-	mutex_lock(&amp;upperCache-&gt;lock);
+	upperCache-&gt;Lock();
 
 	upperCache-&gt;temporary = 1;
 	upperCache-&gt;scan_skip = lowerCache-&gt;scan_skip;
@@ -2302,13 +2307,13 @@
 		ASSERT(!tempArea-&gt;no_cache_change);
 
 		tempArea-&gt;cache = upperCache;
-		atomic_add(&amp;upperCache-&gt;ref_count, 1);
-		atomic_add(&amp;lowerCache-&gt;ref_count, -1);
+		upperCache-&gt;AcquireRefLocked();
+		lowerCache-&gt;ReleaseRefLocked();
 	}
 
 	mutex_unlock(&amp;sAreaCacheLock);
 
-	vm_cache_add_consumer_locked(lowerCache, upperCache);
+	lowerCache-&gt;AddConsumer(upperCache);
 
 	// We now need to remap all pages from all of the cache's areas read-only, so that
 	// a copy will be created on next write access
@@ -2382,7 +2387,7 @@
 	if (sharedArea) {
 		// The new area uses the old area's cache, but map_backing_store()
 		// hasn't acquired a ref. So we have to do that now.
-		vm_cache_acquire_ref(cache);
+		cache-&gt;AcquireRefLocked();
 	}
 
 	// If the source area is writable, we need to move it one layer up as well
@@ -3220,16 +3225,16 @@
 	cache = (vm_cache *)address;
 
 	kprintf(&quot;CACHE %p:\n&quot;, cache);
-	kprintf(&quot;  ref_count:    %ld\n&quot;, cache-&gt;ref_count);
+	kprintf(&quot;  ref_count:    %ld\n&quot;, cache-&gt;RefCount());
 	kprintf(&quot;  source:       %p\n&quot;, cache-&gt;source);
 	kprintf(&quot;  type:         %s\n&quot;, cache_type_to_string(cache-&gt;type));
 	kprintf(&quot;  virtual_base: 0x%Lx\n&quot;, cache-&gt;virtual_base);
 	kprintf(&quot;  virtual_end:  0x%Lx\n&quot;, cache-&gt;virtual_end);
 	kprintf(&quot;  temporary:    %ld\n&quot;, cache-&gt;temporary);
 	kprintf(&quot;  scan_skip:    %ld\n&quot;, cache-&gt;scan_skip);
-	kprintf(&quot;  lock:         %p\n&quot;, &amp;cache-&gt;lock);
+	kprintf(&quot;  lock:         %p\n&quot;, cache-&gt;GetLock());
 #ifdef KDEBUG
-	kprintf(&quot;  lock.holder:  %ld\n&quot;, cache-&gt;lock.holder);
+	kprintf(&quot;  lock.holder:  %ld\n&quot;, cache-&gt;GetLock()-&gt;holder);
 #endif
 	kprintf(&quot;  areas:\n&quot;);
 
@@ -4033,23 +4038,13 @@
 static inline status_t
 fault_acquire_locked_source(vm_cache *cache, vm_cache **_source)
 {
-retry:
 	vm_cache *source = cache-&gt;source;
 	if (source == NULL)
 		return B_ERROR;
-	if (source-&gt;busy)
-		return B_BUSY;
 
-	vm_cache_acquire_ref(source);
+	source-&gt;Lock();
+	source-&gt;AcquireRefLocked();
 
-	mutex_lock(&amp;source-&gt;lock);
-
-	if (source-&gt;busy) {
-		mutex_unlock(&amp;source-&gt;lock);
-		vm_cache_release_ref(source);
-		goto retry;
-	}
-
 	*_source = source;
 	return B_OK;
 }
@@ -4064,8 +4059,8 @@
 	off_t cacheOffset)
 {
 	dummyPage.state = PAGE_STATE_BUSY;
-	vm_cache_acquire_ref(cache);
-	vm_cache_insert_page(cache, &amp;dummyPage, cacheOffset);
+	cache-&gt;AcquireRefLocked();
+	cache-&gt;InsertPage(&amp;dummyPage, cacheOffset);
 	dummyPage.busy_condition.Publish(&amp;dummyPage, &quot;page&quot;);
 }
 
@@ -4079,18 +4074,18 @@
 {
 	vm_cache *cache = dummyPage.cache;
 	if (!isLocked)
-		mutex_lock(&amp;cache-&gt;lock);
+		cache-&gt;Lock();
 
 	if (dummyPage.state == PAGE_STATE_BUSY) {
-		vm_cache_remove_page(cache, &amp;dummyPage);
+		cache-&gt;RemovePage(&amp;dummyPage);
 		dummyPage.state = PAGE_STATE_INACTIVE;
 		dummyPage.busy_condition.Unpublish();
 	}
 
-	if (!isLocked)
-		mutex_unlock(&amp;cache-&gt;lock);
+	cache-&gt;ReleaseRefLocked();
 
-	vm_cache_release_ref(cache);
+	if (!isLocked)
+		cache-&gt;Unlock();
 }
 
 
@@ -4112,20 +4107,20 @@
 	vm_cache *lastCache = NULL;
 	vm_page *page = NULL;
 
-	vm_cache_acquire_ref(cache);
-	mutex_lock(&amp;cache-&gt;lock);
+	cache-&gt;Lock();
+	cache-&gt;AcquireRefLocked();
 		// we release this later in the loop
 
 	while (cache != NULL) {
 		if (lastCache != NULL)
-			vm_cache_release_ref(lastCache);
+			lastCache-&gt;ReleaseRefAndUnlock();
 
 		// we hold the lock of the cache at this point
 
 		lastCache = cache;
 
 		for (;;) {
-			page = vm_cache_lookup_page(cache, cacheOffset);
+			page = cache-&gt;LookupPage(cacheOffset);
 			if (page != NULL &amp;&amp; page-&gt;state != PAGE_STATE_BUSY) {
 				// we found the page
 				break;
@@ -4137,23 +4132,10 @@
 			{
 				ConditionVariableEntry entry;
 				entry.Add(page);
-				mutex_unlock(&amp;cache-&gt;lock);
+				cache-&gt;Unlock();
 				entry.Wait();
-				mutex_lock(&amp;cache-&gt;lock);
+				cache-&gt;Lock();
 			}
-
-			if (cache-&gt;busy) {
-				// The cache became busy, which means, it is about to be
-				// removed by vm_cache_remove_consumer(). We start again with
-				// the top cache.
-				ConditionVariableEntry entry;
-				entry.Add(cache);
-				mutex_unlock(&amp;cache-&gt;lock);
-				vm_cache_release_ref(cache);
-				entry.Wait();
-				*_restart = true;
-				return B_OK;
-			}
 		}
 
 		if (page != NULL &amp;&amp; page != &amp;dummyPage)
@@ -4165,12 +4147,12 @@
 		if (cache-&gt;HasPage(cacheOffset)) {
 			// insert a fresh page and mark it busy -- we're going to read it in
 			page = vm_page_allocate_page(PAGE_STATE_FREE, true);
-			vm_cache_insert_page(cache, page, cacheOffset);
+			cache-&gt;InsertPage(page, cacheOffset);
 
 			ConditionVariable busyCondition;
 			busyCondition.Publish(page, &quot;page&quot;);
 
-			mutex_unlock(&amp;cache-&gt;lock);
+			cache-&gt;Unlock();
 
 			// get a virtual address for the page
 			iovec vec;
@@ -4185,7 +4167,7 @@
 
 			map-&gt;ops-&gt;put_physical_page((addr_t)vec.iov_base);
 
-			mutex_lock(&amp;cache-&gt;lock);
+			cache-&gt;Lock();
 
 			if (status &lt; B_OK) {
 				// on error remove and free the page
@@ -4193,11 +4175,10 @@
 					cache, strerror(status));
 
 				busyCondition.Unpublish();
-				vm_cache_remove_page(cache, page);
+				cache-&gt;RemovePage(page);
 				vm_page_set_state(page, PAGE_STATE_FREE);
 
-				mutex_unlock(&amp;cache-&gt;lock);
-				vm_cache_release_ref(cache);
+				cache-&gt;ReleaseRefAndUnlock();
 				return status;
 			}
 
@@ -4215,76 +4196,41 @@
 
 		vm_cache *nextCache;
 		status_t status = fault_acquire_locked_source(cache, &amp;nextCache);
-		if (status == B_BUSY) {
-			// the source cache is currently in the process of being merged
-			// with his only consumer (cacheRef); since its pages are moved
-			// upwards, too, we try this cache again
-			mutex_unlock(&amp;cache-&gt;lock);
-			thread_yield(true);
-			mutex_lock(&amp;cache-&gt;lock);
-			if (cache-&gt;busy) {
-				// The cache became busy, which means, it is about to be
-				// removed by vm_cache_remove_consumer(). We start again with
-				// the top cache.
-				ConditionVariableEntry entry;
-				entry.Add(cache);
-				mutex_unlock(&amp;cache-&gt;lock);
-				vm_cache_release_ref(cache);
-				entry.Wait();
-				*_restart = true;
-				return B_OK;
-			}
-			lastCache = NULL;
-			continue;
-		} else if (status &lt; B_OK)
+		if (status &lt; B_OK)
 			nextCache = NULL;
 
-		mutex_unlock(&amp;cache-&gt;lock);
-			// at this point, we still hold a ref to this cache (through lastCacheRef)
+		// at this point, we still hold a ref to this cache (through lastCacheRef)
 
 		cache = nextCache;
 	}
 
 	if (page == NULL) {
 		// there was no adequate page, determine the cache for a clean one
-		if (cache == NULL) {
-			// We rolled off the end of the cache chain, so we need to decide which
-			// cache will get the new page we're about to create.
-			cache = isWrite ? topCache : lastCache;
-				// Read-only pages come in the deepest cache - only the
-				// top most cache may have direct write access.
-			vm_cache_acquire_ref(cache);
-			mutex_lock(&amp;cache-&gt;lock);
 
-			if (cache-&gt;busy) {
-				// The cache became busy, which means, it is about to be
-				// removed by vm_cache_remove_consumer(). We start again with
-				// the top cache.
-				ConditionVariableEntry entry;
-				entry.Add(cache);
-				mutex_unlock(&amp;cache-&gt;lock);
-				vm_cache_release_ref(cache);
-				entry.Wait();
-				*_restart = true;
-			} else {
-				vm_page* newPage = vm_cache_lookup_page(cache, cacheOffset);
-				if (newPage &amp;&amp; newPage != &amp;dummyPage) {
-					// A new page turned up. It could be the one we're looking
-					// for, but it could as well be a dummy page from someone
-					// else or an otherwise busy page. We can't really handle
-					// that here. Hence we completely restart this functions.
-					mutex_unlock(&amp;cache-&gt;lock);
-					vm_cache_release_ref(cache);
-					*_restart = true;
-				}
-			}
+		ASSERT(cache == NULL);
+
+		// We rolled off the end of the cache chain, so we need to decide which
+		// cache will get the new page we're about to create.
+		cache = isWrite ? topCache : lastCache;
+			// Read-only pages come in the deepest cache - only the
+			// top most cache may have direct write access.
+		if (cache != lastCache) {
+			lastCache-&gt;ReleaseRefAndUnlock();
+			cache-&gt;Lock();
+			cache-&gt;AcquireRefLocked();
 		}
 
-		// release the reference of the last vm_cache we still have from the loop above
-		if (lastCache != NULL)
-			vm_cache_release_ref(lastCache);
+		vm_page* newPage = cache-&gt;LookupPage(cacheOffset);
+		if (newPage &amp;&amp; newPage != &amp;dummyPage) {
+			// A new page turned up. It could be the one we're looking
+			// for, but it could as well be a dummy page from someone
+			// else or an otherwise busy page. We can't really handle
+			// that here. Hence we completely restart this functions.
+			cache-&gt;ReleaseRefAndUnlock();
+			*_restart = true;
+		}
 	} else {
-		// we still own a reference to the cache
+		// we still own reference and lock to the cache
 	}
 
 	*_pageCache = cache;

[... truncated: 1373 lines follow ...]

</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="010039.html">[Haiku-commits] r26270 -	haiku/branches/developer/bonefish/vm/src/system/kernel/vm
</A></li>
	<LI>Next message: <A HREF="010041.html">[Haiku-commits] r26272 - in haiku/trunk: build/jam	src/add-ons/kernel/bus_managers/firewire
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#10040">[ date ]</a>
              <a href="thread.html#10040">[ thread ]</a>
              <a href="subject.html#10040">[ subject ]</a>
              <a href="author.html#10040">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/haiku-commits">More information about the Haiku-commits
mailing list</a><br>
</body></html>
