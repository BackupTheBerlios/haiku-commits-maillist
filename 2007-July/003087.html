<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Haiku-commits] r21642 - in haiku/trunk: headers/private/kernel	src/system/kernel/cache src/system/kernel/fs src/system/kernel/vm
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/haiku-commits/2007-July/index.html" >
   <LINK REL="made" HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r21642%20-%20in%20haiku/trunk%3A%20headers/private/kernel%0A%09src/system/kernel/cache%20src/system/kernel/fs%20src/system/kernel/vm&In-Reply-To=%3C200707180016.l6I0GZFL006243%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="003086.html">
   <LINK REL="Next"  HREF="003088.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Haiku-commits] r21642 - in haiku/trunk: headers/private/kernel	src/system/kernel/cache src/system/kernel/fs src/system/kernel/vm</H1>
    <B>axeld at BerliOS</B> 
    <A HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r21642%20-%20in%20haiku/trunk%3A%20headers/private/kernel%0A%09src/system/kernel/cache%20src/system/kernel/fs%20src/system/kernel/vm&In-Reply-To=%3C200707180016.l6I0GZFL006243%40sheep.berlios.de%3E"
       TITLE="[Haiku-commits] r21642 - in haiku/trunk: headers/private/kernel	src/system/kernel/cache src/system/kernel/fs src/system/kernel/vm">axeld at mail.berlios.de
       </A><BR>
    <I>Wed Jul 18 02:16:35 CEST 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="003086.html">[Haiku-commits] r21641 - haiku/trunk/src/system/kernel
</A></li>
        <LI>Next message: <A HREF="003088.html">[Haiku-commits] r21643 - in haiku/trunk/src/servers/app: . drawing	drawing/Painter
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#3087">[ date ]</a>
              <a href="thread.html#3087">[ thread ]</a>
              <a href="subject.html#3087">[ subject ]</a>
              <a href="author.html#3087">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: axeld
Date: 2007-07-18 02:16:27 +0200 (Wed, 18 Jul 2007)
New Revision: 21642
ViewCVS: <A HREF="http://svn.berlios.de/viewcvs/haiku?rev=21642&amp;view=rev">http://svn.berlios.de/viewcvs/haiku?rev=21642&amp;view=rev</A>

Added:
   haiku/trunk/src/system/kernel/vm/vm_cache.cpp
Removed:
   haiku/trunk/src/system/kernel/vm/vm_cache.c
Modified:
   haiku/trunk/headers/private/kernel/file_cache.h
   haiku/trunk/headers/private/kernel/vfs.h
   haiku/trunk/headers/private/kernel/vm.h
   haiku/trunk/headers/private/kernel/vm_cache.h
   haiku/trunk/headers/private/kernel/vm_types.h
   haiku/trunk/src/system/kernel/cache/file_cache.cpp
   haiku/trunk/src/system/kernel/fs/vfs.cpp
   haiku/trunk/src/system/kernel/vm/Jamfile
   haiku/trunk/src/system/kernel/vm/vm.cpp
   haiku/trunk/src/system/kernel/vm/vm_page.c
Log:
* Merged vm_cache_ref and vm_cache to a single structure (Axel &amp; Ingo).
* Renamed vm_cache.c to vm_cache.cpp


Modified: haiku/trunk/headers/private/kernel/file_cache.h
===================================================================
--- haiku/trunk/headers/private/kernel/file_cache.h	2007-07-17 23:40:41 UTC (rev 21641)
+++ haiku/trunk/headers/private/kernel/file_cache.h	2007-07-18 00:16:27 UTC (rev 21642)
@@ -37,9 +37,9 @@
 extern &quot;C&quot; {
 #endif
 
-extern void cache_node_opened(void *vnode, int32 fdType, vm_cache_ref *cache,
+extern void cache_node_opened(void *vnode, int32 fdType, vm_cache *cache,
 				dev_t mountID, ino_t parentID, ino_t vnodeID, const char *name);
-extern void cache_node_closed(void *vnode, int32 fdType, vm_cache_ref *cache,
+extern void cache_node_closed(void *vnode, int32 fdType, vm_cache *cache,
 				dev_t mountID, ino_t vnodeID);
 extern void cache_node_launched(size_t argCount, char * const *args);
 extern void cache_prefetch_vnode(void *vnode, off_t offset, size_t size);

Modified: haiku/trunk/headers/private/kernel/vfs.h
===================================================================
--- haiku/trunk/headers/private/kernel/vfs.h	2007-07-17 23:40:41 UTC (rev 21641)
+++ haiku/trunk/headers/private/kernel/vfs.h	2007-07-18 00:16:27 UTC (rev 21642)
@@ -27,7 +27,7 @@
 #define MAX_NODE_MONITORS		65536
 
 struct kernel_args;
-struct vm_cache_ref;
+struct vm_cache;
 struct file_descriptor;
 struct selectsync;
 struct pollfd;
@@ -91,7 +91,7 @@
 			const iovec *vecs, size_t count, size_t *_numBytes, bool fsReenter);
 status_t vfs_write_pages(void *vnode, void *cookie, off_t pos,
 			const iovec *vecs, size_t count, size_t *_numBytes, bool fsReenter);
-status_t vfs_get_vnode_cache(void *vnode, struct vm_cache_ref **_cache, bool allocate);
+status_t vfs_get_vnode_cache(void *vnode, struct vm_cache **_cache, bool allocate);
 status_t vfs_get_file_map( void *_vnode, off_t offset, size_t size,
 			struct file_io_vec *vecs, size_t *_count);
 status_t vfs_get_fs_node_from_path(dev_t mountID, const char *path,

Modified: haiku/trunk/headers/private/kernel/vm.h
===================================================================
--- haiku/trunk/headers/private/kernel/vm.h	2007-07-17 23:40:41 UTC (rev 21641)
+++ haiku/trunk/headers/private/kernel/vm.h	2007-07-18 00:16:27 UTC (rev 21642)
@@ -55,6 +55,8 @@
 area_id vm_map_file(team_id aid, const char *name, void **address, 
 			uint32 addressSpec, addr_t size, uint32 protection, uint32 mapping, 
 			const char *path, off_t offset);
+vm_cache *vm_area_get_locked_cache(vm_area *area);
+void vm_area_put_locked_cache(vm_cache *cache);
 area_id vm_create_null_area(team_id team, const char *name, void **address, 
 			uint32 addressSpec, addr_t size);
 area_id vm_copy_area(team_id team, const char *name, void **_address, 
@@ -63,7 +65,7 @@
 			uint32 addressSpec, uint32 protection, uint32 mapping, 
 			area_id sourceArea);
 status_t vm_delete_area(team_id aid, area_id id);
-status_t vm_create_vnode_cache(void *vnode, vm_cache_ref **_cacheRef);
+status_t vm_create_vnode_cache(void *vnode, vm_cache **_cache);
 vm_area *vm_area_lookup(vm_address_space *addressSpace, addr_t address);
 status_t vm_set_area_memory_type(area_id id, addr_t physicalBase, uint32 type);
 status_t vm_get_page_mapping(team_id team, addr_t vaddr, addr_t *paddr);

Modified: haiku/trunk/headers/private/kernel/vm_cache.h
===================================================================
--- haiku/trunk/headers/private/kernel/vm_cache.h	2007-07-17 23:40:41 UTC (rev 21641)
+++ haiku/trunk/headers/private/kernel/vm_cache.h	2007-07-18 00:16:27 UTC (rev 21642)
@@ -21,19 +21,18 @@
 
 status_t vm_cache_init(struct kernel_args *args);
 vm_cache *vm_cache_create(vm_store *store);
-status_t vm_cache_ref_create(vm_cache *cache, bool acquireLock);
-void vm_cache_acquire_ref(vm_cache_ref *cache_ref);
-void vm_cache_release_ref(vm_cache_ref *cache_ref);
-vm_page *vm_cache_lookup_page(vm_cache_ref *cacheRef, off_t page);
-void vm_cache_insert_page(vm_cache_ref *cacheRef, vm_page *page, off_t offset);
-void vm_cache_remove_page(vm_cache_ref *cacheRef, vm_page *page);
-void vm_cache_remove_consumer(vm_cache_ref *cacheRef, vm_cache *consumer);
-void vm_cache_add_consumer_locked(vm_cache_ref *cacheRef, vm_cache *consumer);
-status_t vm_cache_write_modified(vm_cache_ref *ref, bool fsReenter);
-status_t vm_cache_set_minimal_commitment_locked(vm_cache_ref *ref, off_t commitment);
-status_t vm_cache_resize(vm_cache_ref *cacheRef, off_t newSize);
-status_t vm_cache_insert_area_locked(vm_cache_ref *cacheRef, vm_area *area);
-status_t vm_cache_remove_area(vm_cache_ref *cacheRef, vm_area *area);
+void vm_cache_acquire_ref(vm_cache *cache);
+void vm_cache_release_ref(vm_cache *cache);
+vm_page *vm_cache_lookup_page(vm_cache *cache, off_t page);
+void vm_cache_insert_page(vm_cache *cache, vm_page *page, off_t offset);
+void vm_cache_remove_page(vm_cache *cache, vm_page *page);
+void vm_cache_remove_consumer(vm_cache *cache, vm_cache *consumer);
+void vm_cache_add_consumer_locked(vm_cache *cache, vm_cache *consumer);
+status_t vm_cache_write_modified(vm_cache *cache, bool fsReenter);
+status_t vm_cache_set_minimal_commitment_locked(vm_cache *cache, off_t commitment);
+status_t vm_cache_resize(vm_cache *cache, off_t newSize);
+status_t vm_cache_insert_area_locked(vm_cache *cache, vm_area *area);
+status_t vm_cache_remove_area(vm_cache *cache, vm_area *area);
 
 #ifdef __cplusplus
 }

Modified: haiku/trunk/headers/private/kernel/vm_types.h
===================================================================
--- haiku/trunk/headers/private/kernel/vm_types.h	2007-07-17 23:40:41 UTC (rev 21641)
+++ haiku/trunk/headers/private/kernel/vm_types.h	2007-07-18 00:16:27 UTC (rev 21642)
@@ -125,23 +125,15 @@
 	CACHE_TYPE_NULL
 };
 
-// vm_cache_ref
-typedef struct vm_cache_ref {
-	struct vm_cache		*cache;
+// vm_cache
+typedef struct vm_cache {
 	mutex				lock;
-
 	struct vm_area		*areas;
-
 	vint32				ref_count;
-} vm_cache_ref;
-
-// vm_cache
-typedef struct vm_cache {
 	struct list_link	consumer_link;
 	struct list			consumers;
 		// list of caches that use this cache as a source
 	vm_page				*page_list;
-	vm_cache_ref		*ref;
 	struct vm_cache		*source;
 	struct vm_store		*store;
 	off_t				virtual_base;
@@ -165,7 +157,8 @@
 	uint16				memory_type;
 	vint32				ref_count;
 
-	struct vm_cache_ref	*cache_ref;
+	struct vm_cache		*cache;
+	vint32				no_cache_change;
 	off_t				cache_offset;
 	uint32				cache_type;
 	vm_area_mappings	mappings;

Modified: haiku/trunk/src/system/kernel/cache/file_cache.cpp
===================================================================
--- haiku/trunk/src/system/kernel/cache/file_cache.cpp	2007-07-17 23:40:41 UTC (rev 21641)
+++ haiku/trunk/src/system/kernel/cache/file_cache.cpp	2007-07-18 00:16:27 UTC (rev 21642)
@@ -60,7 +60,7 @@
 };
 
 struct file_cache_ref {
-	vm_cache_ref	*cache;
+	vm_cache		*cache;
 	void			*vnode;
 	void			*device;
 	void			*cookie;
@@ -520,7 +520,7 @@
 	TRACE((&quot;read_chunk(offset = %Ld, size = %lu, pageOffset = %ld, buffer = %#lx, bufferSize = %lu\n&quot;,
 		offset, size, pageOffset, buffer, bufferSize));
 
-	vm_cache_ref *cache = ref-&gt;cache;
+	vm_cache *cache = ref-&gt;cache;
 
 	iovec vecs[MAX_IO_VECS];
 	int32 vecCount = 0;
@@ -713,9 +713,10 @@
 		addr_t last = (addr_t)vecs[vecCount - 1].iov_base
 			+ vecs[vecCount - 1].iov_len - B_PAGE_SIZE;
 
-		if (offset + pageOffset + bufferSize == ref-&gt;cache-&gt;cache-&gt;virtual_size) {
+		if (offset + pageOffset + bufferSize == ref-&gt;cache-&gt;virtual_size) {
 			// the space in the page after this write action needs to be cleaned
-			memset((void *)(last + lastPageOffset), 0, B_PAGE_SIZE - lastPageOffset);
+			memset((void *)(last + lastPageOffset), 0,
+				B_PAGE_SIZE - lastPageOffset);
 		} else if (vecCount &gt; 1) {
 			// the end of this write does not happen on a page boundary, so we
 			// need to fetch the last page before we can update it
@@ -842,8 +843,8 @@
 		panic(&quot;cache_io() called with NULL ref!\n&quot;);
 
 	file_cache_ref *ref = (file_cache_ref *)_cacheRef;
-	vm_cache_ref *cache = ref-&gt;cache;
-	off_t fileSize = cache-&gt;cache-&gt;virtual_size;
+	vm_cache *cache = ref-&gt;cache;
+	off_t fileSize = cache-&gt;virtual_size;
 
 	TRACE((&quot;cache_io(ref = %p, offset = %Ld, buffer = %p, size = %lu, %s)\n&quot;,
 		ref, offset, (void *)buffer, *_size, doWrite ? &quot;write&quot; : &quot;read&quot;));
@@ -1084,12 +1085,13 @@
 extern &quot;C&quot; void
 cache_prefetch_vnode(void *vnode, off_t offset, size_t size)
 {
-	vm_cache_ref *cache;
+	vm_cache *cache;
 	if (vfs_get_vnode_cache(vnode, &amp;cache, false) != B_OK)
 		return;
 
-	file_cache_ref *ref = (struct file_cache_ref *)((vnode_store *)cache-&gt;cache-&gt;store)-&gt;file_cache_ref;
-	off_t fileSize = cache-&gt;cache-&gt;virtual_size;
+	file_cache_ref *ref = (struct file_cache_ref *)
+		((vnode_store *)cache-&gt;store)-&gt;file_cache_ref;
+	off_t fileSize = cache-&gt;virtual_size;
 
 	if (size &gt; fileSize)
 		size = fileSize;
@@ -1160,7 +1162,7 @@
 
 
 extern &quot;C&quot; void
-cache_node_opened(void *vnode, int32 fdType, vm_cache_ref *cache, dev_t mountID,
+cache_node_opened(void *vnode, int32 fdType, vm_cache *cache, dev_t mountID,
 	ino_t parentID, ino_t vnodeID, const char *name)
 {
 	if (sCacheModule == NULL || sCacheModule-&gt;node_opened == NULL)
@@ -1168,9 +1170,10 @@
 
 	off_t size = -1;
 	if (cache != NULL) {
-		file_cache_ref *ref = (file_cache_ref *)((vnode_store *)cache-&gt;cache-&gt;store)-&gt;file_cache_ref;
+		file_cache_ref *ref = (file_cache_ref *)
+			((vnode_store *)cache-&gt;store)-&gt;file_cache_ref;
 		if (ref != NULL)
-			size = ref-&gt;cache-&gt;cache-&gt;virtual_size;
+			size = cache-&gt;virtual_size;
 	}
 
 	sCacheModule-&gt;node_opened(vnode, fdType, mountID, parentID, vnodeID, name, size);
@@ -1178,7 +1181,7 @@
 
 
 extern &quot;C&quot; void
-cache_node_closed(void *vnode, int32 fdType, vm_cache_ref *cache,
+cache_node_closed(void *vnode, int32 fdType, vm_cache *cache,
 	dev_t mountID, ino_t vnodeID)
 {
 	if (sCacheModule == NULL || sCacheModule-&gt;node_closed == NULL)
@@ -1237,7 +1240,7 @@
 	if (ref == NULL)
 		return NULL;
 
-	// TODO: delay vm_cache/vm_cache_ref creation until data is
+	// TODO: delay vm_cache creation until data is
 	//	requested/written for the first time? Listing lots of
 	//	files in Tracker (and elsewhere) could be slowed down.
 	//	Since the file_cache_ref itself doesn't have a lock,
@@ -1255,7 +1258,8 @@
 	if (vfs_get_cookie_from_fd(fd, &amp;ref-&gt;cookie) != B_OK)
 		goto err2;
 
-	// Get the vnode for the object (note, this does not grab a reference to the node)
+	// Get the vnode for the object
+	// (note, this does not grab a reference to the node)
 	if (vfs_lookup_vnode(mountID, vnodeID, &amp;ref-&gt;vnode) != B_OK)
 		goto err2;
 
@@ -1270,8 +1274,8 @@
 	// we don't grab an extra reference).
 	vfs_put_vnode(ref-&gt;vnode);
 
-	ref-&gt;cache-&gt;cache-&gt;virtual_size = size;
-	((vnode_store *)ref-&gt;cache-&gt;cache-&gt;store)-&gt;file_cache_ref = ref;
+	ref-&gt;cache-&gt;virtual_size = size;
+	((vnode_store *)ref-&gt;cache-&gt;store)-&gt;file_cache_ref = ref;
 	return ref;
 
 err2:

Modified: haiku/trunk/src/system/kernel/fs/vfs.cpp
===================================================================
--- haiku/trunk/src/system/kernel/fs/vfs.cpp	2007-07-17 23:40:41 UTC (rev 21641)
+++ haiku/trunk/src/system/kernel/fs/vfs.cpp	2007-07-18 00:16:27 UTC (rev 21642)
@@ -65,7 +65,7 @@
 
 struct vnode {
 	struct vnode	*next;
-	vm_cache_ref	*cache;
+	vm_cache		*cache;
 	dev_t			device;
 	list_link		mount_link;
 	list_link		unused_link;
@@ -2415,13 +2415,14 @@
 
 		// count pages in cache
 		size_t numPages = 0;
-		for (struct vm_page *page = vnode-&gt;cache-&gt;cache-&gt;page_list;
+		for (struct vm_page *page = vnode-&gt;cache-&gt;page_list;
 				page != NULL; page = page-&gt;cache_next) {
 			numPages++;
 		}
 
-		kprintf(&quot;%p%4ld%10Ld %p %8Ld%8ld\n&quot;, vnode, vnode-&gt;device, vnode-&gt;id, vnode-&gt;cache,
-			(vnode-&gt;cache-&gt;cache-&gt;virtual_size + B_PAGE_SIZE - 1) / B_PAGE_SIZE, numPages);
+		kprintf(&quot;%p%4ld%10Ld %p %8Ld%8ld\n&quot;, vnode, vnode-&gt;device, vnode-&gt;id,
+			vnode-&gt;cache, (vnode-&gt;cache-&gt;virtual_size + B_PAGE_SIZE - 1)
+				/ B_PAGE_SIZE, numPages);
 	}
 
 	hash_close(sVnodeTable, &amp;iterator, false);
@@ -3106,7 +3107,7 @@
  */
 
 extern &quot;C&quot; status_t
-vfs_get_vnode_cache(void *_vnode, vm_cache_ref **_cache, bool allocate)
+vfs_get_vnode_cache(void *_vnode, vm_cache **_cache, bool allocate)
 {
 	struct vnode *vnode = (struct vnode *)_vnode;
 

Modified: haiku/trunk/src/system/kernel/vm/Jamfile
===================================================================
--- haiku/trunk/src/system/kernel/vm/Jamfile	2007-07-17 23:40:41 UTC (rev 21641)
+++ haiku/trunk/src/system/kernel/vm/Jamfile	2007-07-18 00:16:27 UTC (rev 21642)
@@ -3,7 +3,7 @@
 KernelMergeObject kernel_vm.o :
 	vm.cpp
 	vm_address_space.c
-	vm_cache.c
+	vm_cache.cpp
 	vm_daemons.c
 	vm_low_memory.cpp
 	vm_page.c

Modified: haiku/trunk/src/system/kernel/vm/vm.cpp
===================================================================
--- haiku/trunk/src/system/kernel/vm/vm.cpp	2007-07-17 23:40:41 UTC (rev 21641)
+++ haiku/trunk/src/system/kernel/vm/vm.cpp	2007-07-18 00:16:27 UTC (rev 21642)
@@ -29,6 +29,7 @@
 #include &lt;lock.h&gt;
 #include &lt;thread.h&gt;
 #include &lt;team.h&gt;
+#include &lt;util/AutoLock.h&gt;
 
 #include &lt;boot/stage2.h&gt;
 #include &lt;boot/elf.h&gt;
@@ -63,6 +64,7 @@
 static hash_table *sAreaHash;
 static sem_id sAreaHashLock;
 static spinlock sMappingLock;
+static mutex sAreaCacheLock;
 
 static off_t sAvailableMemory;
 static benaphore sAvailableMemoryLock;
@@ -158,7 +160,8 @@
 	area-&gt;memory_type = 0;
 	area-&gt;ref_count = 1;
 
-	area-&gt;cache_ref = NULL;
+	area-&gt;cache = NULL;
+	area-&gt;no_cache_change = 0;
 	area-&gt;cache_offset = 0;
 
 	area-&gt;address_space = addressSpace;
@@ -493,30 +496,27 @@
 }
 
 
+//! You need to hold the lock of the cache when calling this function.
 static status_t
-map_backing_store(vm_address_space *addressSpace, vm_cache_ref *cacheRef,
+map_backing_store(vm_address_space *addressSpace, vm_cache *cache,
 	void **_virtualAddress, off_t offset, addr_t size, uint32 addressSpec,
 	int wiring, int protection, int mapping, vm_area **_area, const char *areaName)
 {
-	TRACE((&quot;map_backing_store: aspace %p, cacheref %p, *vaddr %p, offset 0x%Lx, size %lu, addressSpec %ld, wiring %d, protection %d, _area %p, area_name '%s'\n&quot;,
-		addressSpace, cacheRef, *_virtualAddress, offset, size, addressSpec,
+	TRACE((&quot;map_backing_store: aspace %p, cache %p, *vaddr %p, offset 0x%Lx, size %lu, addressSpec %ld, wiring %d, protection %d, _area %p, area_name '%s'\n&quot;,
+		addressSpace, cache, *_virtualAddress, offset, size, addressSpec,
 		wiring, protection, _area, areaName));
+	ASSERT_LOCKED_MUTEX(&amp;cache-&gt;lock);
 
 	vm_area *area = create_area_struct(addressSpace, areaName, wiring, protection);
 	if (area == NULL)
 		return B_NO_MEMORY;
 
-	mutex_lock(&amp;cacheRef-&gt;lock);
-
-	vm_cache *cache = cacheRef-&gt;cache;
 	vm_store *store = cache-&gt;store;
-	bool unlock = true;
 	status_t status;
 
 	// if this is a private map, we need to create a new cache &amp; store object
 	// pair to handle the private copies of pages as they are written to
 	if (mapping == REGION_PRIVATE_MAP) {
-		vm_cache_ref *newCacheRef;
 		vm_cache *newCache;
 		vm_store *newStore;
 
@@ -533,31 +533,21 @@
 			newStore-&gt;ops-&gt;destroy(newStore);
 			goto err1;
 		}
-		status = vm_cache_ref_create(newCache, false);
-		if (status &lt; B_OK) {
-			newStore-&gt;ops-&gt;destroy(newStore);
-			free(newCache);
-			goto err1;
-		}
 
-		newCacheRef = newCache-&gt;ref;
+		mutex_lock(&amp;newCache-&gt;lock);
 		newCache-&gt;type = CACHE_TYPE_RAM;
 		newCache-&gt;temporary = 1;
 		newCache-&gt;scan_skip = cache-&gt;scan_skip;
 		newCache-&gt;virtual_base = offset;
 		newCache-&gt;virtual_size = offset + size;
 
-		vm_cache_add_consumer_locked(cacheRef, newCache);
+		vm_cache_add_consumer_locked(cache, newCache);
 
-		mutex_unlock(&amp;cacheRef-&gt;lock);
-		mutex_lock(&amp;newCacheRef-&gt;lock);
-
 		cache = newCache;
-		cacheRef = newCacheRef;
 		store = newStore;
 	}
 
-	status = vm_cache_set_minimal_commitment_locked(cacheRef, offset + size);
+	status = vm_cache_set_minimal_commitment_locked(cache, offset + size);
 	if (status != B_OK)
 		goto err2;
 
@@ -576,12 +566,13 @@
 		goto err3;
 
 	// attach the cache to the area
-	area-&gt;cache_ref = cacheRef;
+	area-&gt;cache = cache;
 	area-&gt;cache_offset = offset;
 
 	// point the cache back to the area
-	vm_cache_insert_area_locked(cacheRef, area);
-	mutex_unlock(&amp;cacheRef-&gt;lock);
+	vm_cache_insert_area_locked(cache, area);
+	if (mapping == REGION_PRIVATE_MAP)
+		mutex_unlock(&amp;cache-&gt;lock);
 
 	// insert the area in the global area hash table
 	acquire_sem_etc(sAreaHashLock, WRITE_COUNT, 0 ,0);
@@ -601,13 +592,10 @@
 err2:
 	if (mapping == REGION_PRIVATE_MAP) {
 		// we created this cache, so we must delete it again
-		mutex_unlock(&amp;cacheRef-&gt;lock);
-		vm_cache_release_ref(cacheRef);
-		unlock = false;
+		mutex_unlock(&amp;cache-&gt;lock);
+		vm_cache_release_ref(cache);
 	}
 err1:
-	if (unlock)
-		mutex_unlock(&amp;cacheRef-&gt;lock);
 	free(area-&gt;name);
 	free(area);
 	return status;
@@ -722,7 +710,6 @@
 vm_create_anonymous_area(team_id aid, const char *name, void **address, 
 	uint32 addressSpec, addr_t size, uint32 wiring, uint32 protection)
 {
-	vm_cache_ref *cacheRef;
 	vm_area *area;
 	vm_cache *cache;
 	vm_store *store;
@@ -804,9 +791,6 @@
 		status = B_NO_MEMORY;
 		goto err2;
 	}
-	status = vm_cache_ref_create(cache, false);
-	if (status &lt; B_OK)
-		goto err3;
 
 	cache-&gt;temporary = 1;
 	cache-&gt;type = CACHE_TYPE_RAM;
@@ -824,12 +808,15 @@
 			break;
 	}
 
-	cacheRef = cache-&gt;ref;
+	mutex_lock(&amp;cache-&gt;lock);
 
-	status = map_backing_store(addressSpace, cacheRef, address, 0, size,
+	status = map_backing_store(addressSpace, cache, address, 0, size,
 		addressSpec, wiring, protection, REGION_NO_PRIVATE_MAP, &amp;area, name);
+
+	mutex_unlock(&amp;cache-&gt;lock);
+
 	if (status &lt; B_OK) {
-		vm_cache_release_ref(cacheRef);
+		vm_cache_release_ref(cache);
 		goto err1;
 	}
 
@@ -842,7 +829,7 @@
 		case B_FULL_LOCK:
 		{
 			// Allocate and map all pages for this area
-			mutex_lock(&amp;cacheRef-&gt;lock);
+			mutex_lock(&amp;cache-&gt;lock);
 
 			off_t offset = 0;
 			for (addr_t address = area-&gt;base; address &lt; area-&gt;base + (area-&gt;size - 1);
@@ -863,11 +850,11 @@
 					panic(&quot;couldn't fulfill B_FULL lock!&quot;);
 				}
 
-				vm_cache_insert_page(cacheRef, page, offset);
+				vm_cache_insert_page(cache, page, offset);
 				vm_map_page(area, page, address, protection);
 			}
 
-			mutex_unlock(&amp;cacheRef-&gt;lock);
+			mutex_unlock(&amp;cache-&gt;lock);
 			break;
 		}
 
@@ -882,7 +869,7 @@
 			if (!kernel_startup)
 				panic(&quot;ALREADY_WIRED flag used outside kernel startup\n&quot;);
 
-			mutex_lock(&amp;cacheRef-&gt;lock);
+			mutex_lock(&amp;cache-&gt;lock);
 			map-&gt;ops-&gt;lock(map);
 
 			for (addr_t virtualAddress = area-&gt;base; virtualAddress &lt; area-&gt;base
@@ -905,11 +892,11 @@
 				page-&gt;wired_count++;
 					// TODO: needs to be atomic on all platforms!
 				vm_page_set_state(page, PAGE_STATE_WIRED);
-				vm_cache_insert_page(cacheRef, page, offset);
+				vm_cache_insert_page(cache, page, offset);
 			}
 
 			map-&gt;ops-&gt;unlock(map);
-			mutex_unlock(&amp;cacheRef-&gt;lock);
+			mutex_unlock(&amp;cache-&gt;lock);
 			break;
 		}
 
@@ -922,7 +909,7 @@
 			addr_t virtualAddress;
 			off_t offset = 0;
 
-			mutex_lock(&amp;cacheRef-&gt;lock);
+			mutex_lock(&amp;cache-&gt;lock);
 			map-&gt;ops-&gt;lock(map);
 
 			for (virtualAddress = area-&gt;base; virtualAddress &lt; area-&gt;base
@@ -940,11 +927,11 @@
 				page-&gt;wired_count++;
 					// TODO: needs to be atomic on all platforms!
 				vm_page_set_state(page, PAGE_STATE_WIRED);
-				vm_cache_insert_page(cacheRef, page, offset);
+				vm_cache_insert_page(cache, page, offset);
 			}
 
 			map-&gt;ops-&gt;unlock(map);
-			mutex_unlock(&amp;cacheRef-&gt;lock);
+			mutex_unlock(&amp;cache-&gt;lock);
 			break;
 		}
 
@@ -958,8 +945,6 @@
 	area-&gt;cache_type = CACHE_TYPE_RAM;
 	return area-&gt;id;
 
-err3:
-	free(cache);
 err2:
 	store-&gt;ops-&gt;destroy(store);
 err1:
@@ -985,7 +970,6 @@
 vm_map_physical_memory(team_id aspaceID, const char *name, void **_address,
 	uint32 addressSpec, addr_t size, uint32 protection, addr_t physicalAddress)
 {
-	vm_cache_ref *cacheRef;
 	vm_area *area;
 	vm_cache *cache;
 	vm_store *store;
@@ -1024,22 +1008,22 @@
 		status = B_NO_MEMORY;
 		goto err2;
 	}
-	status = vm_cache_ref_create(cache, false);
-	if (status &lt; B_OK)
-		goto err3;
 
 	// tell the page scanner to skip over this area, it's pages are special
 	cache-&gt;scan_skip = 1;
 	cache-&gt;type = CACHE_TYPE_DEVICE;
 	cache-&gt;virtual_size = size;
 
-	cacheRef = cache-&gt;ref;
+	mutex_lock(&amp;cache-&gt;lock);
 
-	status = map_backing_store(addressSpace, cacheRef, _address, 0, size,
+	status = map_backing_store(addressSpace, cache, _address, 0, size,
 		addressSpec &amp; ~B_MTR_MASK, B_FULL_LOCK, protection,
 		REGION_NO_PRIVATE_MAP, &amp;area, name);
+
+	mutex_unlock(&amp;cache-&gt;lock);
+
 	if (status &lt; B_OK)
-		vm_cache_release_ref(cacheRef);
+		vm_cache_release_ref(cache);
 
 	if (status &gt;= B_OK &amp;&amp; (addressSpec &amp; B_MTR_MASK) != 0) {
 		// set requested memory type
@@ -1074,8 +1058,6 @@
 	area-&gt;cache_type = CACHE_TYPE_DEVICE;
 	return area-&gt;id;
 
-err3:
-	free(cache);
 err2:
 	store-&gt;ops-&gt;destroy(store);
 err1:
@@ -1090,7 +1072,6 @@
 {
 	vm_area *area;
 	vm_cache *cache;
-	vm_cache_ref *cacheRef;
 	vm_store *store;
 	status_t status;
 
@@ -1112,32 +1093,28 @@
 		status = B_NO_MEMORY;
 		goto err2;
 	}
-	status = vm_cache_ref_create(cache, false);
-	if (status &lt; B_OK)
-		goto err3;
 
 	// tell the page scanner to skip over this area, no pages will be mapped here
 	cache-&gt;scan_skip = 1;
 	cache-&gt;type = CACHE_TYPE_NULL;
 	cache-&gt;virtual_size = size;
 
-	cacheRef = cache-&gt;ref;
+	mutex_lock(&amp;cache-&gt;lock);
 
-	status = map_backing_store(addressSpace, cacheRef, address, 0, size, addressSpec, 0,
+	status = map_backing_store(addressSpace, cache, address, 0, size, addressSpec, 0,
 		B_KERNEL_READ_AREA, REGION_NO_PRIVATE_MAP, &amp;area, name);
 
+	mutex_unlock(&amp;cache-&gt;lock);
 	vm_put_address_space(addressSpace);
 
 	if (status &lt; B_OK) {
-		vm_cache_release_ref(cacheRef);
+		vm_cache_release_ref(cache);
 		return status;
 	}
 
 	area-&gt;cache_type = CACHE_TYPE_NULL;
 	return area-&gt;id;
 
-err3:
-	free(cache);
 err2:
 	store-&gt;ops-&gt;destroy(store);
 err1:
@@ -1154,7 +1131,7 @@
  */
 
 status_t
-vm_create_vnode_cache(void *vnode, struct vm_cache_ref **_cacheRef)
+vm_create_vnode_cache(void *vnode, struct vm_cache **_cache)
 {
 	status_t status;
 
@@ -1168,18 +1145,13 @@
 		status = B_NO_MEMORY;
 		goto err1;
 	}
-	status = vm_cache_ref_create(cache, false);
-	if (status &lt; B_OK)
-		goto err2;
 
 	cache-&gt;type = CACHE_TYPE_VNODE;
 
-	*_cacheRef = cache-&gt;ref;
+	*_cache = cache;
 	vfs_acquire_vnode(vnode);
 	return B_OK;
 
-err2:
-	free(cache);
 err1:
 	store-&gt;ops-&gt;destroy(store);
 	return status;
@@ -1196,7 +1168,7 @@
 	size_t size, uint32 protection, uint32 mapping, const char *path,
 	off_t offset, bool kernel)
 {
-	vm_cache_ref *cacheRef;
+	vm_cache *cache;
 	vm_area *area;
 	void *vnode;
 	status_t status;
@@ -1224,7 +1196,7 @@
 		goto err1;
 
 	// ToDo: this only works for file systems that use the file cache
-	status = vfs_get_vnode_cache(vnode, &amp;cacheRef, false);
+	status = vfs_get_vnode_cache(vnode, &amp;cache, false);
 
 	vfs_put_vnode(vnode);
 		// we don't need this vnode anymore - if the above call was
@@ -1233,11 +1205,16 @@
 	if (status &lt; B_OK)
 		goto err1;
 
-	status = map_backing_store(addressSpace, cacheRef, _address,
+	mutex_lock(&amp;cache-&gt;lock);
+
+	status = map_backing_store(addressSpace, cache, _address,
 		offset, size, addressSpec, 0, protection, mapping, &amp;area, name);
+
+	mutex_unlock(&amp;cache-&gt;lock);
+
 	if (status &lt; B_OK || mapping == REGION_PRIVATE_MAP) {
 		// map_backing_store() cannot know we no longer need the ref
-		vm_cache_release_ref(cacheRef);
+		vm_cache_release_ref(cache);
 	}
 	if (status &lt; B_OK)
 		goto err1;
@@ -1297,6 +1274,36 @@
 }
 
 
+vm_cache *
+vm_area_get_locked_cache(vm_area *area)
+{
+	MutexLocker locker(sAreaCacheLock);
+	while (true) {
+		vm_cache* cache = area-&gt;cache;
+		vm_cache_acquire_ref(cache);
+		locker.Unlock();
+
+		mutex_lock(&amp;cache-&gt;lock);
+
+		locker.Lock();
+		if (cache == area-&gt;cache)
+			return cache;
+
+		// the cache changed in the meantime
+		mutex_unlock(&amp;cache-&gt;lock);
+		vm_cache_release_ref(cache);
+	}
+}
+
+
+void
+vm_area_put_locked_cache(vm_cache *cache)
+{
+	mutex_unlock(&amp;cache-&gt;lock);
+	vm_cache_release_ref(cache);
+}
+
+
 area_id
 vm_clone_area(team_id team, const char *name, void **address, uint32 addressSpec,
 	uint32 protection, uint32 mapping, area_id sourceID)
@@ -1315,7 +1322,7 @@
 		return B_BAD_VALUE;
 	}
 
-	vm_cache_acquire_ref(sourceArea-&gt;cache_ref);
+	vm_cache *cache = vm_area_get_locked_cache(sourceArea);
 
 	// ToDo: for now, B_USER_CLONEABLE is disabled, until all drivers
 	//	have been adapted. Maybe it should be part of the kernel settings,
@@ -1331,8 +1338,8 @@
 	if (sourceArea-&gt;cache_type == CACHE_TYPE_NULL)
 		status = B_NOT_ALLOWED;
 	else {
-		status = map_backing_store(addressSpace, sourceArea-&gt;cache_ref,
-			address, sourceArea-&gt;cache_offset, sourceArea-&gt;size, addressSpec,
+		status = map_backing_store(addressSpace, cache, address,
+			sourceArea-&gt;cache_offset, sourceArea-&gt;size, addressSpec,
 			sourceArea-&gt;wiring, protection, mapping, &amp;newArea, name);
 	}
 	if (status == B_OK &amp;&amp; mapping != REGION_PRIVATE_MAP) {
@@ -1340,7 +1347,7 @@
 		// to create a new ref, and has therefore already acquired a reference
 		// to the source cache - but otherwise it has no idea that we need
 		// one.
-		vm_cache_acquire_ref(sourceArea-&gt;cache_ref);
+		vm_cache_acquire_ref(cache);
 	}
 	if (status == B_OK &amp;&amp; newArea-&gt;wiring == B_FULL_LOCK) {
 		// we need to map in everything at this point
@@ -1368,23 +1375,18 @@
 			map-&gt;ops-&gt;unlock(map);
 		} else {
 			// map in all pages from source
-			mutex_lock(&amp;sourceArea-&gt;cache_ref-&gt;lock);
-
-			for (vm_page *page = sourceArea-&gt;cache_ref-&gt;cache-&gt;page_list;
-					page != NULL; page = page-&gt;cache_next) {
+			for (vm_page *page = cache-&gt;page_list; page != NULL;
+					page = page-&gt;cache_next) {
 				vm_map_page(newArea, page, newArea-&gt;base
 					+ ((page-&gt;cache_offset &lt;&lt; PAGE_SHIFT) - newArea-&gt;cache_offset),
 					protection);
 			}
-
-			mutex_unlock(&amp;sourceArea-&gt;cache_ref-&gt;lock);
 		}
 	}
 	if (status == B_OK)
 		newArea-&gt;cache_type = sourceArea-&gt;cache_type;
 
-	vm_cache_release_ref(sourceArea-&gt;cache_ref);
-
+	vm_area_put_locked_cache(cache);
 	vm_put_area(sourceArea);
 	vm_put_address_space(addressSpace);
 
@@ -1509,13 +1511,13 @@
 	vm_unmap_pages(area, area-&gt;base, area-&gt;size);
 
 	// ToDo: do that only for vnode stores
-	vm_cache_write_modified(area-&gt;cache_ref, false);
+	vm_cache_write_modified(area-&gt;cache, false);
 
 	arch_vm_unset_memory_type(area);
 	remove_area_from_address_space(addressSpace, area, aspaceLocked);
 
-	vm_cache_remove_area(area-&gt;cache_ref, area);
-	vm_cache_release_ref(area-&gt;cache_ref);
+	vm_cache_remove_area(area-&gt;cache, area);
+	vm_cache_release_ref(area-&gt;cache);
 
 	// now we can give up the area's reference to the address space
 	vm_put_address_space(addressSpace);
@@ -1538,25 +1540,30 @@
 {
 	vm_store *store;
 	vm_cache *upperCache, *lowerCache;
-	vm_cache_ref *upperCacheRef, *lowerCacheRef;
-	vm_translation_map *map;
 	vm_page *page;
-	uint32 protection;
 	status_t status;
 
 	TRACE((&quot;vm_copy_on_write_area(area = %p)\n&quot;, area));
 
-	// We need to separate the vm_cache from its vm_cache_ref: the area
-	// and its cache_ref goes into a new layer on top of the old one.
-	// So the old cache gets a new cache_ref and the area a new cache.
+	// We need to separate the cache from its areas. The cache goes one level
+	// deeper and we create a new cache inbetween.
 
-	upperCacheRef = area-&gt;cache_ref;
+	bool noCacheChange;
+	do {
+		lowerCache = vm_area_get_locked_cache(area);
+		noCacheChange = false;
 
-	// we will exchange the cache_ref's cache, so we better hold its lock
-	mutex_lock(&amp;upperCacheRef-&gt;lock);
+		for (vm_area *tempArea = lowerCache-&gt;areas; tempArea != NULL;
+				tempArea = tempArea-&gt;cache_next) {
+			if (tempArea-&gt;no_cache_change) {
+				noCacheChange = true;
+				vm_area_put_locked_cache(lowerCache);
+				thread_yield();
+				break;
+			}
+		}
+	} while (noCacheChange);
 
-	lowerCache = upperCacheRef-&gt;cache;
-
 	// create an anonymous store object
 	store = vm_store_create_anonymous_noswap(false, 0, 0);
 	if (store == NULL) {
@@ -1570,75 +1577,57 @@
 		goto err2;
 	}
 
-	// we need to hold the cache_ref lock when we want to switch its cache
-	status = vm_cache_ref_create(lowerCache, true);
-	if (status &lt; B_OK)
-		goto err3;
+	mutex_lock(&amp;upperCache-&gt;lock);
 
-	lowerCacheRef = lowerCache-&gt;ref;
-
-	// The area must be readable in the same way it was previously writable
-	protection = B_KERNEL_READ_AREA;
-	if (area-&gt;protection &amp; B_READ_AREA)
-		protection |= B_READ_AREA;
-
 	upperCache-&gt;type = CACHE_TYPE_RAM;
 	upperCache-&gt;temporary = 1;
 	upperCache-&gt;scan_skip = lowerCache-&gt;scan_skip;
 	upperCache-&gt;virtual_base = lowerCache-&gt;virtual_base;
 	upperCache-&gt;virtual_size = lowerCache-&gt;virtual_size;
 
-	upperCache-&gt;ref = upperCacheRef;
-	upperCacheRef-&gt;cache = upperCache;
+	// transfer the lower cache areas to the upper cache
+	mutex_lock(&amp;sAreaCacheLock);
 
-	// we need to manually alter the ref_count (divide it between the two)
-	// the lower cache_ref has only known refs, so compute them
-	{
-		int32 count = 0;
-		vm_cache *consumer = NULL;
-		while ((consumer = (vm_cache *)list_get_next_item(
-				&amp;lowerCache-&gt;consumers, consumer)) != NULL) {
-			count++;
-		}
+	upperCache-&gt;areas = lowerCache-&gt;areas;
+	lowerCache-&gt;areas = NULL;
 
-		atomic_add(&amp;lowerCacheRef-&gt;ref_count, count);
-		atomic_add(&amp;upperCacheRef-&gt;ref_count, -count);
+	for (vm_area *tempArea = upperCache-&gt;areas; tempArea != NULL;
+			tempArea = tempArea-&gt;cache_next) {
+		tempArea-&gt;cache = upperCache;
+		atomic_add(&amp;upperCache-&gt;ref_count, 1);
+		atomic_add(&amp;lowerCache-&gt;ref_count, -1);
 	}
 
-	vm_cache_add_consumer_locked(lowerCacheRef, upperCache);
+	mutex_unlock(&amp;sAreaCacheLock);
 
-	// We now need to remap all pages from the area read-only, so that
+	vm_cache_add_consumer_locked(lowerCache, upperCache);
+
+	// We now need to remap all pages from all of the cache's areas read-only, so that
 	// a copy will be created on next write access
 
-	map = &amp;area-&gt;address_space-&gt;translation_map;
-	map-&gt;ops-&gt;lock(map);
-	map-&gt;ops-&gt;unmap(map, area-&gt;base, area-&gt;base - 1 + area-&gt;size);
-	map-&gt;ops-&gt;flush(map);
+	for (vm_area *tempArea = upperCache-&gt;areas; tempArea != NULL;
+			tempArea = tempArea-&gt;cache_next) {
+// TODO: Don't we have to lock the area's address space for accessing base, size, and protection?
+		// The area must be readable in the same way it was previously writable
+		uint32 protection = B_KERNEL_READ_AREA;
+		if (tempArea-&gt;protection &amp; B_READ_AREA)
+			protection |= B_READ_AREA;
 
-	// TODO: does anything guarantee that we remap the same pages here?
-	//	Shouldn't we better introduce a &quot;change mapping&quot;?
-
-	for (page = lowerCache-&gt;page_list; page; page = page-&gt;cache_next) {
-		map-&gt;ops-&gt;map(map, area-&gt;base + (page-&gt;cache_offset &lt;&lt; PAGE_SHIFT)
-			- area-&gt;cache_offset, page-&gt;physical_page_number &lt;&lt; PAGE_SHIFT,
-			protection);
+		vm_translation_map *map = &amp;tempArea-&gt;address_space-&gt;translation_map;
+		map-&gt;ops-&gt;lock(map);
+		map-&gt;ops-&gt;protect(map, tempArea-&gt;base, tempArea-&gt;base - 1 + tempArea-&gt;size, protection);
+		map-&gt;ops-&gt;unlock(map);
 	}
 
-	map-&gt;ops-&gt;unlock(map);
+	vm_area_put_locked_cache(lowerCache);
+	vm_area_put_locked_cache(upperCache);
 
-	mutex_unlock(&amp;lowerCacheRef-&gt;lock);
-	mutex_unlock(&amp;upperCacheRef-&gt;lock);
-
-	vm_cache_release_ref(lowerCacheRef);
-
 	return B_OK;
 
-err3:
-	free(upperCache);
 err2:
 	store-&gt;ops-&gt;destroy(store);
 err1:
-	mutex_unlock(&amp;upperCacheRef-&gt;lock);
+	vm_area_put_locked_cache(lowerCache);
 	return status;
 }
 
@@ -1648,7 +1637,7 @@
 	uint32 protection, area_id sourceID)
 {
 	vm_address_space *addressSpace;
-	vm_cache_ref *cacheRef;
+	vm_cache *cache;
 	vm_area *target, *source;
 	status_t status;
 	bool writableCopy = (protection &amp; (B_KERNEL_WRITE_AREA | B_WRITE_AREA)) != 0;
@@ -1664,7 +1653,7 @@
 		return B_BAD_VALUE;
 
 	addressSpace = vm_get_address_space_by_id(addressSpaceID);
-	cacheRef = source-&gt;cache_ref;
+	cache = vm_area_get_locked_cache(source);
 
 	if (addressSpec == B_CLONE_ADDRESS) {

[... truncated: 1486 lines follow ...]

</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="003086.html">[Haiku-commits] r21641 - haiku/trunk/src/system/kernel
</A></li>
	<LI>Next message: <A HREF="003088.html">[Haiku-commits] r21643 - in haiku/trunk/src/servers/app: . drawing	drawing/Painter
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#3087">[ date ]</a>
              <a href="thread.html#3087">[ thread ]</a>
              <a href="subject.html#3087">[ subject ]</a>
              <a href="author.html#3087">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/haiku-commits">More information about the Haiku-commits
mailing list</a><br>
</body></html>
