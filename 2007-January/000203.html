<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Haiku-commits] r19796 - in haiku/trunk: headers/private/kernel	src/system/kernel/vm
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/haiku-commits/2007-January/index.html" >
   <LINK REL="made" HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r19796%20-%20in%20haiku/trunk%3A%20headers/private/kernel%0A%09src/system/kernel/vm&In-Reply-To=%3C200701141841.l0EIfwOt027030%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000202.html">
   <LINK REL="Next"  HREF="000204.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Haiku-commits] r19796 - in haiku/trunk: headers/private/kernel	src/system/kernel/vm</H1>
    <B>axeld at BerliOS</B> 
    <A HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r19796%20-%20in%20haiku/trunk%3A%20headers/private/kernel%0A%09src/system/kernel/vm&In-Reply-To=%3C200701141841.l0EIfwOt027030%40sheep.berlios.de%3E"
       TITLE="[Haiku-commits] r19796 - in haiku/trunk: headers/private/kernel	src/system/kernel/vm">axeld at mail.berlios.de
       </A><BR>
    <I>Sun Jan 14 19:41:58 CET 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000202.html">[Haiku-commits] r19795 - in haiku/trunk: build/jam	src/add-ons/accelerants/vmware
</A></li>
        <LI>Next message: <A HREF="000204.html">[Haiku-commits] r19797 - in haiku/trunk/src/add-ons:	accelerants/vmware kernel/drivers/graphics/vmware
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#203">[ date ]</a>
              <a href="thread.html#203">[ thread ]</a>
              <a href="subject.html#203">[ subject ]</a>
              <a href="author.html#203">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: axeld
Date: 2007-01-14 19:41:57 +0100 (Sun, 14 Jan 2007)
New Revision: 19796
ViewCVS: <A HREF="http://svn.berlios.de/viewcvs/haiku?rev=19796&amp;view=rev">http://svn.berlios.de/viewcvs/haiku?rev=19796&amp;view=rev</A>

Modified:
   haiku/trunk/headers/private/kernel/vm_cache.h
   haiku/trunk/src/system/kernel/vm/vm.cpp
   haiku/trunk/src/system/kernel/vm/vm_cache.c
Log:
* vm_copy_on_write_area() did not always correctly divide the ref_count of the
  two cache_refs - it needs to count the consumers of the lower cache to find
  its actual number of references; the upper cache could still be in use by
  someone else.
* There were several locking bugs in the VM code; since cache_ref::cache can
  change, we must not access it without having the cache_ref locked.
* As a result, map_backing_store() now requires you to have the lock of the
  store's cache_ref held.
* And therefore, some functions in vm_cache.c must no longer lock the cache_ref
  on their own, but require the caller to have it locked already.
* Added the -s option to the cache/cache_ref KDL commands: it will only print
  the requested structure, and not its counterpart (useful if accessing one
  structure results in a page fault, as was possible previously).


Modified: haiku/trunk/headers/private/kernel/vm_cache.h
===================================================================
--- haiku/trunk/headers/private/kernel/vm_cache.h	2007-01-14 14:26:29 UTC (rev 19795)
+++ haiku/trunk/headers/private/kernel/vm_cache.h	2007-01-14 18:41:57 UTC (rev 19796)
@@ -1,5 +1,5 @@
 /*
- * Copyright 2003-2006, Axel D&#246;rfler, <A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">axeld at pinc-software.de.</A>
+ * Copyright 2003-2007, Axel D&#246;rfler, <A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">axeld at pinc-software.de.</A>
  * Distributed under the terms of the MIT License.
  *
  * Copyright 2001-2002, Travis Geiselbrecht. All rights reserved.
@@ -28,11 +28,11 @@
 void vm_cache_insert_page(vm_cache_ref *cacheRef, vm_page *page, off_t offset);
 void vm_cache_remove_page(vm_cache_ref *cacheRef, vm_page *page);
 void vm_cache_remove_consumer(vm_cache_ref *cacheRef, vm_cache *consumer);
-void vm_cache_add_consumer(vm_cache_ref *cacheRef, vm_cache *consumer);
+void vm_cache_add_consumer_locked(vm_cache_ref *cacheRef, vm_cache *consumer);
 status_t vm_cache_write_modified(vm_cache_ref *ref, bool fsReenter);
-status_t vm_cache_set_minimal_commitment(vm_cache_ref *ref, off_t commitment);
+status_t vm_cache_set_minimal_commitment_locked(vm_cache_ref *ref, off_t commitment);
 status_t vm_cache_resize(vm_cache_ref *cacheRef, off_t newSize);
-status_t vm_cache_insert_area(vm_cache_ref *cacheRef, vm_area *area);
+status_t vm_cache_insert_area_locked(vm_cache_ref *cacheRef, vm_area *area);
 status_t vm_cache_remove_area(vm_cache_ref *cacheRef, vm_area *area);
 
 #ifdef __cplusplus

Modified: haiku/trunk/src/system/kernel/vm/vm.cpp
===================================================================
--- haiku/trunk/src/system/kernel/vm/vm.cpp	2007-01-14 14:26:29 UTC (rev 19795)
+++ haiku/trunk/src/system/kernel/vm/vm.cpp	2007-01-14 18:41:57 UTC (rev 19796)
@@ -501,6 +501,9 @@
 }
 
 
+/*!
+	The \a store's owner must be locked when calling this function.
+*/
 static status_t
 map_backing_store(vm_address_space *addressSpace, vm_store *store, void **_virtualAddress,
 	off_t offset, addr_t size, uint32 addressSpec, int wiring, int protection,
@@ -550,7 +553,7 @@
 		newCache-&gt;temporary = 1;
 		newCache-&gt;scan_skip = cache-&gt;scan_skip;
 
-		vm_cache_add_consumer(cacheRef, newCache);
+		vm_cache_add_consumer_locked(cacheRef, newCache);
 
 		cache = newCache;
 		cacheRef = newCache-&gt;ref;
@@ -559,7 +562,7 @@
 		cache-&gt;virtual_size = offset + size;
 	}
 
-	status = vm_cache_set_minimal_commitment(cacheRef, offset + size);
+	status = vm_cache_set_minimal_commitment_locked(cacheRef, offset + size);
 	if (status != B_OK)
 		goto err2;
 
@@ -581,7 +584,7 @@
 	area-&gt;cache_ref = cacheRef;
 	area-&gt;cache_offset = offset;
 	// point the cache back to the area
-	vm_cache_insert_area(cacheRef, area);
+	vm_cache_insert_area_locked(cacheRef, area);
 
 	// insert the area in the global area hash table
 	acquire_sem_etc(sAreaHashLock, WRITE_COUNT, 0 ,0);
@@ -817,8 +820,13 @@
 			break;
 	}
 
+	mutex_lock(&amp;cache-&gt;ref-&gt;lock);
+
 	status = map_backing_store(addressSpace, store, address, 0, size, addressSpec, wiring,
 		protection, REGION_NO_PRIVATE_MAP, &amp;area, name);
+
+	mutex_unlock(&amp;cache-&gt;ref-&gt;lock);
+
 	if (status &lt; B_OK) {
 		vm_cache_release_ref(cache-&gt;ref);
 		goto err1;
@@ -1006,8 +1014,13 @@
 	cache-&gt;scan_skip = 1;
 	cache-&gt;virtual_size = size;
 
+	mutex_lock(&amp;cache-&gt;ref-&gt;lock);
+
 	status = map_backing_store(addressSpace, store, _address, 0, size,
 		addressSpec &amp; ~B_MTR_MASK, 0, protection, REGION_NO_PRIVATE_MAP, &amp;area, name);
+
+	mutex_unlock(&amp;cache-&gt;ref-&gt;lock);
+
 	if (status &lt; B_OK)
 		vm_cache_release_ref(cache-&gt;ref);
 
@@ -1083,8 +1096,12 @@
 	cache-&gt;scan_skip = 1;
 	cache-&gt;virtual_size = size;
 
+	mutex_lock(&amp;cache-&gt;ref-&gt;lock);
+
 	status = map_backing_store(addressSpace, store, address, 0, size, addressSpec, 0,
 		B_KERNEL_READ_AREA, REGION_NO_PRIVATE_MAP, &amp;area, name);
+
+	mutex_unlock(&amp;cache-&gt;ref-&gt;lock);
 	vm_put_address_space(addressSpace);
 
 	if (status &lt; B_OK) {
@@ -1188,9 +1205,13 @@
 	if (status &lt; B_OK)
 		goto err1;
 
+	mutex_lock(&amp;cacheRef-&gt;lock);
+
 	status = map_backing_store(addressSpace, cacheRef-&gt;cache-&gt;store, _address,
 		offset, size, addressSpec, 0, protection, mapping, &amp;area, name);
 
+	mutex_unlock(&amp;cacheRef-&gt;lock);
+
 	if (status &lt; B_OK || mapping == REGION_PRIVATE_MAP) {
 		// map_backing_store() cannot know we no longer need the ref
 		vm_cache_release_ref(cacheRef);
@@ -1282,9 +1303,13 @@
 	} else
 #endif
 	{
+		mutex_lock(&amp;sourceArea-&gt;cache_ref-&gt;lock);
+
 		status = map_backing_store(addressSpace, sourceArea-&gt;cache_ref-&gt;cache-&gt;store,
 			address, sourceArea-&gt;cache_offset, sourceArea-&gt;size, addressSpec,
 			sourceArea-&gt;wiring, protection, mapping, &amp;newArea, name);
+
+		mutex_unlock(&amp;sourceArea-&gt;cache_ref-&gt;lock);
 	}
 	if (status == B_OK &amp;&amp; mapping != REGION_PRIVATE_MAP) {
 		// If the mapping is REGION_PRIVATE_MAP, map_backing_store() needed
@@ -1463,22 +1488,28 @@
 	// So the old cache gets a new cache_ref and the area a new cache.
 
 	upperCacheRef = area-&gt;cache_ref;
+
+	// we will exchange the cache_ref's cache, so we better hold its lock
+	mutex_lock(&amp;upperCacheRef-&gt;lock);
+
 	lowerCache = upperCacheRef-&gt;cache;
 
 	// create an anonymous store object
 	store = vm_store_create_anonymous_noswap(false, 0, 0);
-	if (store == NULL)
-		return B_NO_MEMORY;
+	if (store == NULL) {
+		status = B_NO_MEMORY;
+		goto err1;
+	}
 
 	upperCache = vm_cache_create(store);
 	if (upperCache == NULL) {
 		status = B_NO_MEMORY;
-		goto err1;
+		goto err2;
 	}
 
 	status = vm_cache_ref_create(lowerCache);
 	if (status &lt; B_OK)
-		goto err2;
+		goto err3;
 
 	lowerCacheRef = lowerCache-&gt;ref;
 
@@ -1488,24 +1519,30 @@
 		protection |= B_READ_AREA;
 
 	// we need to hold the cache_ref lock when we want to switch its cache
-	mutex_lock(&amp;upperCacheRef-&gt;lock);
 	mutex_lock(&amp;lowerCacheRef-&gt;lock);
 
 	upperCache-&gt;temporary = 1;
 	upperCache-&gt;scan_skip = lowerCache-&gt;scan_skip;
 	upperCache-&gt;virtual_base = lowerCache-&gt;virtual_base;
 	upperCache-&gt;virtual_size = lowerCache-&gt;virtual_size;
-	upperCache-&gt;source = lowerCache;
-	list_add_item(&amp;lowerCache-&gt;consumers, upperCache);
+
 	upperCache-&gt;ref = upperCacheRef;
 	upperCacheRef-&gt;cache = upperCache;
 
 	// we need to manually alter the ref_count (divide it between the two)
-	lowerCacheRef-&gt;ref_count = upperCacheRef-&gt;ref_count - 1;
-	upperCacheRef-&gt;ref_count = 1;
+	// the lower cache_ref has only known refs, so compute them
+	{
+		int32 count = 0;
+		vm_cache *consumer = NULL;
+		while ((consumer = (vm_cache *)list_get_next_item(
+				&amp;lowerCache-&gt;consumers, consumer)) != NULL) {
+			count++;
+		}
+		lowerCacheRef-&gt;ref_count = count;
+		atomic_add(&amp;upperCacheRef-&gt;ref_count, -count);
+	}
 
-	// grab a ref to the cache object we're now linked to as a source
-	vm_cache_acquire_ref(lowerCacheRef);
+	vm_cache_add_consumer_locked(lowerCacheRef, upperCache);
 
 	// We now need to remap all pages from the area read-only, so that
 	// a copy will be created on next write access
@@ -1527,10 +1564,12 @@
 
 	return B_OK;
 
+err3:
+	free(upperCache);
 err2:
-	free(upperCache);
+	store-&gt;ops-&gt;destroy(store);
 err1:
-	store-&gt;ops-&gt;destroy(store);
+	mutex_unlock(&amp;upperCacheRef-&gt;lock);
 	return status;
 }
 
@@ -1565,11 +1604,15 @@
 
 	// First, create a cache on top of the source area
 
+	mutex_lock(&amp;cacheRef-&gt;lock);
+
 	status = map_backing_store(addressSpace, cacheRef-&gt;cache-&gt;store, _address,
 		source-&gt;cache_offset, source-&gt;size, addressSpec, source-&gt;wiring, protection,
 		writableCopy ? REGION_PRIVATE_MAP : REGION_NO_PRIVATE_MAP,
 		&amp;target, name);
 
+	mutex_unlock(&amp;cacheRef-&gt;lock);
+
 	if (status &lt; B_OK)
 		goto err;
 
@@ -1640,10 +1683,10 @@
 	}
 
 	cacheRef = area-&gt;cache_ref;
+	mutex_lock(&amp;cacheRef-&gt;lock);
+
 	cache = cacheRef-&gt;cache;
 
-	mutex_lock(&amp;cacheRef-&gt;lock);
-
 	if ((area-&gt;protection &amp; (B_WRITE_AREA | B_KERNEL_WRITE_AREA)) != 0
 		&amp;&amp; (newProtection &amp; (B_WRITE_AREA | B_KERNEL_WRITE_AREA)) == 0) {
 		// change from read/write to read-only
@@ -1860,25 +1903,39 @@
 	vm_cache *cache;
 	vm_cache_ref *cacheRef;
 	bool showPages = false;
-	int addressIndex = 1;
+	bool showCache = true;
+	bool showCacheRef = true;
+	int i = 1;
 
 	if (argc &lt; 2) {
-		kprintf(&quot;usage: %s [-p] &lt;address&gt;\n&quot;
-			&quot;  if -p is specified, all pages are shown.&quot;, argv[0]);
+		kprintf(&quot;usage: %s [-ps] &lt;address&gt;\n&quot;
+			&quot;  if -p is specified, all pages are shown, if -s is used\n&quot;
+			&quot;  only the cache/cache_ref info is shown respectively.\n&quot;, argv[0]);
 		return 0;
 	}
-	if (!strcmp(argv[1], &quot;-p&quot;)) {
-		showPages = true;
-		addressIndex++;
+	while (argv[i][0] == '-') {
+		char *arg = argv[i] + 1;
+		while (arg[0]) {
+			if (arg[0] == 'p')
+				showPages = true;
+			else if (arg[0] == 's') {
+				if (!strcmp(argv[0], &quot;cache&quot;))
+					showCacheRef = false;
+				else
+					showCache = false;
+			}
+			arg++;
+		}
+		i++;
 	}
-	if (strlen(argv[addressIndex]) &lt; 2
-		|| argv[addressIndex][0] != '0'
-		|| argv[addressIndex][1] != 'x') {
+	if (argv[i] == NULL || strlen(argv[i]) &lt; 2
+		|| argv[i][0] != '0'
+		|| argv[i][1] != 'x') {
 		kprintf(&quot;%s: invalid argument, pass address\n&quot;, argv[0]);
 		return 0;
 	}
 
-	addr_t address = strtoul(argv[addressIndex], NULL, 0);
+	addr_t address = strtoul(argv[i], NULL, 0);
 	if (address == NULL)
 		return 0;
 
@@ -1890,54 +1947,62 @@
 		cache = cacheRef-&gt;cache;
 	}
 
-	kprintf(&quot;cache_ref at %p:\n&quot;, cacheRef);
-	kprintf(&quot;  ref_count:    %ld\n&quot;, cacheRef-&gt;ref_count);
-	kprintf(&quot;  lock.holder:  %ld\n&quot;, cacheRef-&gt;lock.holder);
-	kprintf(&quot;  lock.sem:     0x%lx\n&quot;, cacheRef-&gt;lock.sem);
-	kprintf(&quot;  areas:\n&quot;);
+	if (showCacheRef) {
+		kprintf(&quot;cache_ref at %p:\n&quot;, cacheRef);
+		if (!showCache)
+			kprintf(&quot;  cache:        %p\n&quot;, cacheRef-&gt;cache);
+		kprintf(&quot;  ref_count:    %ld\n&quot;, cacheRef-&gt;ref_count);
+		kprintf(&quot;  lock.holder:  %ld\n&quot;, cacheRef-&gt;lock.holder);
+		kprintf(&quot;  lock.sem:     0x%lx\n&quot;, cacheRef-&gt;lock.sem);
+		kprintf(&quot;  areas:\n&quot;);
 
-	for (vm_area *area = cacheRef-&gt;areas; area != NULL; area = area-&gt;cache_next) {
-		kprintf(&quot;   area 0x%lx, %s\n&quot;, area-&gt;id, area-&gt;name);
-		kprintf(&quot;\tbase_addr:  0x%lx, size: 0x%lx\n&quot;, area-&gt;base, area-&gt;size);
-		kprintf(&quot;\tprotection: 0x%lx\n&quot;, area-&gt;protection);
-		kprintf(&quot;\towner:      0x%lx &quot;, area-&gt;address_space-&gt;id);
+		for (vm_area *area = cacheRef-&gt;areas; area != NULL; area = area-&gt;cache_next) {
+			kprintf(&quot;    area 0x%lx, %s\n&quot;, area-&gt;id, area-&gt;name);
+			kprintf(&quot;\tbase_addr:  0x%lx, size: 0x%lx\n&quot;, area-&gt;base, area-&gt;size);
+			kprintf(&quot;\tprotection: 0x%lx\n&quot;, area-&gt;protection);
+			kprintf(&quot;\towner:      0x%lx\n&quot;, area-&gt;address_space-&gt;id);
+		}
 	}
 
-	kprintf(&quot;cache at %p:\n&quot;, cache);
-	kprintf(&quot;  source:       %p\n&quot;, cache-&gt;source);
-	kprintf(&quot;  store:        %p\n&quot;, cache-&gt;store);
-	kprintf(&quot;  virtual_base: 0x%Lx\n&quot;, cache-&gt;virtual_base);
-	kprintf(&quot;  virtual_size: 0x%Lx\n&quot;, cache-&gt;virtual_size);
-	kprintf(&quot;  temporary:    %ld\n&quot;, cache-&gt;temporary);
-	kprintf(&quot;  scan_skip:    %ld\n&quot;, cache-&gt;scan_skip);
+	if (showCache) {
+		kprintf(&quot;cache at %p:\n&quot;, cache);
+		if (!showCacheRef)
+			kprintf(&quot;  cache_ref:    %p\n&quot;, cache-&gt;ref);
+		kprintf(&quot;  source:       %p\n&quot;, cache-&gt;source);
+		kprintf(&quot;  store:        %p\n&quot;, cache-&gt;store);
+		kprintf(&quot;  virtual_base: 0x%Lx\n&quot;, cache-&gt;virtual_base);
+		kprintf(&quot;  virtual_size: 0x%Lx\n&quot;, cache-&gt;virtual_size);
+		kprintf(&quot;  temporary:    %ld\n&quot;, cache-&gt;temporary);
+		kprintf(&quot;  scan_skip:    %ld\n&quot;, cache-&gt;scan_skip);
 
-	kprintf(&quot;  consumers:\n&quot;);
-	vm_cache *consumer = NULL;
-	while ((consumer = (vm_cache *)list_get_next_item(&amp;cache-&gt;consumers, consumer)) != NULL) {
-		kprintf(&quot;\t%p\n&quot;, consumer);
-	}
+		kprintf(&quot;  consumers:\n&quot;);
+		vm_cache *consumer = NULL;
+		while ((consumer = (vm_cache *)list_get_next_item(&amp;cache-&gt;consumers, consumer)) != NULL) {
+			kprintf(&quot;\t%p\n&quot;, consumer);
+		}
 
-	kprintf(&quot;  pages:\n&quot;);
-	int32 count = 0;
-	for (vm_page *page = cache-&gt;page_list; page != NULL; page = page-&gt;cache_next) {
-		count++;
+		kprintf(&quot;  pages:\n&quot;);
+		int32 count = 0;
+		for (vm_page *page = cache-&gt;page_list; page != NULL; page = page-&gt;cache_next) {
+			count++;
+			if (!showPages)
+				continue;
+
+			if (page-&gt;type == PAGE_TYPE_PHYSICAL) {
+				kprintf(&quot;\t%p ppn 0x%lx offset 0x%lx type %ld state %ld (%s) ref_count %ld\n&quot;,
+					page, page-&gt;physical_page_number, page-&gt;cache_offset, page-&gt;type, page-&gt;state, 
+					page_state_to_text(page-&gt;state), page-&gt;ref_count);
+			} else if(page-&gt;type == PAGE_TYPE_DUMMY) {
+				kprintf(&quot;\t%p DUMMY PAGE state %ld (%s)\n&quot;, 
+					page, page-&gt;state, page_state_to_text(page-&gt;state));
+			} else
+				kprintf(&quot;\t%p UNKNOWN PAGE type %ld\n&quot;, page, page-&gt;type);
+		}
+
 		if (!showPages)
-			continue;
-
-		if (page-&gt;type == PAGE_TYPE_PHYSICAL) {
-			kprintf(&quot;\t%p ppn 0x%lx offset 0x%lx type %ld state %ld (%s) ref_count %ld\n&quot;,
-				page, page-&gt;physical_page_number, page-&gt;cache_offset, page-&gt;type, page-&gt;state, 
-				page_state_to_text(page-&gt;state), page-&gt;ref_count);
-		} else if(page-&gt;type == PAGE_TYPE_DUMMY) {
-			kprintf(&quot;\t%p DUMMY PAGE state %ld (%s)\n&quot;, 
-				page, page-&gt;state, page_state_to_text(page-&gt;state));
-		} else
-			kprintf(&quot;\t%p UNKNOWN PAGE type %ld\n&quot;, page, page-&gt;type);
+			kprintf(&quot;\t%ld in cache\n&quot;, count);
 	}
 
-	if (!showPages)
-		kprintf(&quot;\t%ld in cache\n&quot;, count);
-
 	return 0;
 }
 
@@ -2624,6 +2689,8 @@
 	changeCount = addressSpace-&gt;change_count;
 	release_sem_etc(addressSpace-&gt;sem, READ_COUNT, 0);
 
+	mutex_lock(&amp;topCacheRef-&gt;lock);
+
 	// See if this cache has a fault handler - this will do all the work for us
 	if (topCacheRef-&gt;cache-&gt;store-&gt;ops-&gt;fault != NULL) {
 		// Note, since the page fault is resolved with interrupts enabled, the
@@ -2631,12 +2698,15 @@
 		// the store must take this into account
 		status_t status = (*topCacheRef-&gt;cache-&gt;store-&gt;ops-&gt;fault)(topCacheRef-&gt;cache-&gt;store, addressSpace, cacheOffset);
 		if (status != B_BAD_HANDLER) {
+			mutex_unlock(&amp;topCacheRef-&gt;lock);
 			vm_cache_release_ref(topCacheRef);
 			vm_put_address_space(addressSpace);
 			return status;
 		}
 	}
 
+	mutex_unlock(&amp;topCacheRef-&gt;lock);
+
 	// The top most cache has no fault handler, so let's see if the cache or its sources
 	// already have the page we're searching for (we're going from top to bottom)
 
@@ -2833,10 +2903,10 @@
 
 		if (dummyPage.state == PAGE_STATE_BUSY) {
 			// we had inserted the dummy cache in another cache, so let's remove it from there
-			vm_cache_ref *temp_cache = dummyPage.cache-&gt;ref;
-			mutex_lock(&amp;temp_cache-&gt;lock);
-			vm_cache_remove_page(temp_cache, &amp;dummyPage);
-			mutex_unlock(&amp;temp_cache-&gt;lock);
+			vm_cache_ref *tempRef = dummyPage.cache-&gt;ref;
+			mutex_lock(&amp;tempRef-&gt;lock);
+			vm_cache_remove_page(tempRef, &amp;dummyPage);
+			mutex_unlock(&amp;tempRef-&gt;lock);
 			dummyPage.state = PAGE_STATE_INACTIVE;
 		}
 	}
@@ -2875,10 +2945,10 @@
 	if (dummyPage.state == PAGE_STATE_BUSY) {
 		// We still have the dummy page in the cache - that happens if we didn't need
 		// to allocate a new page before, but could use one in another cache
-		vm_cache_ref *temp_cache = dummyPage.cache-&gt;ref;
-		mutex_lock(&amp;temp_cache-&gt;lock);
-		vm_cache_remove_page(temp_cache, &amp;dummyPage);
-		mutex_unlock(&amp;temp_cache-&gt;lock);
+		vm_cache_ref *tempRef = dummyPage.cache-&gt;ref;
+		mutex_lock(&amp;tempRef-&gt;lock);
+		vm_cache_remove_page(tempRef, &amp;dummyPage);
+		mutex_unlock(&amp;tempRef-&gt;lock);
 		dummyPage.state = PAGE_STATE_INACTIVE;
 	}
 
@@ -3339,7 +3409,7 @@
 status_t
 resize_area(area_id areaID, size_t newSize)
 {
-	vm_cache_ref *cache;
+	vm_cache_ref *cacheRef;
 	vm_area *area, *current;
 	status_t status = B_OK;
 	size_t oldSize;
@@ -3352,25 +3422,25 @@
 	if (area == NULL)
 		return B_BAD_VALUE;
 
+	cacheRef = area-&gt;cache_ref;
+	mutex_lock(&amp;cacheRef-&gt;lock);
+
 	// Resize all areas of this area's cache
 
-	cache = area-&gt;cache_ref;
 	oldSize = area-&gt;size;
 
 	// ToDo: we should only allow to resize anonymous memory areas!
-	if (!cache-&gt;cache-&gt;temporary) {
+	if (!cacheRef-&gt;cache-&gt;temporary) {
 		status = B_NOT_ALLOWED;
-		goto err1;
+		goto out;
 	}
 
 	// ToDo: we must lock all address spaces here!
 
-	mutex_lock(&amp;cache-&gt;lock);
-
 	if (oldSize &lt; newSize) {
 		// We need to check if all areas of this cache can be resized
 
-		for (current = cache-&gt;areas; current; current = current-&gt;cache_next) {
+		for (current = cacheRef-&gt;areas; current; current = current-&gt;cache_next) {
 			if (current-&gt;address_space_next &amp;&amp; current-&gt;address_space_next-&gt;base &lt;= (current-&gt;base + newSize)) {
 				// if the area was created inside a reserved area, it can also be
 				// resized in that area
@@ -3381,14 +3451,14 @@
 					continue;
 
 				status = B_ERROR;
-				goto err2;
+				goto out;
 			}
 		}
 	}
 
 	// Okay, looks good so far, so let's do it
 
-	for (current = cache-&gt;areas; current; current = current-&gt;cache_next) {
+	for (current = cacheRef-&gt;areas; current; current = current-&gt;cache_next) {
 		if (current-&gt;address_space_next &amp;&amp; current-&gt;address_space_next-&gt;base &lt;= (current-&gt;base + newSize)) {
 			vm_area *next = current-&gt;address_space_next;
 			if (next-&gt;id == RESERVED_AREA_ID &amp;&amp; next-&gt;cache_offset &lt;= current-&gt;base
@@ -3421,17 +3491,16 @@
 	}
 
 	if (status == B_OK)
-		status = vm_cache_resize(cache, newSize);
+		status = vm_cache_resize(cacheRef, newSize);
 
 	if (status &lt; B_OK) {
 		// This shouldn't really be possible, but hey, who knows
-		for (current = cache-&gt;areas; current; current = current-&gt;cache_next)
+		for (current = cacheRef-&gt;areas; current; current = current-&gt;cache_next)
 			current-&gt;size = oldSize;
 	}
 
-err2:
-	mutex_unlock(&amp;cache-&gt;lock);
-err1:
+out:
+	mutex_unlock(&amp;cacheRef-&gt;lock);
 	vm_put_area(area);
 
 	// ToDo: we must honour the lock restrictions of this area

Modified: haiku/trunk/src/system/kernel/vm/vm_cache.c
===================================================================
--- haiku/trunk/src/system/kernel/vm/vm_cache.c	2007-01-14 14:26:29 UTC (rev 19795)
+++ haiku/trunk/src/system/kernel/vm/vm_cache.c	2007-01-14 18:41:57 UTC (rev 19796)
@@ -185,7 +185,7 @@
 	// delete this cache
 
 	if (cacheRef-&gt;areas != NULL)
-		panic(&quot;cache to be deleted still has areas&quot;);
+		panic(&quot;cache_ref %p to be deleted still has areas&quot;, cacheRef);
 	if (!list_is_empty(&amp;cacheRef-&gt;cache-&gt;consumers))
 		panic(&quot;cache %p to be deleted still has consumers&quot;, cacheRef-&gt;cache);
 
@@ -330,14 +330,18 @@
 }
 
 
+/*!
+	Commits the memory to the store if the \a commitment is larger than
+	what's committed already.
+	Assumes you have the \a ref's lock held.
+*/
 status_t
-vm_cache_set_minimal_commitment(vm_cache_ref *ref, off_t commitment)
+vm_cache_set_minimal_commitment_locked(vm_cache_ref *ref, off_t commitment)
 {
 	status_t status = B_OK;
-	vm_store *store;
+	vm_store *store = ref-&gt;cache-&gt;store;
 
-	mutex_lock(&amp;ref-&gt;lock);
-	store = ref-&gt;cache-&gt;store;
+	ASSERT_LOCKED_MUTEX(&amp;ref-&gt;lock);
 
 	// If we don't have enough committed space to cover through to the new end of region...
 	if (store-&gt;committed_size &lt; commitment) {
@@ -348,7 +352,6 @@
 		status = store-&gt;ops-&gt;commit(store, commitment);
 	}
 
-	mutex_unlock(&amp;ref-&gt;lock);
 	return status;
 }
 
@@ -399,6 +402,7 @@
 /*!
 	Removes the \a consumer from the \a cacheRef's cache.
 	It will also release the reference to the cacheRef owned by the consumer.
+	Assumes you have the consumer's cache_ref lock held.
 */
 void
 vm_cache_remove_consumer(vm_cache_ref *cacheRef, vm_cache *consumer)
@@ -407,9 +411,9 @@
 	vm_cache *newSource = NULL;
 
 	TRACE((&quot;remove consumer vm cache %p from cache %p\n&quot;, consumer, cache));
+	ASSERT_LOCKED_MUTEX(&amp;consumer-&gt;ref-&gt;lock);
 
 	mutex_lock(&amp;cacheRef-&gt;lock);
-
 	list_remove_item(&amp;cache-&gt;consumers, consumer);
 	consumer-&gt;source = NULL;
 
@@ -453,19 +457,21 @@
 
 	if (newSource != NULL) {
 		// The remaining consumer has gotten a new source
+		mutex_lock(&amp;newSource-&gt;ref-&gt;lock);
+
 		list_remove_item(&amp;newSource-&gt;consumers, cache);
 		list_add_item(&amp;newSource-&gt;consumers, consumer);
 		consumer-&gt;source = newSource;
 		cache-&gt;source = NULL;
 
-		mutex_unlock(&amp;cacheRef-&gt;lock);
+		mutex_unlock(&amp;newSource-&gt;ref-&gt;lock);
 
 		// Release the other reference to the cache - we take over
 		// its reference of its source cache
 		vm_cache_release_ref(cacheRef);
-	} else
-		mutex_unlock(&amp;cacheRef-&gt;lock);
+	}
 
+	mutex_unlock(&amp;cacheRef-&gt;lock);
 	vm_cache_release_ref(cacheRef);
 }
 
@@ -474,26 +480,30 @@
 	Marks the \a cacheRef's cache as source of the \a consumer cache,
 	and adds the \a consumer to its list.
 	This also grabs a reference to the source cache.
+	Assumes you have the cache_ref and the consumer's lock held.
 */
 void
-vm_cache_add_consumer(vm_cache_ref *cacheRef, vm_cache *consumer)
+vm_cache_add_consumer_locked(vm_cache_ref *cacheRef, vm_cache *consumer)
 {
 	TRACE((&quot;add consumer vm cache %p to cache %p\n&quot;, consumer, cacheRef-&gt;cache));
-	mutex_lock(&amp;cacheRef-&gt;lock);
+	ASSERT_LOCKED_MUTEX(&amp;cacheRef-&gt;lock);
+	ASSERT_LOCKED_MUTEX(&amp;consumer-&gt;ref-&gt;lock);
 
 	consumer-&gt;source = cacheRef-&gt;cache;
 	list_add_item(&amp;cacheRef-&gt;cache-&gt;consumers, consumer);
 
 	vm_cache_acquire_ref(cacheRef);
-
-	mutex_unlock(&amp;cacheRef-&gt;lock);
 }
 
 
+/*!
+	Adds the \a area to the \a cacheRef.
+	Assumes you have the locked the cache_ref.
+*/
 status_t
-vm_cache_insert_area(vm_cache_ref *cacheRef, vm_area *area)
+vm_cache_insert_area_locked(vm_cache_ref *cacheRef, vm_area *area)
 {
-	mutex_lock(&amp;cacheRef-&gt;lock);
+	ASSERT_LOCKED_MUTEX(&amp;cacheRef-&gt;lock);
 
 	area-&gt;cache_next = cacheRef-&gt;areas;
 	if (area-&gt;cache_next)
@@ -501,7 +511,6 @@
 	area-&gt;cache_prev = NULL;
 	cacheRef-&gt;areas = area;
 
-	mutex_unlock(&amp;cacheRef-&gt;lock);
 	return B_OK;
 }
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000202.html">[Haiku-commits] r19795 - in haiku/trunk: build/jam	src/add-ons/accelerants/vmware
</A></li>
	<LI>Next message: <A HREF="000204.html">[Haiku-commits] r19797 - in haiku/trunk/src/add-ons:	accelerants/vmware kernel/drivers/graphics/vmware
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#203">[ date ]</a>
              <a href="thread.html#203">[ thread ]</a>
              <a href="subject.html#203">[ subject ]</a>
              <a href="author.html#203">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/haiku-commits">More information about the Haiku-commits
mailing list</a><br>
</body></html>
