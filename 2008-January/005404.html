<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Haiku-commits] r23440 -	haiku/trunk/src/system/boot/platform/atari_m68k
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/haiku-commits/2008-January/index.html" >
   <LINK REL="made" HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r23440%20-%0A%09haiku/trunk/src/system/boot/platform/atari_m68k&In-Reply-To=%3C200801121604.m0CG4igA026258%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="005403.html">
   <LINK REL="Next"  HREF="005405.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Haiku-commits] r23440 -	haiku/trunk/src/system/boot/platform/atari_m68k</H1>
    <B>mmu_man at BerliOS</B> 
    <A HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r23440%20-%0A%09haiku/trunk/src/system/boot/platform/atari_m68k&In-Reply-To=%3C200801121604.m0CG4igA026258%40sheep.berlios.de%3E"
       TITLE="[Haiku-commits] r23440 -	haiku/trunk/src/system/boot/platform/atari_m68k">mmu_man at mail.berlios.de
       </A><BR>
    <I>Sat Jan 12 17:04:44 CET 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="005403.html">[Haiku-commits] r23439 - haiku/trunk/src/system/libroot/posix/unistd
</A></li>
        <LI>Next message: <A HREF="005405.html">[Haiku-commits] r23441 - haiku/trunk/src/system/kernel/fs
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#5404">[ date ]</a>
              <a href="thread.html#5404">[ thread ]</a>
              <a href="subject.html#5404">[ subject ]</a>
              <a href="author.html#5404">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: mmu_man
Date: 2008-01-12 17:04:44 +0100 (Sat, 12 Jan 2008)
New Revision: 23440
ViewCVS: <A HREF="http://svn.berlios.de/viewcvs/haiku?rev=23440&amp;view=rev">http://svn.berlios.de/viewcvs/haiku?rev=23440&amp;view=rev</A>

Added:
   haiku/trunk/src/system/boot/platform/atari_m68k/mmu.cpp
   haiku/trunk/src/system/boot/platform/atari_m68k/mmu.h
Modified:
   haiku/trunk/src/system/boot/platform/atari_m68k/start.c
Log:
* x86 mmu init code, to be changed.
* remove some unneeded stuff in start.
* we just quit the boot prg instead of rebooting (should try Puntaes also)


Added: haiku/trunk/src/system/boot/platform/atari_m68k/mmu.cpp
===================================================================
--- haiku/trunk/src/system/boot/platform/atari_m68k/mmu.cpp	2008-01-12 15:51:02 UTC (rev 23439)
+++ haiku/trunk/src/system/boot/platform/atari_m68k/mmu.cpp	2008-01-12 16:04:44 UTC (rev 23440)
@@ -0,0 +1,689 @@
+/*
+ * Copyright 2004-2007, Axel D&#246;rfler, <A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">axeld at pinc-software.de.</A>
+ * Based on code written by Travis Geiselbrecht for NewOS.
+ *
+ * Distributed under the terms of the MIT License.
+ */
+
+
+#include &quot;mmu.h&quot;
+#include &quot;bios.h&quot;
+
+#include &lt;boot/platform.h&gt;
+#include &lt;boot/stdio.h&gt;
+#include &lt;boot/kernel_args.h&gt;
+#include &lt;boot/stage2.h&gt;
+#include &lt;arch/cpu.h&gt;
+#include &lt;arch_kernel.h&gt;
+#include &lt;kernel.h&gt;
+
+#include &lt;OS.h&gt;
+
+#include &lt;string.h&gt;
+
+
+/** The (physical) memory layout of the boot loader is currently as follows:
+ *	  0x0500 - 0x10000	protected mode stack
+ *	  0x0500 - 0x09000	real mode stack
+ *	 0x10000 - ?		code (up to ~500 kB)
+ *	 0x90000			1st temporary page table (identity maps 0-4 MB)
+ *	 0x91000			2nd (4-8 MB)
+ *	 0x92000 - 0x92000	further page tables
+ *	 0x9e000 - 0xa0000	SMP trampoline code
+ *	[0xa0000 - 0x100000	BIOS/ROM/reserved area]
+ *	0x100000			page directory
+ *	     ...			boot loader heap (32 kB)
+ *	     ...			free physical memory
+ *
+ *	The first 8 MB are identity mapped (0x0 - 0x0800000); paging is turned
+ *	on. The kernel is mapped at 0x80000000, all other stuff mapped by the
+ *	loader (kernel args, modules, driver settings, ...) comes after
+ *	0x81000000 which means that there is currently only 1 MB reserved for
+ *	the kernel itself (see kMaxKernelSize).
+ */
+
+//#define TRACE_MMU
+#ifdef TRACE_MMU
+#	define TRACE(x) dprintf x
+#else
+#	define TRACE(x) ;
+#endif
+
+struct gdt_idt_descr {
+	uint16 limit;
+	uint32 *base;
+} _PACKED;
+
+// memory structure returned by int 0x15, ax 0xe820
+struct extended_memory {
+	uint64 base_addr;
+	uint64 length;
+	uint32 type;
+};
+
+#ifdef _PXE_ENV
+
+static const uint32 kDefaultPageTableFlags = 0x07;	// present, user, R/W
+static const size_t kMaxKernelSize = 0x100000;		// 1 MB for the kernel
+
+// working page directory and page table
+static uint32 *sPageDirectory = 0;
+
+static addr_t sNextPhysicalAddress = 0x112000;
+static addr_t sNextVirtualAddress = KERNEL_BASE + kMaxKernelSize;
+static addr_t sMaxVirtualAddress = KERNEL_BASE + 0x400000;
+
+static addr_t sNextPageTableAddress = 0x7d000;
+static const uint32 kPageTableRegionEnd = 0x8b000;
+	// we need to reserve 2 pages for the SMP trampoline code
+
+#else
+
+static const uint32 kDefaultPageTableFlags = 0x07;	// present, user, R/W
+static const size_t kMaxKernelSize = 0x100000;		// 1 MB for the kernel
+
+// working page directory and page table
+static uint32 *sPageDirectory = 0;
+
+static addr_t sNextPhysicalAddress = 0x100000;
+static addr_t sNextVirtualAddress = KERNEL_BASE + kMaxKernelSize;
+static addr_t sMaxVirtualAddress = KERNEL_BASE + 0x400000;
+
+static addr_t sNextPageTableAddress = 0x90000;
+static const uint32 kPageTableRegionEnd = 0x9e000;
+	// we need to reserve 2 pages for the SMP trampoline code
+
+#endif
+
+
+static addr_t
+get_next_virtual_address(size_t size)
+{
+	addr_t address = sNextVirtualAddress;
+	sNextVirtualAddress += size;
+
+	return address;
+}
+
+
+static addr_t
+get_next_physical_address(size_t size)
+{
+	addr_t address = sNextPhysicalAddress;
+	sNextPhysicalAddress += size;
+
+	return address;
+}
+
+
+static addr_t
+get_next_virtual_page()
+{
+	return get_next_virtual_address(B_PAGE_SIZE);
+}
+
+
+static addr_t
+get_next_physical_page()
+{
+	return get_next_physical_address(B_PAGE_SIZE);
+}
+
+
+static uint32 *
+get_next_page_table()
+{
+	TRACE((&quot;get_next_page_table, sNextPageTableAddress %p, kPageTableRegionEnd %p\n&quot;, 
+		sNextPageTableAddress, kPageTableRegionEnd));
+	
+	addr_t address = sNextPageTableAddress;
+	if (address &gt;= kPageTableRegionEnd)
+		return (uint32 *)get_next_physical_page();
+
+	sNextPageTableAddress += B_PAGE_SIZE;
+	return (uint32 *)address;
+}
+
+
+/**	Adds a new page table for the specified base address */
+
+static void
+add_page_table(addr_t base)
+{
+	TRACE((&quot;add_page_table(base = %p)\n&quot;, (void *)base));
+
+	// Get new page table and clear it out
+	uint32 *pageTable = get_next_page_table();
+	if (pageTable &gt; (uint32 *)(8 * 1024 * 1024))
+		panic(&quot;tried to add page table beyond the indentity mapped 8 MB region\n&quot;);
+
+	gKernelArgs.arch_args.pgtables[gKernelArgs.arch_args.num_pgtables++] = (uint32)pageTable;
+
+	for (int32 i = 0; i &lt; 1024; i++)
+		pageTable[i] = 0;
+
+	// put the new page table into the page directory
+	sPageDirectory[base/(4*1024*1024)] = (uint32)pageTable | kDefaultPageTableFlags;
+}
+
+
+static void
+unmap_page(addr_t virtualAddress)
+{
+	TRACE((&quot;unmap_page(virtualAddress = %p)\n&quot;, (void *)virtualAddress));
+
+	if (virtualAddress &lt; KERNEL_BASE)
+		panic(&quot;unmap_page: asked to unmap invalid page %p!\n&quot;, (void *)virtualAddress);
+
+	// unmap the page from the correct page table
+	uint32 *pageTable = (uint32 *)(sPageDirectory[virtualAddress
+		/ (B_PAGE_SIZE * 1024)] &amp; 0xfffff000);
+	pageTable[(virtualAddress % (B_PAGE_SIZE * 1024)) / B_PAGE_SIZE] = 0;
+
+	asm volatile(&quot;invlpg (%0)&quot; : : &quot;r&quot; (virtualAddress));
+}
+
+
+/** Creates an entry to map the specified virtualAddress to the given
+ *	physicalAddress.
+ *	If the mapping goes beyond the current page table, it will allocate
+ *	a new one. If it cannot map the requested page, it panics.
+ */
+
+static void
+map_page(addr_t virtualAddress, addr_t physicalAddress, uint32 flags)
+{
+	TRACE((&quot;map_page: vaddr 0x%lx, paddr 0x%lx\n&quot;, virtualAddress, physicalAddress));
+
+	if (virtualAddress &lt; KERNEL_BASE)
+		panic(&quot;map_page: asked to map invalid page %p!\n&quot;, (void *)virtualAddress);
+
+	if (virtualAddress &gt;= sMaxVirtualAddress) {
+		// we need to add a new page table
+
+		add_page_table(sMaxVirtualAddress);
+		sMaxVirtualAddress += B_PAGE_SIZE * 1024;
+
+		if (virtualAddress &gt;= sMaxVirtualAddress)
+			panic(&quot;map_page: asked to map a page to %p\n&quot;, (void *)virtualAddress);
+	}
+
+	physicalAddress &amp;= ~(B_PAGE_SIZE - 1);
+
+	// map the page to the correct page table
+	uint32 *pageTable = (uint32 *)(sPageDirectory[virtualAddress
+		/ (B_PAGE_SIZE * 1024)] &amp; 0xfffff000);
+	uint32 tableEntry = (virtualAddress % (B_PAGE_SIZE * 1024)) / B_PAGE_SIZE;
+	
+	TRACE((&quot;map_page: inserting pageTable %p, tableEntry %ld, physicalAddress %p\n&quot;, 
+		pageTable, tableEntry, physicalAddress));
+
+	pageTable[tableEntry] = physicalAddress | flags;
+
+	asm volatile(&quot;invlpg (%0)&quot; : : &quot;r&quot; (virtualAddress));
+
+	TRACE((&quot;map_page: done\n&quot;));
+}
+
+
+static void
+sort_addr_range(addr_range *range, int count)
+{
+	addr_range tempRange;
+	bool done;
+	int i;
+
+	do {
+		done = true;
+		for (i = 1; i &lt; count; i++) {
+			if (range[i].start &lt; range[i - 1].start) {
+				done = false;
+				memcpy(&amp;tempRange, &amp;range[i], sizeof(addr_range));
+				memcpy(&amp;range[i], &amp;range[i - 1], sizeof(addr_range));
+				memcpy(&amp;range[i - 1], &amp;tempRange, sizeof(addr_range));
+			}
+		}
+	} while (!done);
+}
+
+
+static uint32
+get_memory_map(extended_memory **_extendedMemory)
+{
+	extended_memory *block = (extended_memory *)kExtraSegmentScratch;
+	bios_regs regs = { 0, 0, sizeof(extended_memory), 0, 0, (uint32)block, 0, 0};
+	uint32 count = 0;
+
+	TRACE((&quot;get_memory_map()\n&quot;));
+
+	do {
+		regs.eax = 0xe820;
+		regs.edx = 'SMAP';
+
+		call_bios(0x15, &amp;regs);
+		if (regs.flags &amp; CARRY_FLAG)
+			return 0;
+
+		regs.edi += sizeof(extended_memory);
+		count++;
+	} while (regs.ebx != 0);
+
+	*_extendedMemory = block;
+
+#ifdef TRACE_MMU
+	dprintf(&quot;extended memory info (from 0xe820):\n&quot;);
+	for (uint32 i = 0; i &lt; count; i++) {
+		dprintf(&quot;    base 0x%Lx, len 0x%Lx, type %lu\n&quot;, 
+			block[i].base_addr, block[i].length, block[i].type);
+	}
+#endif
+
+	return count;
+}
+
+
+static void
+init_page_directory(void)
+{
+	TRACE((&quot;init_page_directory\n&quot;));
+
+	// allocate a new pgdir
+	sPageDirectory = (uint32 *)get_next_physical_page();
+	gKernelArgs.arch_args.phys_pgdir = (uint32)sPageDirectory;
+
+	// clear out the pgdir
+	for (int32 i = 0; i &lt; 1024; i++) {
+		sPageDirectory[i] = 0;
+	}
+
+	// Identity map the first 8 MB of memory so that their
+	// physical and virtual address are the same.
+	// These page tables won't be taken over into the kernel.
+
+	// make the first page table at the first free spot
+	uint32 *pageTable = get_next_page_table();
+
+	for (int32 i = 0; i &lt; 1024; i++) {
+		pageTable[i] = (i * 0x1000) | kDefaultPageFlags;
+	}
+
+	sPageDirectory[0] = (uint32)pageTable | kDefaultPageFlags;
+
+	// make the second page table
+	pageTable = get_next_page_table();
+
+	for (int32 i = 0; i &lt; 1024; i++) {
+		pageTable[i] = (i * 0x1000 + 0x400000) | kDefaultPageFlags;
+	}
+
+	sPageDirectory[1] = (uint32)pageTable | kDefaultPageFlags;
+
+	gKernelArgs.arch_args.num_pgtables = 0;
+	add_page_table(KERNEL_BASE);
+
+	// switch to the new pgdir and enable paging
+	asm(&quot;movl %0, %%eax;&quot;
+		&quot;movl %%eax, %%cr3;&quot; : : &quot;m&quot; (sPageDirectory) : &quot;eax&quot;);
+	// Important.  Make sure supervisor threads can fault on read only pages...
+	asm(&quot;movl %%eax, %%cr0&quot; : : &quot;a&quot; ((1 &lt;&lt; 31) | (1 &lt;&lt; 16) | (1 &lt;&lt; 5) | 1));
+}
+
+
+//	#pragma mark -
+
+
+extern &quot;C&quot; addr_t
+mmu_map_physical_memory(addr_t physicalAddress, size_t size, uint32 flags)
+{
+	addr_t address = sNextVirtualAddress;
+	addr_t pageOffset = physicalAddress &amp; (B_PAGE_SIZE - 1);
+
+	physicalAddress -= pageOffset;
+
+	for (addr_t offset = 0; offset &lt; size; offset += B_PAGE_SIZE) {
+		map_page(get_next_virtual_page(), physicalAddress + offset, flags);
+	}
+
+	return address + pageOffset;
+}
+
+
+extern &quot;C&quot; void *
+mmu_allocate(void *virtualAddress, size_t size)
+{
+	TRACE((&quot;mmu_allocate: requested vaddr: %p, next free vaddr: 0x%lx, size: %ld\n&quot;,
+		virtualAddress, sNextVirtualAddress, size));
+
+	size = (size + B_PAGE_SIZE - 1) / B_PAGE_SIZE;
+		// get number of pages to map
+
+	if (virtualAddress != NULL) {
+		// This special path is almost only useful for loading the
+		// kernel into memory; it will only allow you to map the
+		// 1 MB following the kernel base address.
+		// Also, it won't check for already mapped addresses, so
+		// you better know why you are here :)
+		addr_t address = (addr_t)virtualAddress;
+
+		// is the address within the valid range?
+		if (address &lt; KERNEL_BASE || address + size &gt;= KERNEL_BASE + kMaxKernelSize)
+			return NULL;
+
+		for (uint32 i = 0; i &lt; size; i++) {
+			map_page(address, get_next_physical_page(), kDefaultPageFlags);
+			address += B_PAGE_SIZE;
+		}
+
+		return virtualAddress;
+	}
+
+	void *address = (void *)sNextVirtualAddress;
+
+	for (uint32 i = 0; i &lt; size; i++) {
+		map_page(get_next_virtual_page(), get_next_physical_page(), kDefaultPageFlags);
+	}
+
+	return address;
+}
+
+
+/**	This will unmap the allocated chunk of memory from the virtual
+ *	address space. It might not actually free memory (as its implementation
+ *	is very simple), but it might.
+ */
+
+extern &quot;C&quot; void
+mmu_free(void *virtualAddress, size_t size)
+{
+	TRACE((&quot;mmu_free(virtualAddress = %p, size: %ld)\n&quot;, virtualAddress, size));
+
+	addr_t address = (addr_t)virtualAddress;
+	size = (size + B_PAGE_SIZE - 1) / B_PAGE_SIZE;
+		// get number of pages to map
+
+	// is the address within the valid range?
+	if (address &lt; KERNEL_BASE
+		|| address + size &gt;= KERNEL_BASE + kMaxKernelSize) {
+		panic(&quot;mmu_free: asked to unmap out of range region (%p, size %lx)\n&quot;,
+			(void *)address, size);
+	}
+
+	// unmap all pages within the range
+	for (uint32 i = 0; i &lt; size; i++) {
+		unmap_page(address);
+		address += B_PAGE_SIZE;
+	}
+
+	if (address == sNextVirtualAddress) {
+		// we can actually reuse the virtual address space
+		sNextVirtualAddress -= size;
+	}
+}
+
+
+/** Sets up the final and kernel accessible GDT and IDT tables.
+ *	BIOS calls won't work any longer after this function has
+ *	been called.
+ */
+
+extern &quot;C&quot; void
+mmu_init_for_kernel(void)
+{
+	TRACE((&quot;mmu_init_for_kernel\n&quot;));
+	// set up a new idt
+	{
+		struct gdt_idt_descr idtDescriptor;
+		uint32 *idt;
+
+		// find a new idt
+		idt = (uint32 *)get_next_physical_page();
+		gKernelArgs.arch_args.phys_idt = (uint32)idt;
+
+		TRACE((&quot;idt at %p\n&quot;, idt));
+
+		// map the idt into virtual space
+		gKernelArgs.arch_args.vir_idt = (uint32)get_next_virtual_page();
+		map_page(gKernelArgs.arch_args.vir_idt, (uint32)idt, kDefaultPageFlags);
+
+		// clear it out
+		uint32* virtualIDT = (uint32*)gKernelArgs.arch_args.vir_idt;
+		for (int32 i = 0; i &lt; IDT_LIMIT / 4; i++) {
+			virtualIDT[i] = 0;
+		}
+
+		// load the idt
+		idtDescriptor.limit = IDT_LIMIT - 1;
+		idtDescriptor.base = (uint32 *)gKernelArgs.arch_args.vir_idt;
+
+		asm(&quot;lidt	%0;&quot;
+			: : &quot;m&quot; (idtDescriptor));
+
+		TRACE((&quot;idt at virtual address 0x%lx\n&quot;, gKernelArgs.arch_args.vir_idt));
+	}
+
+	// set up a new gdt
+	{
+		struct gdt_idt_descr gdtDescriptor;
+		segment_descriptor *gdt;
+
+		// find a new gdt
+		gdt = (segment_descriptor *)get_next_physical_page();
+		gKernelArgs.arch_args.phys_gdt = (uint32)gdt;
+
+		TRACE((&quot;gdt at %p\n&quot;, gdt));
+
+		// map the gdt into virtual space
+		gKernelArgs.arch_args.vir_gdt = (uint32)get_next_virtual_page();
+		map_page(gKernelArgs.arch_args.vir_gdt, (uint32)gdt, kDefaultPageFlags);
+
+		// put standard segment descriptors in it
+		segment_descriptor* virtualGDT
+			= (segment_descriptor*)gKernelArgs.arch_args.vir_gdt;
+		clear_segment_descriptor(&amp;virtualGDT[0]);
+
+		// seg 0x08 - kernel 4GB code
+		set_segment_descriptor(&amp;virtualGDT[1], 0, 0xffffffff, DT_CODE_READABLE,
+			DPL_KERNEL);
+
+		// seg 0x10 - kernel 4GB data
+		set_segment_descriptor(&amp;virtualGDT[2], 0, 0xffffffff, DT_DATA_WRITEABLE,
+			DPL_KERNEL);
+		
+		// seg 0x1b - ring 3 user 4GB code
+		set_segment_descriptor(&amp;virtualGDT[3], 0, 0xffffffff, DT_CODE_READABLE,
+			DPL_USER);
+
+		// seg 0x23 - ring 3 user 4GB data
+		set_segment_descriptor(&amp;virtualGDT[4], 0, 0xffffffff, DT_DATA_WRITEABLE,
+			DPL_USER);
+
+		// virtualGDT[5] and above will be filled later by the kernel
+		// to contain the TSS descriptors, and for TLS (one for every CPU)
+
+		// load the GDT
+		gdtDescriptor.limit = GDT_LIMIT - 1;
+		gdtDescriptor.base = (uint32 *)gKernelArgs.arch_args.vir_gdt;
+
+		asm(&quot;lgdt	%0;&quot;
+			: : &quot;m&quot; (gdtDescriptor));
+
+		TRACE((&quot;gdt at virtual address %p\n&quot;, (void *)gKernelArgs.arch_args.vir_gdt));
+	}
+
+	// save the memory we've physically allocated
+	gKernelArgs.physical_allocated_range[0].size = sNextPhysicalAddress - gKernelArgs.physical_allocated_range[0].start;
+
+	// save the memory we've virtually allocated (for the kernel and other stuff)
+	gKernelArgs.virtual_allocated_range[0].start = KERNEL_BASE;
+	gKernelArgs.virtual_allocated_range[0].size = sNextVirtualAddress - KERNEL_BASE;
+	gKernelArgs.num_virtual_allocated_ranges = 1;
+
+	// sort the address ranges
+	sort_addr_range(gKernelArgs.physical_memory_range, gKernelArgs.num_physical_memory_ranges);
+	sort_addr_range(gKernelArgs.physical_allocated_range, gKernelArgs.num_physical_allocated_ranges);
+	sort_addr_range(gKernelArgs.virtual_allocated_range, gKernelArgs.num_virtual_allocated_ranges);
+
+#ifdef TRACE_MMU
+	{
+		uint32 i;
+
+		dprintf(&quot;phys memory ranges:\n&quot;);
+		for (i = 0; i &lt; gKernelArgs.num_physical_memory_ranges; i++) {
+			dprintf(&quot;    base 0x%08lx, length 0x%08lx\n&quot;, gKernelArgs.physical_memory_range[i].start, gKernelArgs.physical_memory_range[i].size);
+		}
+
+		dprintf(&quot;allocated phys memory ranges:\n&quot;);
+		for (i = 0; i &lt; gKernelArgs.num_physical_allocated_ranges; i++) {
+			dprintf(&quot;    base 0x%08lx, length 0x%08lx\n&quot;, gKernelArgs.physical_allocated_range[i].start, gKernelArgs.physical_allocated_range[i].size);
+		}
+
+		dprintf(&quot;allocated virt memory ranges:\n&quot;);
+		for (i = 0; i &lt; gKernelArgs.num_virtual_allocated_ranges; i++) {
+			dprintf(&quot;    base 0x%08lx, length 0x%08lx\n&quot;, gKernelArgs.virtual_allocated_range[i].start, gKernelArgs.virtual_allocated_range[i].size);
+		}
+	}
+#endif
+}
+
+
+extern &quot;C&quot; void
+mmu_init(void)
+{
+	TRACE((&quot;mmu_init\n&quot;));
+
+	gKernelArgs.physical_allocated_range[0].start = sNextPhysicalAddress;
+	gKernelArgs.physical_allocated_range[0].size = 0;
+	gKernelArgs.num_physical_allocated_ranges = 1;
+		// remember the start of the allocated physical pages
+
+	init_page_directory();
+
+	// Map the page directory into kernel space at 0xffc00000-0xffffffff
+	// this enables a mmu trick where the 4 MB region that this pgdir entry
+	// represents now maps the 4MB of potential pagetables that the pgdir
+	// points to. Thrown away later in VM bringup, but useful for now.
+	sPageDirectory[1023] = (uint32)sPageDirectory | kDefaultPageFlags;
+
+	// also map it on the next vpage
+	gKernelArgs.arch_args.vir_pgdir = get_next_virtual_page();
+	map_page(gKernelArgs.arch_args.vir_pgdir, (uint32)sPageDirectory, kDefaultPageFlags);
+
+	// map in a kernel stack
+	gKernelArgs.cpu_kstack[0].start = (addr_t)mmu_allocate(NULL, KERNEL_STACK_SIZE);
+	gKernelArgs.cpu_kstack[0].size = KERNEL_STACK_SIZE;
+
+	TRACE((&quot;kernel stack at 0x%lx to 0x%lx\n&quot;, gKernelArgs.cpu_kstack[0].start,
+		gKernelArgs.cpu_kstack[0].start + gKernelArgs.cpu_kstack[0].size));
+
+	extended_memory *extMemoryBlock;
+	uint32 extMemoryCount = get_memory_map(&amp;extMemoryBlock);
+
+	// figure out the memory map
+	if (extMemoryCount &gt; 0) {
+		gKernelArgs.num_physical_memory_ranges = 0;
+
+		for (uint32 i = 0; i &lt; extMemoryCount; i++) {
+			// Type 1 is available memory
+			if (extMemoryBlock[i].type == 1) {
+				// round everything up to page boundaries, exclusive of pages
+				// it partially occupies
+				extMemoryBlock[i].length -= (extMemoryBlock[i].base_addr % B_PAGE_SIZE)
+					? (B_PAGE_SIZE - (extMemoryBlock[i].base_addr % B_PAGE_SIZE)) : 0;
+				extMemoryBlock[i].base_addr = ROUNDUP(extMemoryBlock[i].base_addr, B_PAGE_SIZE);
+				extMemoryBlock[i].length = ROUNDOWN(extMemoryBlock[i].length, B_PAGE_SIZE);
+
+				// we ignore all memory beyond 4 GB
+				if (extMemoryBlock[i].base_addr &gt; 0xffffffffULL)
+					continue;
+				if (extMemoryBlock[i].base_addr + extMemoryBlock[i].length &gt; 0xffffffffULL)
+					extMemoryBlock[i].length = 0x100000000ULL - extMemoryBlock[i].base_addr;
+
+				if (gKernelArgs.num_physical_memory_ranges &gt; 0) {
+					// we might want to extend a previous hole
+					addr_t previousEnd = gKernelArgs.physical_memory_range[
+							gKernelArgs.num_physical_memory_ranges - 1].start
+						+ gKernelArgs.physical_memory_range[
+							gKernelArgs.num_physical_memory_ranges - 1].size;
+					addr_t holeSize = extMemoryBlock[i].base_addr - previousEnd;
+
+					// if the hole is smaller than 1 MB, we try to mark the memory
+					// as allocated and extend the previous memory range
+					if (previousEnd &lt;= extMemoryBlock[i].base_addr
+						&amp;&amp; holeSize &lt; 0x100000
+						&amp;&amp; insert_physical_allocated_range(previousEnd,
+							extMemoryBlock[i].base_addr - previousEnd) == B_OK) {
+						gKernelArgs.physical_memory_range[
+							gKernelArgs.num_physical_memory_ranges - 1].size += holeSize;
+					}
+				}
+
+				insert_physical_memory_range(extMemoryBlock[i].base_addr,
+					extMemoryBlock[i].length);
+			}
+		}
+	} else {
+		// ToDo: for now!
+		dprintf(&quot;No extended memory block - using 32 MB (fix me!)\n&quot;);
+		uint32 memSize = 32 * 1024 * 1024;
+
+		// we dont have an extended map, assume memory is contiguously mapped at 0x0
+		gKernelArgs.physical_memory_range[0].start = 0;
+		gKernelArgs.physical_memory_range[0].size = memSize;
+		gKernelArgs.num_physical_memory_ranges = 1;
+
+		// mark the bios area allocated
+		gKernelArgs.physical_allocated_range[gKernelArgs.num_physical_allocated_ranges].start = 0x9f000; // 640k - 1 page
+		gKernelArgs.physical_allocated_range[gKernelArgs.num_physical_allocated_ranges].size = 0x61000;
+		gKernelArgs.num_physical_allocated_ranges++;
+	}
+
+	gKernelArgs.arch_args.page_hole = 0xffc00000;
+}
+
+
+//	#pragma mark -
+
+
+extern &quot;C&quot; status_t
+platform_allocate_region(void **_address, size_t size, uint8 protection,
+	bool /*exactAddress*/)
+{
+	void *address = mmu_allocate(*_address, size);
+	if (address == NULL)
+		return B_NO_MEMORY;
+
+	*_address = address;
+	return B_OK;
+}
+
+
+extern &quot;C&quot; status_t
+platform_free_region(void *address, size_t size)
+{
+	mmu_free(address, size);
+	return B_OK;
+}
+
+
+void
+platform_release_heap(struct stage2_args *args, void *base)
+{
+	// It will be freed automatically, since it is in the
+	// identity mapped region, and not stored in the kernel's
+	// page tables.
+}
+
+
+status_t
+platform_init_heap(struct stage2_args *args, void **_base, void **_top)
+{
+	void *heap = (void *)get_next_physical_address(args-&gt;heap_size);
+	if (heap == NULL)
+		return B_NO_MEMORY;
+
+	*_base = heap;
+	*_top = (void *)((int8 *)heap + args-&gt;heap_size);
+	return B_OK;
+}
+
+

Added: haiku/trunk/src/system/boot/platform/atari_m68k/mmu.h
===================================================================
--- haiku/trunk/src/system/boot/platform/atari_m68k/mmu.h	2008-01-12 15:51:02 UTC (rev 23439)
+++ haiku/trunk/src/system/boot/platform/atari_m68k/mmu.h	2008-01-12 16:04:44 UTC (rev 23440)
@@ -0,0 +1,29 @@
+/*
+ * Copyright 2004-2005, Axel D&#246;rfler, <A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">axeld at pinc-software.de.</A> All rights reserved.
+ * Distributed under the terms of the MIT License.
+ */
+#ifndef MMU_H
+#define MMU_H
+
+
+#include &lt;SupportDefs.h&gt;
+
+
+// For use with mmu_map_physical_memory()
+static const uint32 kDefaultPageFlags = 0x3;	// present, R/W
+
+#ifdef __cplusplus
+extern &quot;C&quot; {
+#endif
+
+extern void mmu_init(void);
+extern void mmu_init_for_kernel(void);
+extern addr_t mmu_map_physical_memory(addr_t physicalAddress, size_t size, uint32 flags);
+extern void *mmu_allocate(void *virtualAddress, size_t size);
+extern void mmu_free(void *virtualAddress, size_t size);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif	/* MMU_H */

Modified: haiku/trunk/src/system/boot/platform/atari_m68k/start.c
===================================================================
--- haiku/trunk/src/system/boot/platform/atari_m68k/start.c	2008-01-12 15:51:02 UTC (rev 23439)
+++ haiku/trunk/src/system/boot/platform/atari_m68k/start.c	2008-01-12 16:04:44 UTC (rev 23440)
@@ -72,10 +72,10 @@
 		// or I don't see something important...
 	addr_t stackTop = gKernelArgs.cpu_kstack[0].start + gKernelArgs.cpu_kstack[0].size;
 
-	smp_init_other_cpus();
-	serial_cleanup();
+	//smp_init_other_cpus();
+	//serial_cleanup();
 	mmu_init_for_kernel();
-	smp_boot_other_cpus();
+	//smp_boot_other_cpus();
 
 	dprintf(&quot;kernel entry at %lx\n&quot;, gKernelArgs.kernel_image.elf_header.e_entry);
 
@@ -96,8 +96,9 @@
 void
 platform_exit(void)
 {
-	// reset the system using the keyboard controller
-	out8(0xfe, 0x64);
+	// Terminate
+	// XXX: Puntaes() instead ?
+	Pterm0();
 }
 
 
@@ -126,7 +127,7 @@
 	// reading the keyboard doesn't seem to work in graphics mode (maybe a bochs problem)
 	sBootOptions = check_for_boot_keys();
 	//if (sBootOptions &amp; BOOT_OPTION_DEBUG_OUTPUT)
-		serial_enable();
+		//serial_enable();
 
 	//apm_init();
 	//smp_init();


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="005403.html">[Haiku-commits] r23439 - haiku/trunk/src/system/libroot/posix/unistd
</A></li>
	<LI>Next message: <A HREF="005405.html">[Haiku-commits] r23441 - haiku/trunk/src/system/kernel/fs
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#5404">[ date ]</a>
              <a href="thread.html#5404">[ thread ]</a>
              <a href="subject.html#5404">[ subject ]</a>
              <a href="author.html#5404">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/haiku-commits">More information about the Haiku-commits
mailing list</a><br>
</body></html>
