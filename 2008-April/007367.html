<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Haiku-commits] r24742 - haiku/trunk/src/system/kernel/vm
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/haiku-commits/2008-April/index.html" >
   <LINK REL="made" HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r24742%20-%20haiku/trunk/src/system/kernel/vm&In-Reply-To=%3C200804021219.m32CJT5k018206%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="007375.html">
   <LINK REL="Next"  HREF="007368.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Haiku-commits] r24742 - haiku/trunk/src/system/kernel/vm</H1>
    <B>axeld at BerliOS</B> 
    <A HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r24742%20-%20haiku/trunk/src/system/kernel/vm&In-Reply-To=%3C200804021219.m32CJT5k018206%40sheep.berlios.de%3E"
       TITLE="[Haiku-commits] r24742 - haiku/trunk/src/system/kernel/vm">axeld at mail.berlios.de
       </A><BR>
    <I>Wed Apr  2 14:19:29 CEST 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="007375.html">[Haiku-commits] r24741 - in haiku/trunk/src/servers/app: . drawing
</A></li>
        <LI>Next message: <A HREF="007368.html">[Haiku-commits] r24742 - haiku/trunk/src/system/kernel/vm
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#7367">[ date ]</a>
              <a href="thread.html#7367">[ thread ]</a>
              <a href="subject.html#7367">[ subject ]</a>
              <a href="author.html#7367">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: axeld
Date: 2008-04-02 14:19:28 +0200 (Wed, 02 Apr 2008)
New Revision: 24742
ViewCVS: <A HREF="http://svn.berlios.de/viewcvs/haiku?rev=24742&amp;view=rev">http://svn.berlios.de/viewcvs/haiku?rev=24742&amp;view=rev</A>

Modified:
   haiku/trunk/src/system/kernel/vm/vm_cache.cpp
   haiku/trunk/src/system/kernel/vm/vm_store_anonymous_noswap.cpp
Log:
* Fixed several occasions of bugs with respect to the handling of
  overcommitting stores:
  - has_precommitted was incorrectly set to true in the constructor
  - when a precommitted page was committed, vm_store::committed_size
    was still changed.
  - unreserving memory did not update vm_store::committed_size.
  - when precommitted pages were committed, their page count instead of their
    size was reserved.
* All this lead to bug #1970 which should be fixed now.
* Cleanup of vm_cache.cpp, no functional change.


Modified: haiku/trunk/src/system/kernel/vm/vm_cache.cpp
===================================================================
--- haiku/trunk/src/system/kernel/vm/vm_cache.cpp	2008-04-02 11:12:39 UTC (rev 24741)
+++ haiku/trunk/src/system/kernel/vm/vm_cache.cpp	2008-04-02 12:19:28 UTC (rev 24742)
@@ -1,5 +1,5 @@
 /*
- * Copyright 2002-2007, Axel D&#246;rfler, <A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">axeld at pinc-software.de.</A>
+ * Copyright 2002-2008, Axel D&#246;rfler, <A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">axeld at pinc-software.de.</A>
  * Distributed under the terms of the MIT License.
  *
  * Copyright 2001-2002, Travis Geiselbrecht. All rights reserved.
@@ -34,7 +34,7 @@
 #endif
 
 
-static hash_table *sPageCacheTable;
+static hash_table* sPageCacheTable;
 static spinlock sPageCacheTableLock;
 
 #if DEBUG_CACHE_LIST
@@ -45,15 +45,15 @@
 
 struct page_lookup_key {
 	uint32	offset;
-	vm_cache *cache;
+	vm_cache* cache;
 };
 
 
 static int
-page_compare_func(void *_p, const void *_key)
+page_compare_func(void* _p, const void* _key)
 {
-	vm_page *page = (vm_page *)_p;
-	const struct page_lookup_key *key = (page_lookup_key *)_key;
+	vm_page* page = (vm_page*)_p;
+	const struct page_lookup_key* key = (page_lookup_key*)_key;
 
 	TRACE((&quot;page_compare_func: page %p, key %p\n&quot;, page, key));
 
@@ -65,10 +65,10 @@
 
 
 static uint32
-page_hash_func(void *_p, const void *_key, uint32 range)
+page_hash_func(void* _p, const void* _key, uint32 range)
 {
-	vm_page *page = (vm_page *)_p;
-	const struct page_lookup_key *key = (page_lookup_key *)_key;
+	vm_page* page = (vm_page*)_p;
+	const struct page_lookup_key* key = (page_lookup_key*)_key;
 
 	#define HASH(offset, ref) ((offset) + ((uint32)(ref) &gt;&gt; 6) * 997)
 		// sizeof(vm_cache) &gt;= 64, hence (uint32)(ref) &gt;&gt; 6 is still unique
@@ -101,7 +101,7 @@
 
 
 static void
-delete_cache(vm_cache *cache)
+delete_cache(vm_cache* cache)
 {
 	if (cache-&gt;areas != NULL)
 		panic(&quot;cache %p to be deleted still has areas&quot;, cache);
@@ -127,9 +127,9 @@
 	cache-&gt;store-&gt;ops-&gt;destroy(cache-&gt;store);
 
 	// free all of the pages in the cache
-	vm_page *page = cache-&gt;page_list;
+	vm_page* page = cache-&gt;page_list;
 	while (page) {
-		vm_page *oldPage = page;
+		vm_page* oldPage = page;
 		int state;
 
 		page = page-&gt;cache_next;
@@ -168,7 +168,7 @@
 
 
 status_t
-vm_cache_init(kernel_args *args)
+vm_cache_init(kernel_args* args)
 {
 	// TODO: The table should grow/shrink dynamically.
 	sPageCacheTable = hash_init(vm_page_num_pages() / 2,
@@ -180,17 +180,17 @@
 }
 
 
-vm_cache *
-vm_cache_create(vm_store *store)
+vm_cache*
+vm_cache_create(vm_store* store)
 {
-	vm_cache *cache;
+	vm_cache* cache;
 
 	if (store == NULL) {
 		panic(&quot;vm_cache created with NULL store!&quot;);
 		return NULL;
 	}
 
-	cache = (vm_cache *)malloc(sizeof(vm_cache));
+	cache = (vm_cache*)malloc(sizeof(vm_cache));
 	if (cache == NULL)
 		return NULL;
 
@@ -238,7 +238,7 @@
 
 
 void
-vm_cache_acquire_ref(vm_cache *cache)
+vm_cache_acquire_ref(vm_cache* cache)
 {
 	TRACE((&quot;vm_cache_acquire_ref: cache %p, ref will be %ld\n&quot;,
 		cache, cache-&gt;ref_count + 1));
@@ -251,7 +251,7 @@
 
 
 void
-vm_cache_release_ref(vm_cache *cache)
+vm_cache_release_ref(vm_cache* cache)
 {
 	TRACE((&quot;vm_cache_release_ref: cacheRef %p, ref will be %ld\n&quot;,
 		cache, cache-&gt;ref_count - 1));
@@ -268,8 +268,8 @@
 		void*				return_address;
 	};
 	int32 min = 0;
-	vm_area *a;
-	vm_cache *c;
+	vm_area* a;
+	vm_cache* c;
 	bool locked = false;
 	if (cacheRef-&gt;lock.holder != find_thread(NULL)) {
 		mutex_lock(&amp;cacheRef-&gt;lock);
@@ -281,7 +281,7 @@
 		min++;
 	dprintf(&quot;! %ld release cache_ref %p, ref_count is now %ld (min %ld, called from %p)\n&quot;,
 		find_thread(NULL), cacheRef, cacheRef-&gt;ref_count,
-		min, ((struct stack_frame *)x86_read_ebp())-&gt;return_address);
+		min, ((struct stack_frame*)x86_read_ebp())-&gt;return_address);
 	if (cacheRef-&gt;ref_count &lt; min)
 		panic(&quot;cache_ref %p has too little ref_count!!!!&quot;, cacheRef);
 	if (locked)
@@ -314,8 +314,8 @@
 }
 
 
-vm_page *
-vm_cache_lookup_page(vm_cache *cache, off_t offset)
+vm_page*
+vm_cache_lookup_page(vm_cache* cache, off_t offset)
 {
 	ASSERT_LOCKED_MUTEX(&amp;cache-&gt;lock);
 
@@ -326,7 +326,7 @@
 	cpu_status state = disable_interrupts();
 	acquire_spinlock(&amp;sPageCacheTableLock);
 
-	vm_page *page = (vm_page *)hash_lookup(sPageCacheTable, &amp;key);
+	vm_page* page = (vm_page*)hash_lookup(sPageCacheTable, &amp;key);
 
 	release_spinlock(&amp;sPageCacheTableLock);
 	restore_interrupts(state);
@@ -339,7 +339,7 @@
 
 
 void
-vm_cache_insert_page(vm_cache *cache, vm_page *page, off_t offset)
+vm_cache_insert_page(vm_cache* cache, vm_page* page, off_t offset)
 {
 	TRACE((&quot;vm_cache_insert_page: cache %p, page %p, offset %Ld\n&quot;,
 		cache, page, offset));
@@ -370,7 +370,7 @@
 	struct page_lookup_key key;
 	key.offset = (uint32)(offset &gt;&gt; PAGE_SHIFT);
 	key.cache = cache;
-	vm_page* otherPage = (vm_page *)hash_lookup(sPageCacheTable, &amp;key);
+	vm_page* otherPage = (vm_page*)hash_lookup(sPageCacheTable, &amp;key);
 	if (otherPage != NULL) {
 		panic(&quot;vm_cache_insert_page(): there's already page %p with cache &quot;
 			&quot;offset %lu in cache %p; inserting page %p&quot;, otherPage,
@@ -382,13 +382,12 @@
 }
 
 
-/*!
-	Removes the vm_page from this cache. Of course, the page must
+/*!	Removes the vm_page from this cache. Of course, the page must
 	really be in this cache or evil things will happen.
 	The cache lock must be held.
 */
 void
-vm_cache_remove_page(vm_cache *cache, vm_page *page)
+vm_cache_remove_page(vm_cache* cache, vm_page* page)
 {
 	TRACE((&quot;vm_cache_remove_page: cache %p, page %p\n&quot;, cache, page));
 	ASSERT_LOCKED_MUTEX(&amp;cache-&gt;lock);
@@ -422,7 +421,7 @@
 
 
 status_t
-vm_cache_write_modified(vm_cache *cache, bool fsReenter)
+vm_cache_write_modified(vm_cache* cache, bool fsReenter)
 {
 	TRACE((&quot;vm_cache_write_modified(cache = %p)\n&quot;, cache));
 
@@ -437,22 +436,22 @@
 }
 
 
-/*!
-	Commits the memory to the store if the \a commitment is larger than
+/*!	Commits the memory to the store if the \a commitment is larger than
 	what's committed already.
 	Assumes you have the \a ref's lock held.
 */
 status_t
-vm_cache_set_minimal_commitment_locked(vm_cache *cache, off_t commitment)
+vm_cache_set_minimal_commitment_locked(vm_cache* cache, off_t commitment)
 {
 	TRACE((&quot;vm_cache_set_minimal_commitment_locked(cache %p, commitment %Ld)\n&quot;,
 		cache, commitment));
 	ASSERT_LOCKED_MUTEX(&amp;cache-&gt;lock);
 
-	vm_store *store = cache-&gt;store;
+	vm_store* store = cache-&gt;store;
 	status_t status = B_OK;
 
-	// If we don't have enough committed space to cover through to the new end of region...
+	// If we don't have enough committed space to cover through to the new end
+	// of the area...
 	if (store-&gt;committed_size &lt; commitment) {
 		// ToDo: should we check if the cache's virtual size is large
 		//	enough for a commitment of that size?
@@ -465,8 +464,7 @@
 }
 
 
-/*!
-	This function updates the size field of the vm_cache structure.
+/*!	This function updates the size field of the vm_cache structure.
 	If needed, it will free up all pages that don't belong to the cache anymore.
 	The cache lock must be held when you call it.
 	Since removed pages don't belong to the cache any longer, they are not
@@ -476,7 +474,7 @@
 	has to wait for busy pages.
 */
 status_t
-vm_cache_resize(vm_cache *cache, off_t newSize)
+vm_cache_resize(vm_cache* cache, off_t newSize)
 {
 	TRACE((&quot;vm_cache_resize(cache %p, newSize %Ld) old size %Ld\n&quot;,
 		cache, newSize, cache-&gt;virtual_size));
@@ -493,7 +491,8 @@
 	if (newPageCount &lt; oldPageCount) {
 		// we need to remove all pages in the cache outside of the new virtual
 		// size
-		vm_page *page = cache-&gt;page_list, *next;
+		vm_page* page = cache-&gt;page_list;
+		vm_page* next;
 
 		while (page != NULL) {
 			next = page-&gt;cache_next;
@@ -534,13 +533,12 @@
 }
 
 
-/*!
-	Removes the \a consumer from the \a cache.
+/*!	Removes the \a consumer from the \a cache.
 	It will also release the reference to the cacheRef owned by the consumer.
 	Assumes you have the consumer's cache lock held.
 */
 void
-vm_cache_remove_consumer(vm_cache *cache, vm_cache *consumer)
+vm_cache_remove_consumer(vm_cache* cache, vm_cache* consumer)
 {
 	TRACE((&quot;remove consumer vm cache %p from cache %p\n&quot;, consumer, cache));
 	ASSERT_LOCKED_MUTEX(&amp;consumer-&gt;lock);
@@ -562,7 +560,7 @@
 		// The cache is not really needed anymore - it can be merged with its only
 		// consumer left.
 
-		consumer = (vm_cache *)list_get_first_item(&amp;cache-&gt;consumers);
+		consumer = (vm_cache*)list_get_first_item(&amp;cache-&gt;consumers);
 
 		bool merge = acquire_unreferenced_cache_ref(consumer);
 
@@ -598,16 +596,15 @@
 		}
 
 		if (merge) {
-			vm_page *page, *nextPage;
-			vm_cache *newSource;
+			consumer = (vm_cache*)list_remove_head_item(&amp;cache-&gt;consumers);
 
-			consumer = (vm_cache *)list_remove_head_item(&amp;cache-&gt;consumers);
-
 			TRACE((&quot;merge vm cache %p (ref == %ld) with vm cache %p\n&quot;,
 				cache, cache-&gt;ref_count, consumer));
 
-			for (page = cache-&gt;page_list; page != NULL; page = nextPage) {
-				vm_page *consumerPage;
+			vm_page* nextPage;
+			for (vm_page* page = cache-&gt;page_list; page != NULL;
+					page = nextPage) {
+				vm_page* consumerPage;
 				nextPage = page-&gt;cache_next;
 
 				consumerPage = vm_cache_lookup_page(consumer,
@@ -644,7 +641,7 @@
 				}
 			}
 
-			newSource = cache-&gt;source;
+			vm_cache* newSource = cache-&gt;source;
 
 			// The remaining consumer has gotten a new source
 			mutex_lock(&amp;newSource-&gt;lock);
@@ -667,7 +664,7 @@
 			mutex_unlock(&amp;consumer-&gt;lock);
 			vm_cache_release_ref(consumer);
 		}
-	
+
 		if (cache-&gt;busy)
 			busyCondition.Unpublish();
 	}
@@ -677,14 +674,13 @@
 }
 
 
-/*!
-	Marks the \a cache as source of the \a consumer cache,
+/*!	Marks the \a cache as source of the \a consumer cache,
 	and adds the \a consumer to its list.
 	This also grabs a reference to the source cache.
 	Assumes you have the cache and the consumer's lock held.
 */
 void
-vm_cache_add_consumer_locked(vm_cache *cache, vm_cache *consumer)
+vm_cache_add_consumer_locked(vm_cache* cache, vm_cache* consumer)
 {
 	TRACE((&quot;add consumer vm cache %p to cache %p\n&quot;, consumer, cache));
 	ASSERT_LOCKED_MUTEX(&amp;cache-&gt;lock);
@@ -700,12 +696,11 @@
 }
 
 
-/*!
-	Adds the \a area to the \a cache.
+/*!	Adds the \a area to the \a cache.
 	Assumes you have the locked the cache.
 */
 status_t
-vm_cache_insert_area_locked(vm_cache *cache, vm_area *area)
+vm_cache_insert_area_locked(vm_cache* cache, vm_area* area)
 {
 	TRACE((&quot;vm_cache_insert_area_locked(cache %p, area %p)\n&quot;, cache, area));
 	ASSERT_LOCKED_MUTEX(&amp;cache-&gt;lock);
@@ -724,7 +719,7 @@
 
 
 status_t
-vm_cache_remove_area(vm_cache *cache, vm_area *area)
+vm_cache_remove_area(vm_cache* cache, vm_area* area)
 {
 	TRACE((&quot;vm_cache_remove_area(cache %p, area %p)\n&quot;, cache, area));
 

Modified: haiku/trunk/src/system/kernel/vm/vm_store_anonymous_noswap.cpp
===================================================================
--- haiku/trunk/src/system/kernel/vm/vm_store_anonymous_noswap.cpp	2008-04-02 11:12:39 UTC (rev 24741)
+++ haiku/trunk/src/system/kernel/vm/vm_store_anonymous_noswap.cpp	2008-04-02 12:19:28 UTC (rev 24742)
@@ -1,5 +1,5 @@
-/* 
- * Copyright 2002-2007, Axel D&#246;rfler, <A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">axeld at pinc-software.de.</A>
+/*
+ * Copyright 2002-2008, Axel D&#246;rfler, <A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">axeld at pinc-software.de.</A>
  * Distributed under the terms of the MIT License.
  *
  * Copyright 2001-2002, Travis Geiselbrecht. All rights reserved.
@@ -25,7 +25,7 @@
 
 // The stack functionality looks like a good candidate to put into its own
 // store. I have not done this because once we have a swap file backing up
-// the memory, it would probably not be a good idea to separate this 
+// the memory, it would probably not be a good idea to separate this
 // anymore.
 
 typedef struct anonymous_store {
@@ -50,6 +50,9 @@
 {
 	anonymous_store *store = (anonymous_store *)_store;
 
+	size -= store-&gt;vm.cache-&gt;virtual_base;
+		// anonymous stores don't need to span over their whole source
+
 	// if we can overcommit, we don't commit here, but in anonymous_fault()
 	if (store-&gt;can_overcommit) {
 		if (store-&gt;has_precommitted)
@@ -57,26 +60,23 @@
 
 		// pre-commit some pages to make a later failure less probable
 		store-&gt;has_precommitted = true;
-		if (size &gt; store-&gt;vm.cache-&gt;virtual_base + store-&gt;precommitted_pages)
-			size = store-&gt;vm.cache-&gt;virtual_base + store-&gt;precommitted_pages;
+		uint32 precommitted = store-&gt;precommitted_pages * B_PAGE_SIZE;
+		if (size &gt; precommitted)
+			size = precommitted;
 	}
 
-	size -= store-&gt;vm.cache-&gt;virtual_base;
-		// anonymous stores don't need to span over their whole source
-
 	// Check to see how much we could commit - we need real memory
 
 	if (size &gt; store-&gt;vm.committed_size) {
 		// try to commit
 		if (vm_try_reserve_memory(size - store-&gt;vm.committed_size) != B_OK)
 			return B_NO_MEMORY;
-
-		store-&gt;vm.committed_size = size;
 	} else {
 		// we can release some
 		vm_unreserve_memory(store-&gt;vm.committed_size - size);
 	}
 
+	store-&gt;vm.committed_size = size;
 	return B_OK;
 }
 
@@ -136,10 +136,10 @@
 			// try to commit additional memory
 			if (vm_try_reserve_memory(B_PAGE_SIZE) != B_OK)
 				return B_NO_MEMORY;
+
+			store-&gt;vm.committed_size += B_PAGE_SIZE;
 		} else
 			store-&gt;precommitted_pages--;
-
-		store-&gt;vm.committed_size += B_PAGE_SIZE;
 	}
 
 	// This will cause vm_soft_fault() to handle the fault
@@ -177,7 +177,7 @@
 	store-&gt;vm.cache = NULL;
 	store-&gt;vm.committed_size = 0;
 	store-&gt;can_overcommit = canOvercommit;
-	store-&gt;has_precommitted = numPrecommittedPages != 0;
+	store-&gt;has_precommitted = false;
 	store-&gt;precommitted_pages = min_c(numPrecommittedPages, 255);
 	store-&gt;guarded_size = numGuardPages * B_PAGE_SIZE;
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="007375.html">[Haiku-commits] r24741 - in haiku/trunk/src/servers/app: . drawing
</A></li>
	<LI>Next message: <A HREF="007368.html">[Haiku-commits] r24742 - haiku/trunk/src/system/kernel/vm
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#7367">[ date ]</a>
              <a href="thread.html#7367">[ thread ]</a>
              <a href="subject.html#7367">[ subject ]</a>
              <a href="author.html#7367">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/haiku-commits">More information about the Haiku-commits
mailing list</a><br>
</body></html>
