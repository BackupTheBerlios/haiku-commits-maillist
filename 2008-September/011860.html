<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Haiku-commits] r27550 - in	haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec: . i386
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/haiku-commits/2008-September/index.html" >
   <LINK REL="made" HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r27550%20-%20in%0A%09haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec%3A%20.%20i386&In-Reply-To=%3C200809151405.m8FE5Gl8000387%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="011898.html">
   <LINK REL="Next"  HREF="011861.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Haiku-commits] r27550 - in	haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec: . i386</H1>
    <B>dlmcpaul at BerliOS</B> 
    <A HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r27550%20-%20in%0A%09haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec%3A%20.%20i386&In-Reply-To=%3C200809151405.m8FE5Gl8000387%40sheep.berlios.de%3E"
       TITLE="[Haiku-commits] r27550 - in	haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec: . i386">dlmcpaul at mail.berlios.de
       </A><BR>
    <I>Mon Sep 15 16:05:16 CEST 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="011898.html">[Haiku-commits] r27549 -	haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec
</A></li>
        <LI>Next message: <A HREF="011861.html">[Haiku-commits] r27551 -	haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#11860">[ date ]</a>
              <a href="thread.html#11860">[ thread ]</a>
              <a href="subject.html#11860">[ subject ]</a>
              <a href="author.html#11860">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: dlmcpaul
Date: 2008-09-15 16:04:59 +0200 (Mon, 15 Sep 2008)
New Revision: 27550
ViewCVS: <A HREF="http://svn.berlios.de/viewcvs/haiku?rev=27550&amp;view=rev">http://svn.berlios.de/viewcvs/haiku?rev=27550&amp;view=rev</A>

Added:
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/cavsdsp_mmx.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/cpuid.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/dsputil_h264_template_mmx.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/dsputil_h264_template_ssse3.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/dsputil_mmx.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/dsputil_mmx.h
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/dsputil_mmx_avg.h
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/dsputil_mmx_qns.h
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/dsputil_mmx_rnd.h
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/dsputil_yasm.asm
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/dsputilenc_mmx.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/fdct_mmx.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/fft_3dn.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/fft_3dn2.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/fft_mmx.asm
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/fft_sse.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/flacdsp_mmx.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/h264_i386.h
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/h264dsp_mmx.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/idct_mmx.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/idct_mmx_xvid.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/idct_sse2_xvid.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/idct_xvid.h
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/mathops.h
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/mmx.h
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/motion_est_mmx.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/mpegvideo_mmx.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/mpegvideo_mmx_template.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/simple_idct_mmx.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/snowdsp_mmx.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/vc1dsp_mmx.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/vp3dsp_mmx.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/vp3dsp_mmx.h
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/vp3dsp_sse2.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/vp3dsp_sse2.h
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/x86inc.asm
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/imc.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/imcdata.h
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/imgconvert.h
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/imx_dump_header_bsf.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/indeo2.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/indeo2data.h
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/intrax8.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/intrax8.h
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/intrax8dsp.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/intrax8huf.h
Modified:
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/idcinvideo.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/imgconvert.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/imgconvert_template.h
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/imgresample.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/indeo3.c
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/indeo3data.h
   haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/interplayvideo.c
Log:
Update avcodec to 20080825

Added: haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/cavsdsp_mmx.c
===================================================================
--- haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/cavsdsp_mmx.c	2008-09-15 14:04:37 UTC (rev 27549)
+++ haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/cavsdsp_mmx.c	2008-09-15 14:04:59 UTC (rev 27550)
@@ -0,0 +1,497 @@
+/*
+ * Chinese AVS video (AVS1-P2, JiZhun profile) decoder.
+ * Copyright (c) 2006  Stefan Gehrer &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">stefan.gehrer at gmx.de</A>&gt;
+ *
+ * MMX-optimized DSP functions, based on H.264 optimizations by
+ * Michael Niedermayer and Loren Merritt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include &quot;libavutil/common.h&quot;
+#include &quot;libavutil/x86_cpu.h&quot;
+#include &quot;libavcodec/dsputil.h&quot;
+#include &quot;dsputil_mmx.h&quot;
+
+/*****************************************************************************
+ *
+ * inverse transform
+ *
+ ****************************************************************************/
+
+static inline void cavs_idct8_1d(int16_t *block, uint64_t bias)
+{
+    asm volatile(
+        &quot;movq 112(%0), %%mm4  \n\t&quot; /* mm4 = src7 */
+        &quot;movq  16(%0), %%mm5  \n\t&quot; /* mm5 = src1 */
+        &quot;movq  80(%0), %%mm2  \n\t&quot; /* mm2 = src5 */
+        &quot;movq  48(%0), %%mm7  \n\t&quot; /* mm7 = src3 */
+        &quot;movq   %%mm4, %%mm0  \n\t&quot;
+        &quot;movq   %%mm5, %%mm3  \n\t&quot;
+        &quot;movq   %%mm2, %%mm6  \n\t&quot;
+        &quot;movq   %%mm7, %%mm1  \n\t&quot;
+
+        &quot;paddw  %%mm4, %%mm4  \n\t&quot; /* mm4 = 2*src7 */
+        &quot;paddw  %%mm3, %%mm3  \n\t&quot; /* mm3 = 2*src1 */
+        &quot;paddw  %%mm6, %%mm6  \n\t&quot; /* mm6 = 2*src5 */
+        &quot;paddw  %%mm1, %%mm1  \n\t&quot; /* mm1 = 2*src3 */
+        &quot;paddw  %%mm4, %%mm0  \n\t&quot; /* mm0 = 3*src7 */
+        &quot;paddw  %%mm3, %%mm5  \n\t&quot; /* mm5 = 3*src1 */
+        &quot;paddw  %%mm6, %%mm2  \n\t&quot; /* mm2 = 3*src5 */
+        &quot;paddw  %%mm1, %%mm7  \n\t&quot; /* mm7 = 3*src3 */
+        &quot;psubw  %%mm4, %%mm5  \n\t&quot; /* mm5 = 3*src1 - 2*src7 = a0 */
+        &quot;paddw  %%mm6, %%mm7  \n\t&quot; /* mm7 = 3*src3 + 2*src5 = a1 */
+        &quot;psubw  %%mm2, %%mm1  \n\t&quot; /* mm1 = 2*src3 - 3*src5 = a2 */
+        &quot;paddw  %%mm0, %%mm3  \n\t&quot; /* mm3 = 2*src1 + 3*src7 = a3 */
+
+        &quot;movq   %%mm5, %%mm4  \n\t&quot;
+        &quot;movq   %%mm7, %%mm6  \n\t&quot;
+        &quot;movq   %%mm3, %%mm0  \n\t&quot;
+        &quot;movq   %%mm1, %%mm2  \n\t&quot;
+        SUMSUB_BA( %%mm7, %%mm5 )   /* mm7 = a0 + a1  mm5 = a0 - a1 */
+        &quot;paddw  %%mm3, %%mm7  \n\t&quot; /* mm7 = a0 + a1 + a3 */
+        &quot;paddw  %%mm1, %%mm5  \n\t&quot; /* mm5 = a0 - a1 + a2 */
+        &quot;paddw  %%mm7, %%mm7  \n\t&quot;
+        &quot;paddw  %%mm5, %%mm5  \n\t&quot;
+        &quot;paddw  %%mm6, %%mm7  \n\t&quot; /* mm7 = b4 */
+        &quot;paddw  %%mm4, %%mm5  \n\t&quot; /* mm5 = b5 */
+
+        SUMSUB_BA( %%mm1, %%mm3 )   /* mm1 = a3 + a2  mm3 = a3 - a2 */
+        &quot;psubw  %%mm1, %%mm4  \n\t&quot; /* mm4 = a0 - a2 - a3 */
+        &quot;movq   %%mm4, %%mm1  \n\t&quot; /* mm1 = a0 - a2 - a3 */
+        &quot;psubw  %%mm6, %%mm3  \n\t&quot; /* mm3 = a3 - a2 - a1 */
+        &quot;paddw  %%mm1, %%mm1  \n\t&quot;
+        &quot;paddw  %%mm3, %%mm3  \n\t&quot;
+        &quot;psubw  %%mm2, %%mm1  \n\t&quot; /* mm1 = b7 */
+        &quot;paddw  %%mm0, %%mm3  \n\t&quot; /* mm3 = b6 */
+
+        &quot;movq  32(%0), %%mm2  \n\t&quot; /* mm2 = src2 */
+        &quot;movq  96(%0), %%mm6  \n\t&quot; /* mm6 = src6 */
+        &quot;movq   %%mm2, %%mm4  \n\t&quot;
+        &quot;movq   %%mm6, %%mm0  \n\t&quot;
+        &quot;psllw  $2,    %%mm4  \n\t&quot; /* mm4 = 4*src2 */
+        &quot;psllw  $2,    %%mm6  \n\t&quot; /* mm6 = 4*src6 */
+        &quot;paddw  %%mm4, %%mm2  \n\t&quot; /* mm2 = 5*src2 */
+        &quot;paddw  %%mm6, %%mm0  \n\t&quot; /* mm0 = 5*src6 */
+        &quot;paddw  %%mm2, %%mm2  \n\t&quot;
+        &quot;paddw  %%mm0, %%mm0  \n\t&quot;
+        &quot;psubw  %%mm0, %%mm4  \n\t&quot; /* mm4 = 4*src2 - 10*src6 = a7 */
+        &quot;paddw  %%mm2, %%mm6  \n\t&quot; /* mm6 = 4*src6 + 10*src2 = a6 */
+
+        &quot;movq    (%0), %%mm2  \n\t&quot; /* mm2 = src0 */
+        &quot;movq  64(%0), %%mm0  \n\t&quot; /* mm0 = src4 */
+        SUMSUB_BA( %%mm0, %%mm2 )   /* mm0 = src0+src4  mm2 = src0-src4 */
+        &quot;psllw  $3,    %%mm0  \n\t&quot;
+        &quot;psllw  $3,    %%mm2  \n\t&quot;
+        &quot;paddw  %1,    %%mm0  \n\t&quot; /* add rounding bias */
+        &quot;paddw  %1,    %%mm2  \n\t&quot; /* add rounding bias */
+
+        SUMSUB_BA( %%mm6, %%mm0 )   /* mm6 = a4 + a6  mm0 = a4 - a6 */
+        SUMSUB_BA( %%mm4, %%mm2 )   /* mm4 = a5 + a7  mm2 = a5 - a7 */
+        SUMSUB_BA( %%mm7, %%mm6 )   /* mm7 = dst0  mm6 = dst7 */
+        SUMSUB_BA( %%mm5, %%mm4 )   /* mm5 = dst1  mm4 = dst6 */
+        SUMSUB_BA( %%mm3, %%mm2 )   /* mm3 = dst2  mm2 = dst5 */
+        SUMSUB_BA( %%mm1, %%mm0 )   /* mm1 = dst3  mm0 = dst4 */
+        :: &quot;r&quot;(block), &quot;m&quot;(bias)
+    );
+}
+
+static void cavs_idct8_add_mmx(uint8_t *dst, int16_t *block, int stride)
+{
+    int i;
+    DECLARE_ALIGNED_8(int16_t, b2[64]);
+
+    for(i=0; i&lt;2; i++){
+        DECLARE_ALIGNED_8(uint64_t, tmp);
+
+        cavs_idct8_1d(block+4*i, ff_pw_4);
+
+        asm volatile(
+            &quot;psraw     $3, %%mm7  \n\t&quot;
+            &quot;psraw     $3, %%mm6  \n\t&quot;
+            &quot;psraw     $3, %%mm5  \n\t&quot;
+            &quot;psraw     $3, %%mm4  \n\t&quot;
+            &quot;psraw     $3, %%mm3  \n\t&quot;
+            &quot;psraw     $3, %%mm2  \n\t&quot;
+            &quot;psraw     $3, %%mm1  \n\t&quot;
+            &quot;psraw     $3, %%mm0  \n\t&quot;
+            &quot;movq   %%mm7,    %0   \n\t&quot;
+            TRANSPOSE4( %%mm0, %%mm2, %%mm4, %%mm6, %%mm7 )
+            &quot;movq   %%mm0,  8(%1)  \n\t&quot;
+            &quot;movq   %%mm6, 24(%1)  \n\t&quot;
+            &quot;movq   %%mm7, 40(%1)  \n\t&quot;
+            &quot;movq   %%mm4, 56(%1)  \n\t&quot;
+            &quot;movq    %0,    %%mm7  \n\t&quot;
+            TRANSPOSE4( %%mm7, %%mm5, %%mm3, %%mm1, %%mm0 )
+            &quot;movq   %%mm7,   (%1)  \n\t&quot;
+            &quot;movq   %%mm1, 16(%1)  \n\t&quot;
+            &quot;movq   %%mm0, 32(%1)  \n\t&quot;
+            &quot;movq   %%mm3, 48(%1)  \n\t&quot;
+            : &quot;=m&quot;(tmp)
+            : &quot;r&quot;(b2+32*i)
+            : &quot;memory&quot;
+        );
+    }
+
+    for(i=0; i&lt;2; i++){
+        cavs_idct8_1d(b2+4*i, ff_pw_64);
+
+        asm volatile(
+            &quot;psraw     $7, %%mm7  \n\t&quot;
+            &quot;psraw     $7, %%mm6  \n\t&quot;
+            &quot;psraw     $7, %%mm5  \n\t&quot;
+            &quot;psraw     $7, %%mm4  \n\t&quot;
+            &quot;psraw     $7, %%mm3  \n\t&quot;
+            &quot;psraw     $7, %%mm2  \n\t&quot;
+            &quot;psraw     $7, %%mm1  \n\t&quot;
+            &quot;psraw     $7, %%mm0  \n\t&quot;
+            &quot;movq   %%mm7,    (%0)  \n\t&quot;
+            &quot;movq   %%mm5,  16(%0)  \n\t&quot;
+            &quot;movq   %%mm3,  32(%0)  \n\t&quot;
+            &quot;movq   %%mm1,  48(%0)  \n\t&quot;
+            &quot;movq   %%mm0,  64(%0)  \n\t&quot;
+            &quot;movq   %%mm2,  80(%0)  \n\t&quot;
+            &quot;movq   %%mm4,  96(%0)  \n\t&quot;
+            &quot;movq   %%mm6, 112(%0)  \n\t&quot;
+            :: &quot;r&quot;(b2+4*i)
+            : &quot;memory&quot;
+        );
+    }
+
+    add_pixels_clamped_mmx(b2, dst, stride);
+
+    /* clear block */
+    asm volatile(
+            &quot;pxor %%mm7, %%mm7   \n\t&quot;
+            &quot;movq %%mm7, (%0)    \n\t&quot;
+            &quot;movq %%mm7, 8(%0)   \n\t&quot;
+            &quot;movq %%mm7, 16(%0)  \n\t&quot;
+            &quot;movq %%mm7, 24(%0)  \n\t&quot;
+            &quot;movq %%mm7, 32(%0)  \n\t&quot;
+            &quot;movq %%mm7, 40(%0)  \n\t&quot;
+            &quot;movq %%mm7, 48(%0)  \n\t&quot;
+            &quot;movq %%mm7, 56(%0)  \n\t&quot;
+            &quot;movq %%mm7, 64(%0)  \n\t&quot;
+            &quot;movq %%mm7, 72(%0)  \n\t&quot;
+            &quot;movq %%mm7, 80(%0)  \n\t&quot;
+            &quot;movq %%mm7, 88(%0)  \n\t&quot;
+            &quot;movq %%mm7, 96(%0)  \n\t&quot;
+            &quot;movq %%mm7, 104(%0) \n\t&quot;
+            &quot;movq %%mm7, 112(%0) \n\t&quot;
+            &quot;movq %%mm7, 120(%0) \n\t&quot;
+            :: &quot;r&quot; (block)
+    );
+}
+
+/*****************************************************************************
+ *
+ * motion compensation
+ *
+ ****************************************************************************/
+
+/* vertical filter [-1 -2 96 42 -7  0]  */
+#define QPEL_CAVSV1(A,B,C,D,E,F,OP)      \
+        &quot;movd (%0), &quot;#F&quot;            \n\t&quot;\
+        &quot;movq &quot;#C&quot;, %%mm6           \n\t&quot;\
+        &quot;pmullw %5, %%mm6           \n\t&quot;\
+        &quot;movq &quot;#D&quot;, %%mm7           \n\t&quot;\
+        &quot;pmullw %6, %%mm7           \n\t&quot;\
+        &quot;psllw $3, &quot;#E&quot;             \n\t&quot;\
+        &quot;psubw &quot;#E&quot;, %%mm6          \n\t&quot;\
+        &quot;psraw $3, &quot;#E&quot;             \n\t&quot;\
+        &quot;paddw %%mm7, %%mm6         \n\t&quot;\
+        &quot;paddw &quot;#E&quot;, %%mm6          \n\t&quot;\
+        &quot;paddw &quot;#B&quot;, &quot;#B&quot;           \n\t&quot;\
+        &quot;pxor %%mm7, %%mm7          \n\t&quot;\
+        &quot;add %2, %0                 \n\t&quot;\
+        &quot;punpcklbw %%mm7, &quot;#F&quot;      \n\t&quot;\
+        &quot;psubw &quot;#B&quot;, %%mm6          \n\t&quot;\
+        &quot;psraw $1, &quot;#B&quot;             \n\t&quot;\
+        &quot;psubw &quot;#A&quot;, %%mm6          \n\t&quot;\
+        &quot;paddw %4, %%mm6            \n\t&quot;\
+        &quot;psraw $7, %%mm6            \n\t&quot;\
+        &quot;packuswb %%mm6, %%mm6      \n\t&quot;\
+        OP(%%mm6, (%1), A, d)            \
+        &quot;add %3, %1                 \n\t&quot;
+
+/* vertical filter [ 0 -1  5  5 -1  0]  */
+#define QPEL_CAVSV2(A,B,C,D,E,F,OP)      \
+        &quot;movd (%0), &quot;#F&quot;            \n\t&quot;\
+        &quot;movq &quot;#C&quot;, %%mm6           \n\t&quot;\
+        &quot;paddw &quot;#D&quot;, %%mm6          \n\t&quot;\
+        &quot;pmullw %5, %%mm6           \n\t&quot;\
+        &quot;add %2, %0                 \n\t&quot;\
+        &quot;punpcklbw %%mm7, &quot;#F&quot;      \n\t&quot;\
+        &quot;psubw &quot;#B&quot;, %%mm6          \n\t&quot;\
+        &quot;psubw &quot;#E&quot;, %%mm6          \n\t&quot;\
+        &quot;paddw %4, %%mm6            \n\t&quot;\
+        &quot;psraw $3, %%mm6            \n\t&quot;\
+        &quot;packuswb %%mm6, %%mm6      \n\t&quot;\
+        OP(%%mm6, (%1), A, d)            \
+        &quot;add %3, %1                 \n\t&quot;
+
+/* vertical filter [ 0 -7 42 96 -2 -1]  */
+#define QPEL_CAVSV3(A,B,C,D,E,F,OP)      \
+        &quot;movd (%0), &quot;#F&quot;            \n\t&quot;\
+        &quot;movq &quot;#C&quot;, %%mm6           \n\t&quot;\
+        &quot;pmullw %6, %%mm6           \n\t&quot;\
+        &quot;movq &quot;#D&quot;, %%mm7           \n\t&quot;\
+        &quot;pmullw %5, %%mm7           \n\t&quot;\
+        &quot;psllw $3, &quot;#B&quot;             \n\t&quot;\
+        &quot;psubw &quot;#B&quot;, %%mm6          \n\t&quot;\
+        &quot;psraw $3, &quot;#B&quot;             \n\t&quot;\
+        &quot;paddw %%mm7, %%mm6         \n\t&quot;\
+        &quot;paddw &quot;#B&quot;, %%mm6          \n\t&quot;\
+        &quot;paddw &quot;#E&quot;, &quot;#E&quot;           \n\t&quot;\
+        &quot;pxor %%mm7, %%mm7          \n\t&quot;\
+        &quot;add %2, %0                 \n\t&quot;\
+        &quot;punpcklbw %%mm7, &quot;#F&quot;      \n\t&quot;\
+        &quot;psubw &quot;#E&quot;, %%mm6          \n\t&quot;\
+        &quot;psraw $1, &quot;#E&quot;             \n\t&quot;\
+        &quot;psubw &quot;#F&quot;, %%mm6          \n\t&quot;\
+        &quot;paddw %4, %%mm6            \n\t&quot;\
+        &quot;psraw $7, %%mm6            \n\t&quot;\
+        &quot;packuswb %%mm6, %%mm6      \n\t&quot;\
+        OP(%%mm6, (%1), A, d)            \
+        &quot;add %3, %1                 \n\t&quot;
+
+
+#define QPEL_CAVSVNUM(VOP,OP,ADD,MUL1,MUL2)\
+    int w= 2;\
+    src -= 2*srcStride;\
+    \
+    while(w--){\
+      asm volatile(\
+        &quot;pxor %%mm7, %%mm7          \n\t&quot;\
+        &quot;movd (%0), %%mm0           \n\t&quot;\
+        &quot;add %2, %0                 \n\t&quot;\
+        &quot;movd (%0), %%mm1           \n\t&quot;\
+        &quot;add %2, %0                 \n\t&quot;\
+        &quot;movd (%0), %%mm2           \n\t&quot;\
+        &quot;add %2, %0                 \n\t&quot;\
+        &quot;movd (%0), %%mm3           \n\t&quot;\
+        &quot;add %2, %0                 \n\t&quot;\
+        &quot;movd (%0), %%mm4           \n\t&quot;\
+        &quot;add %2, %0                 \n\t&quot;\
+        &quot;punpcklbw %%mm7, %%mm0     \n\t&quot;\
+        &quot;punpcklbw %%mm7, %%mm1     \n\t&quot;\
+        &quot;punpcklbw %%mm7, %%mm2     \n\t&quot;\
+        &quot;punpcklbw %%mm7, %%mm3     \n\t&quot;\
+        &quot;punpcklbw %%mm7, %%mm4     \n\t&quot;\
+        VOP(%%mm0, %%mm1, %%mm2, %%mm3, %%mm4, %%mm5, OP)\
+        VOP(%%mm1, %%mm2, %%mm3, %%mm4, %%mm5, %%mm0, OP)\
+        VOP(%%mm2, %%mm3, %%mm4, %%mm5, %%mm0, %%mm1, OP)\
+        VOP(%%mm3, %%mm4, %%mm5, %%mm0, %%mm1, %%mm2, OP)\
+        VOP(%%mm4, %%mm5, %%mm0, %%mm1, %%mm2, %%mm3, OP)\
+        VOP(%%mm5, %%mm0, %%mm1, %%mm2, %%mm3, %%mm4, OP)\
+        VOP(%%mm0, %%mm1, %%mm2, %%mm3, %%mm4, %%mm5, OP)\
+        VOP(%%mm1, %%mm2, %%mm3, %%mm4, %%mm5, %%mm0, OP)\
+        \
+        : &quot;+a&quot;(src), &quot;+c&quot;(dst)\
+        : &quot;S&quot;((x86_reg)srcStride), &quot;D&quot;((x86_reg)dstStride), &quot;m&quot;(ADD), &quot;m&quot;(MUL1), &quot;m&quot;(MUL2)\
+        : &quot;memory&quot;\
+     );\
+     if(h==16){\
+        asm volatile(\
+            VOP(%%mm2, %%mm3, %%mm4, %%mm5, %%mm0, %%mm1, OP)\
+            VOP(%%mm3, %%mm4, %%mm5, %%mm0, %%mm1, %%mm2, OP)\
+            VOP(%%mm4, %%mm5, %%mm0, %%mm1, %%mm2, %%mm3, OP)\
+            VOP(%%mm5, %%mm0, %%mm1, %%mm2, %%mm3, %%mm4, OP)\
+            VOP(%%mm0, %%mm1, %%mm2, %%mm3, %%mm4, %%mm5, OP)\
+            VOP(%%mm1, %%mm2, %%mm3, %%mm4, %%mm5, %%mm0, OP)\
+            VOP(%%mm2, %%mm3, %%mm4, %%mm5, %%mm0, %%mm1, OP)\
+            VOP(%%mm3, %%mm4, %%mm5, %%mm0, %%mm1, %%mm2, OP)\
+            \
+           : &quot;+a&quot;(src), &quot;+c&quot;(dst)\
+           : &quot;S&quot;((x86_reg)srcStride), &quot;D&quot;((x86_reg)dstStride), &quot;m&quot;(ADD),  &quot;m&quot;(MUL1), &quot;m&quot;(MUL2)\
+           : &quot;memory&quot;\
+        );\
+     }\
+     src += 4-(h+5)*srcStride;\
+     dst += 4-h*dstStride;\
+   }
+
+#define QPEL_CAVS(OPNAME, OP, MMX)\
+static void OPNAME ## cavs_qpel8_h_ ## MMX(uint8_t *dst, uint8_t *src, int dstStride, int srcStride){\
+    int h=8;\
+    asm volatile(\
+        &quot;pxor %%mm7, %%mm7          \n\t&quot;\
+        &quot;movq %5, %%mm6             \n\t&quot;\
+        &quot;1:                         \n\t&quot;\
+        &quot;movq    (%0), %%mm0        \n\t&quot;\
+        &quot;movq   1(%0), %%mm2        \n\t&quot;\
+        &quot;movq %%mm0, %%mm1          \n\t&quot;\
+        &quot;movq %%mm2, %%mm3          \n\t&quot;\
+        &quot;punpcklbw %%mm7, %%mm0     \n\t&quot;\
+        &quot;punpckhbw %%mm7, %%mm1     \n\t&quot;\
+        &quot;punpcklbw %%mm7, %%mm2     \n\t&quot;\
+        &quot;punpckhbw %%mm7, %%mm3     \n\t&quot;\
+        &quot;paddw %%mm2, %%mm0         \n\t&quot;\
+        &quot;paddw %%mm3, %%mm1         \n\t&quot;\
+        &quot;pmullw %%mm6, %%mm0        \n\t&quot;\
+        &quot;pmullw %%mm6, %%mm1        \n\t&quot;\
+        &quot;movq   -1(%0), %%mm2       \n\t&quot;\
+        &quot;movq    2(%0), %%mm4       \n\t&quot;\
+        &quot;movq %%mm2, %%mm3          \n\t&quot;\
+        &quot;movq %%mm4, %%mm5          \n\t&quot;\
+        &quot;punpcklbw %%mm7, %%mm2     \n\t&quot;\
+        &quot;punpckhbw %%mm7, %%mm3     \n\t&quot;\
+        &quot;punpcklbw %%mm7, %%mm4     \n\t&quot;\
+        &quot;punpckhbw %%mm7, %%mm5     \n\t&quot;\
+        &quot;paddw %%mm4, %%mm2         \n\t&quot;\
+        &quot;paddw %%mm3, %%mm5         \n\t&quot;\
+        &quot;psubw %%mm2, %%mm0         \n\t&quot;\
+        &quot;psubw %%mm5, %%mm1         \n\t&quot;\
+        &quot;movq %6, %%mm5             \n\t&quot;\
+        &quot;paddw %%mm5, %%mm0         \n\t&quot;\
+        &quot;paddw %%mm5, %%mm1         \n\t&quot;\
+        &quot;psraw $3, %%mm0            \n\t&quot;\
+        &quot;psraw $3, %%mm1            \n\t&quot;\
+        &quot;packuswb %%mm1, %%mm0      \n\t&quot;\
+        OP(%%mm0, (%1),%%mm5, q)         \
+        &quot;add %3, %0                 \n\t&quot;\
+        &quot;add %4, %1                 \n\t&quot;\
+        &quot;decl %2                    \n\t&quot;\
+        &quot; jnz 1b                    \n\t&quot;\
+        : &quot;+a&quot;(src), &quot;+c&quot;(dst), &quot;+m&quot;(h)\
+        : &quot;d&quot;((x86_reg)srcStride), &quot;S&quot;((x86_reg)dstStride), &quot;m&quot;(ff_pw_5), &quot;m&quot;(ff_pw_4)\
+        : &quot;memory&quot;\
+    );\
+}\
+\
+static inline void OPNAME ## cavs_qpel8or16_v1_ ## MMX(uint8_t *dst, uint8_t *src, int dstStride, int srcStride, int h){\
+  QPEL_CAVSVNUM(QPEL_CAVSV1,OP,ff_pw_64,ff_pw_96,ff_pw_42)      \
+}\
+\
+static inline void OPNAME ## cavs_qpel8or16_v2_ ## MMX(uint8_t *dst, uint8_t *src, int dstStride, int srcStride, int h){\
+  QPEL_CAVSVNUM(QPEL_CAVSV2,OP,ff_pw_4,ff_pw_5,ff_pw_5)         \
+}\
+\
+static inline void OPNAME ## cavs_qpel8or16_v3_ ## MMX(uint8_t *dst, uint8_t *src, int dstStride, int srcStride, int h){\
+  QPEL_CAVSVNUM(QPEL_CAVSV3,OP,ff_pw_64,ff_pw_96,ff_pw_42)      \
+}\
+\
+static void OPNAME ## cavs_qpel8_v1_ ## MMX(uint8_t *dst, uint8_t *src, int dstStride, int srcStride){\
+    OPNAME ## cavs_qpel8or16_v1_ ## MMX(dst  , src  , dstStride, srcStride, 8);\
+}\
+static void OPNAME ## cavs_qpel16_v1_ ## MMX(uint8_t *dst, uint8_t *src, int dstStride, int srcStride){\
+    OPNAME ## cavs_qpel8or16_v1_ ## MMX(dst  , src  , dstStride, srcStride, 16);\
+    OPNAME ## cavs_qpel8or16_v1_ ## MMX(dst+8, src+8, dstStride, srcStride, 16);\
+}\
+\
+static void OPNAME ## cavs_qpel8_v2_ ## MMX(uint8_t *dst, uint8_t *src, int dstStride, int srcStride){\
+    OPNAME ## cavs_qpel8or16_v2_ ## MMX(dst  , src  , dstStride, srcStride, 8);\
+}\
+static void OPNAME ## cavs_qpel16_v2_ ## MMX(uint8_t *dst, uint8_t *src, int dstStride, int srcStride){\
+    OPNAME ## cavs_qpel8or16_v2_ ## MMX(dst  , src  , dstStride, srcStride, 16);\
+    OPNAME ## cavs_qpel8or16_v2_ ## MMX(dst+8, src+8, dstStride, srcStride, 16);\
+}\
+\
+static void OPNAME ## cavs_qpel8_v3_ ## MMX(uint8_t *dst, uint8_t *src, int dstStride, int srcStride){\
+    OPNAME ## cavs_qpel8or16_v3_ ## MMX(dst  , src  , dstStride, srcStride, 8);\
+}\
+static void OPNAME ## cavs_qpel16_v3_ ## MMX(uint8_t *dst, uint8_t *src, int dstStride, int srcStride){\
+    OPNAME ## cavs_qpel8or16_v3_ ## MMX(dst  , src  , dstStride, srcStride, 16);\
+    OPNAME ## cavs_qpel8or16_v3_ ## MMX(dst+8, src+8, dstStride, srcStride, 16);\
+}\
+\
+static void OPNAME ## cavs_qpel16_h_ ## MMX(uint8_t *dst, uint8_t *src, int dstStride, int srcStride){\
+    OPNAME ## cavs_qpel8_h_ ## MMX(dst  , src  , dstStride, srcStride);\
+    OPNAME ## cavs_qpel8_h_ ## MMX(dst+8, src+8, dstStride, srcStride);\
+    src += 8*srcStride;\
+    dst += 8*dstStride;\
+    OPNAME ## cavs_qpel8_h_ ## MMX(dst  , src  , dstStride, srcStride);\
+    OPNAME ## cavs_qpel8_h_ ## MMX(dst+8, src+8, dstStride, srcStride);\
+}\
+
+#define CAVS_MC(OPNAME, SIZE, MMX) \
+static void ff_ ## OPNAME ## cavs_qpel ## SIZE ## _mc20_ ## MMX(uint8_t *dst, uint8_t *src, int stride){\
+    OPNAME ## cavs_qpel ## SIZE ## _h_ ## MMX(dst, src, stride, stride);\
+}\
+\
+static void ff_ ## OPNAME ## cavs_qpel ## SIZE ## _mc01_ ## MMX(uint8_t *dst, uint8_t *src, int stride){\
+    OPNAME ## cavs_qpel ## SIZE ## _v1_ ## MMX(dst, src, stride, stride);\
+}\
+\
+static void ff_ ## OPNAME ## cavs_qpel ## SIZE ## _mc02_ ## MMX(uint8_t *dst, uint8_t *src, int stride){\
+    OPNAME ## cavs_qpel ## SIZE ## _v2_ ## MMX(dst, src, stride, stride);\
+}\
+\
+static void ff_ ## OPNAME ## cavs_qpel ## SIZE ## _mc03_ ## MMX(uint8_t *dst, uint8_t *src, int stride){\
+    OPNAME ## cavs_qpel ## SIZE ## _v3_ ## MMX(dst, src, stride, stride);\
+}\
+
+#define PUT_OP(a,b,temp, size) &quot;mov&quot; #size &quot; &quot; #a &quot;, &quot; #b &quot;    \n\t&quot;
+#define AVG_3DNOW_OP(a,b,temp, size) \
+&quot;mov&quot; #size &quot; &quot; #b &quot;, &quot; #temp &quot;   \n\t&quot;\
+&quot;pavgusb &quot; #temp &quot;, &quot; #a &quot;        \n\t&quot;\
+&quot;mov&quot; #size &quot; &quot; #a &quot;, &quot; #b &quot;      \n\t&quot;
+#define AVG_MMX2_OP(a,b,temp, size) \
+&quot;mov&quot; #size &quot; &quot; #b &quot;, &quot; #temp &quot;   \n\t&quot;\
+&quot;pavgb &quot; #temp &quot;, &quot; #a &quot;          \n\t&quot;\
+&quot;mov&quot; #size &quot; &quot; #a &quot;, &quot; #b &quot;      \n\t&quot;
+
+QPEL_CAVS(put_,       PUT_OP, 3dnow)
+QPEL_CAVS(avg_, AVG_3DNOW_OP, 3dnow)
+QPEL_CAVS(put_,       PUT_OP, mmx2)
+QPEL_CAVS(avg_,  AVG_MMX2_OP, mmx2)
+
+CAVS_MC(put_, 8, 3dnow)
+CAVS_MC(put_, 16,3dnow)
+CAVS_MC(avg_, 8, 3dnow)
+CAVS_MC(avg_, 16,3dnow)
+CAVS_MC(put_, 8, mmx2)
+CAVS_MC(put_, 16,mmx2)
+CAVS_MC(avg_, 8, mmx2)
+CAVS_MC(avg_, 16,mmx2)
+
+void ff_put_cavs_qpel8_mc00_mmx2(uint8_t *dst, uint8_t *src, int stride);
+void ff_avg_cavs_qpel8_mc00_mmx2(uint8_t *dst, uint8_t *src, int stride);
+void ff_put_cavs_qpel16_mc00_mmx2(uint8_t *dst, uint8_t *src, int stride);
+void ff_avg_cavs_qpel16_mc00_mmx2(uint8_t *dst, uint8_t *src, int stride);
+
+void ff_cavsdsp_init_mmx2(DSPContext* c, AVCodecContext *avctx) {
+#define dspfunc(PFX, IDX, NUM) \
+    c-&gt;PFX ## _pixels_tab[IDX][ 0] = ff_ ## PFX ## NUM ## _mc00_mmx2; \
+    c-&gt;PFX ## _pixels_tab[IDX][ 2] = ff_ ## PFX ## NUM ## _mc20_mmx2; \
+    c-&gt;PFX ## _pixels_tab[IDX][ 4] = ff_ ## PFX ## NUM ## _mc01_mmx2; \
+    c-&gt;PFX ## _pixels_tab[IDX][ 8] = ff_ ## PFX ## NUM ## _mc02_mmx2; \
+    c-&gt;PFX ## _pixels_tab[IDX][12] = ff_ ## PFX ## NUM ## _mc03_mmx2; \
+
+    dspfunc(put_cavs_qpel, 0, 16);
+    dspfunc(put_cavs_qpel, 1, 8);
+    dspfunc(avg_cavs_qpel, 0, 16);
+    dspfunc(avg_cavs_qpel, 1, 8);
+#undef dspfunc
+    c-&gt;cavs_idct8_add = cavs_idct8_add_mmx;
+}
+
+void ff_cavsdsp_init_3dnow(DSPContext* c, AVCodecContext *avctx) {
+#define dspfunc(PFX, IDX, NUM) \
+    c-&gt;PFX ## _pixels_tab[IDX][ 0] = ff_ ## PFX ## NUM ## _mc00_mmx2; \
+    c-&gt;PFX ## _pixels_tab[IDX][ 2] = ff_ ## PFX ## NUM ## _mc20_3dnow; \
+    c-&gt;PFX ## _pixels_tab[IDX][ 4] = ff_ ## PFX ## NUM ## _mc01_3dnow; \
+    c-&gt;PFX ## _pixels_tab[IDX][ 8] = ff_ ## PFX ## NUM ## _mc02_3dnow; \
+    c-&gt;PFX ## _pixels_tab[IDX][12] = ff_ ## PFX ## NUM ## _mc03_3dnow; \
+
+    dspfunc(put_cavs_qpel, 0, 16);
+    dspfunc(put_cavs_qpel, 1, 8);
+    dspfunc(avg_cavs_qpel, 0, 16);
+    dspfunc(avg_cavs_qpel, 1, 8);
+#undef dspfunc
+    c-&gt;cavs_idct8_add = cavs_idct8_add_mmx;
+}

Added: haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/cpuid.c
===================================================================
--- haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/cpuid.c	2008-09-15 14:04:37 UTC (rev 27549)
+++ haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/cpuid.c	2008-09-15 14:04:59 UTC (rev 27550)
@@ -0,0 +1,127 @@
+/*
+ * CPU detection code, extracted from mmx.h
+ * (c)1997-99 by H. Dietz and R. Fisher
+ * Converted to C and improved by Fabrice Bellard.
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include &lt;stdlib.h&gt;
+#include &quot;libavutil/x86_cpu.h&quot;
+#include &quot;libavcodec/dsputil.h&quot;
+
+#undef printf
+
+/* ebx saving is necessary for PIC. gcc seems unable to see it alone */
+#define cpuid(index,eax,ebx,ecx,edx)\
+    asm volatile\
+        (&quot;mov %%&quot;REG_b&quot;, %%&quot;REG_S&quot;\n\t&quot;\
+         &quot;cpuid\n\t&quot;\
+         &quot;xchg %%&quot;REG_b&quot;, %%&quot;REG_S\
+         : &quot;=a&quot; (eax), &quot;=S&quot; (ebx),\
+           &quot;=c&quot; (ecx), &quot;=d&quot; (edx)\
+         : &quot;0&quot; (index));
+
+/* Function to test if multimedia instructions are supported...  */
+int mm_support(void)
+{
+    int rval = 0;
+    int eax, ebx, ecx, edx;
+    int max_std_level, max_ext_level, std_caps=0, ext_caps=0;
+    x86_reg a, c;
+
+    asm volatile (
+        /* See if CPUID instruction is supported ... */
+        /* ... Get copies of EFLAGS into eax and ecx */
+        &quot;pushf\n\t&quot;
+        &quot;pop %0\n\t&quot;
+        &quot;mov %0, %1\n\t&quot;
+
+        /* ... Toggle the ID bit in one copy and store */
+        /*     to the EFLAGS reg */
+        &quot;xor $0x200000, %0\n\t&quot;
+        &quot;push %0\n\t&quot;
+        &quot;popf\n\t&quot;
+
+        /* ... Get the (hopefully modified) EFLAGS */
+        &quot;pushf\n\t&quot;
+        &quot;pop %0\n\t&quot;
+        : &quot;=a&quot; (a), &quot;=c&quot; (c)
+        :
+        : &quot;cc&quot;
+        );
+
+    if (a == c)
+        return 0; /* CPUID not supported */
+
+    cpuid(0, max_std_level, ebx, ecx, edx);
+
+    if(max_std_level &gt;= 1){
+        cpuid(1, eax, ebx, ecx, std_caps);
+        if (std_caps &amp; (1&lt;&lt;23))
+            rval |= FF_MM_MMX;
+        if (std_caps &amp; (1&lt;&lt;25))
+            rval |= FF_MM_MMXEXT
+#if !defined(__GNUC__) || __GNUC__ &gt; 2
+                  | FF_MM_SSE;
+        if (std_caps &amp; (1&lt;&lt;26))
+            rval |= FF_MM_SSE2;
+        if (ecx &amp; 1)
+            rval |= FF_MM_SSE3;
+        if (ecx &amp; 0x00000200 )
+            rval |= FF_MM_SSSE3
+#endif
+                  ;
+    }
+
+    cpuid(0x80000000, max_ext_level, ebx, ecx, edx);
+
+    if(max_ext_level &gt;= 0x80000001){
+        cpuid(0x80000001, eax, ebx, ecx, ext_caps);
+        if (ext_caps &amp; (1&lt;&lt;31))
+            rval |= FF_MM_3DNOW;
+        if (ext_caps &amp; (1&lt;&lt;30))
+            rval |= FF_MM_3DNOWEXT;
+        if (ext_caps &amp; (1&lt;&lt;23))
+            rval |= FF_MM_MMX;
+        if (ext_caps &amp; (1&lt;&lt;22))
+            rval |= FF_MM_MMXEXT;
+    }
+
+#if 0
+    av_log(NULL, AV_LOG_DEBUG, &quot;%s%s%s%s%s%s%s%s\n&quot;,
+        (rval&amp;FF_MM_MMX) ? &quot;MMX &quot;:&quot;&quot;,
+        (rval&amp;FF_MM_MMXEXT) ? &quot;MMX2 &quot;:&quot;&quot;,
+        (rval&amp;FF_MM_SSE) ? &quot;SSE &quot;:&quot;&quot;,
+        (rval&amp;FF_MM_SSE2) ? &quot;SSE2 &quot;:&quot;&quot;,
+        (rval&amp;FF_MM_SSE3) ? &quot;SSE3 &quot;:&quot;&quot;,
+        (rval&amp;FF_MM_SSSE3) ? &quot;SSSE3 &quot;:&quot;&quot;,
+        (rval&amp;FF_MM_3DNOW) ? &quot;3DNow &quot;:&quot;&quot;,
+        (rval&amp;FF_MM_3DNOWEXT) ? &quot;3DNowExt &quot;:&quot;&quot;);
+#endif
+    return rval;
+}
+
+#ifdef TEST
+int main ( void )
+{
+    int mm_flags;
+    mm_flags = mm_support();
+    printf(&quot;mm_support = 0x%08X\n&quot;,mm_flags);
+    return 0;
+}
+#endif

Added: haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/dsputil_h264_template_mmx.c
===================================================================
--- haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/dsputil_h264_template_mmx.c	2008-09-15 14:04:37 UTC (rev 27549)
+++ haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/dsputil_h264_template_mmx.c	2008-09-15 14:04:59 UTC (rev 27550)
@@ -0,0 +1,308 @@
+/*
+ * Copyright (c) 2005 Zoltan Hidvegi &lt;hzoli -a- hzoli -d- com&gt;,
+ *                    Loren Merritt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * MMX optimized version of (put|avg)_h264_chroma_mc8.
+ * H264_CHROMA_MC8_TMPL must be defined to the desired function name
+ * H264_CHROMA_OP must be defined to empty for put and pavgb/pavgusb for avg
+ * H264_CHROMA_MC8_MV0 must be defined to a (put|avg)_pixels8 function
+ */
+static void H264_CHROMA_MC8_TMPL(uint8_t *dst/*align 8*/, uint8_t *src/*align 1*/, int stride, int h, int x, int y, int rnd)
+{
+    const uint64_t *rnd_reg;
+    DECLARE_ALIGNED_8(uint64_t, AA);
+    DECLARE_ALIGNED_8(uint64_t, DD);
+    int i;
+
+    if(y==0 &amp;&amp; x==0) {
+        /* no filter needed */
+        H264_CHROMA_MC8_MV0(dst, src, stride, h);
+        return;
+    }
+
+    assert(x&lt;8 &amp;&amp; y&lt;8 &amp;&amp; x&gt;=0 &amp;&amp; y&gt;=0);
+
+    if(y==0 || x==0)
+    {
+        /* 1 dimensional filter only */
+        const int dxy = x ? 1 : stride;
+
+        rnd_reg = rnd ? &amp;ff_pw_4 : &amp;ff_pw_3;
+
+        asm volatile(
+            &quot;movd %0, %%mm5\n\t&quot;
+            &quot;movq %1, %%mm4\n\t&quot;
+            &quot;movq %2, %%mm6\n\t&quot;         /* mm6 = rnd */
+            &quot;punpcklwd %%mm5, %%mm5\n\t&quot;
+            &quot;punpckldq %%mm5, %%mm5\n\t&quot; /* mm5 = B = x */
+            &quot;pxor %%mm7, %%mm7\n\t&quot;
+            &quot;psubw %%mm5, %%mm4\n\t&quot;     /* mm4 = A = 8-x */
+            :: &quot;rm&quot;(x+y), &quot;m&quot;(ff_pw_8), &quot;m&quot;(*rnd_reg));
+
+        for(i=0; i&lt;h; i++) {
+            asm volatile(
+                /* mm0 = src[0..7], mm1 = src[1..8] */
+                &quot;movq %0, %%mm0\n\t&quot;
+                &quot;movq %1, %%mm2\n\t&quot;
+                :: &quot;m&quot;(src[0]), &quot;m&quot;(src[dxy]));
+
+            asm volatile(
+                /* [mm0,mm1] = A * src[0..7] */
+                /* [mm2,mm3] = B * src[1..8] */
+                &quot;movq %%mm0, %%mm1\n\t&quot;
+                &quot;movq %%mm2, %%mm3\n\t&quot;
+                &quot;punpcklbw %%mm7, %%mm0\n\t&quot;
+                &quot;punpckhbw %%mm7, %%mm1\n\t&quot;
+                &quot;punpcklbw %%mm7, %%mm2\n\t&quot;
+                &quot;punpckhbw %%mm7, %%mm3\n\t&quot;
+                &quot;pmullw %%mm4, %%mm0\n\t&quot;
+                &quot;pmullw %%mm4, %%mm1\n\t&quot;
+                &quot;pmullw %%mm5, %%mm2\n\t&quot;
+                &quot;pmullw %%mm5, %%mm3\n\t&quot;
+
+                /* dst[0..7] = (A * src[0..7] + B * src[1..8] + 4) &gt;&gt; 3 */
+                &quot;paddw %%mm6, %%mm0\n\t&quot;
+                &quot;paddw %%mm6, %%mm1\n\t&quot;
+                &quot;paddw %%mm2, %%mm0\n\t&quot;
+                &quot;paddw %%mm3, %%mm1\n\t&quot;
+                &quot;psrlw $3, %%mm0\n\t&quot;
+                &quot;psrlw $3, %%mm1\n\t&quot;
+                &quot;packuswb %%mm1, %%mm0\n\t&quot;
+                H264_CHROMA_OP(%0, %%mm0)
+                &quot;movq %%mm0, %0\n\t&quot;
+                : &quot;=m&quot; (dst[0]));
+
+            src += stride;
+            dst += stride;
+        }
+        return;
+    }
+
+    /* general case, bilinear */
+    rnd_reg = rnd ? &amp;ff_pw_32.a : &amp;ff_pw_28.a;
+    asm volatile(&quot;movd %2, %%mm4\n\t&quot;
+                 &quot;movd %3, %%mm6\n\t&quot;
+                 &quot;punpcklwd %%mm4, %%mm4\n\t&quot;
+                 &quot;punpcklwd %%mm6, %%mm6\n\t&quot;
+                 &quot;punpckldq %%mm4, %%mm4\n\t&quot; /* mm4 = x words */
+                 &quot;punpckldq %%mm6, %%mm6\n\t&quot; /* mm6 = y words */
+                 &quot;movq %%mm4, %%mm5\n\t&quot;
+                 &quot;pmullw %%mm6, %%mm4\n\t&quot;    /* mm4 = x * y */
+                 &quot;psllw $3, %%mm5\n\t&quot;
+                 &quot;psllw $3, %%mm6\n\t&quot;
+                 &quot;movq %%mm5, %%mm7\n\t&quot;
+                 &quot;paddw %%mm6, %%mm7\n\t&quot;
+                 &quot;movq %%mm4, %1\n\t&quot;         /* DD = x * y */
+                 &quot;psubw %%mm4, %%mm5\n\t&quot;     /* mm5 = B = 8x - xy */
+                 &quot;psubw %%mm4, %%mm6\n\t&quot;     /* mm6 = C = 8y - xy */
+                 &quot;paddw %4, %%mm4\n\t&quot;
+                 &quot;psubw %%mm7, %%mm4\n\t&quot;     /* mm4 = A = xy - (8x+8y) + 64 */
+                 &quot;pxor %%mm7, %%mm7\n\t&quot;
+                 &quot;movq %%mm4, %0\n\t&quot;
+                 : &quot;=m&quot; (AA), &quot;=m&quot; (DD) : &quot;rm&quot; (x), &quot;rm&quot; (y), &quot;m&quot; (ff_pw_64));
+
+    asm volatile(
+        /* mm0 = src[0..7], mm1 = src[1..8] */
+        &quot;movq %0, %%mm0\n\t&quot;
+        &quot;movq %1, %%mm1\n\t&quot;
+        : : &quot;m&quot; (src[0]), &quot;m&quot; (src[1]));
+
+    for(i=0; i&lt;h; i++) {
+        src += stride;
+
+        asm volatile(
+            /* mm2 = A * src[0..3] + B * src[1..4] */
+            /* mm3 = A * src[4..7] + B * src[5..8] */
+            &quot;movq %%mm0, %%mm2\n\t&quot;
+            &quot;movq %%mm1, %%mm3\n\t&quot;
+            &quot;punpckhbw %%mm7, %%mm0\n\t&quot;
+            &quot;punpcklbw %%mm7, %%mm1\n\t&quot;
+            &quot;punpcklbw %%mm7, %%mm2\n\t&quot;
+            &quot;punpckhbw %%mm7, %%mm3\n\t&quot;
+            &quot;pmullw %0, %%mm0\n\t&quot;
+            &quot;pmullw %0, %%mm2\n\t&quot;
+            &quot;pmullw %%mm5, %%mm1\n\t&quot;
+            &quot;pmullw %%mm5, %%mm3\n\t&quot;
+            &quot;paddw %%mm1, %%mm2\n\t&quot;
+            &quot;paddw %%mm0, %%mm3\n\t&quot;
+            : : &quot;m&quot; (AA));
+
+        asm volatile(
+            /* [mm2,mm3] += C * src[0..7] */
+            &quot;movq %0, %%mm0\n\t&quot;
+            &quot;movq %%mm0, %%mm1\n\t&quot;
+            &quot;punpcklbw %%mm7, %%mm0\n\t&quot;
+            &quot;punpckhbw %%mm7, %%mm1\n\t&quot;
+            &quot;pmullw %%mm6, %%mm0\n\t&quot;
+            &quot;pmullw %%mm6, %%mm1\n\t&quot;
+            &quot;paddw %%mm0, %%mm2\n\t&quot;
+            &quot;paddw %%mm1, %%mm3\n\t&quot;
+            : : &quot;m&quot; (src[0]));
+
+        asm volatile(
+            /* [mm2,mm3] += D * src[1..8] */
+            &quot;movq %1, %%mm1\n\t&quot;
+            &quot;movq %%mm1, %%mm0\n\t&quot;
+            &quot;movq %%mm1, %%mm4\n\t&quot;
+            &quot;punpcklbw %%mm7, %%mm0\n\t&quot;
+            &quot;punpckhbw %%mm7, %%mm4\n\t&quot;
+            &quot;pmullw %2, %%mm0\n\t&quot;
+            &quot;pmullw %2, %%mm4\n\t&quot;
+            &quot;paddw %%mm0, %%mm2\n\t&quot;
+            &quot;paddw %%mm4, %%mm3\n\t&quot;
+            &quot;movq %0, %%mm0\n\t&quot;
+            : : &quot;m&quot; (src[0]), &quot;m&quot; (src[1]), &quot;m&quot; (DD));
+
+        asm volatile(
+            /* dst[0..7] = ([mm2,mm3] + 32) &gt;&gt; 6 */
+            &quot;paddw %1, %%mm2\n\t&quot;
+            &quot;paddw %1, %%mm3\n\t&quot;
+            &quot;psrlw $6, %%mm2\n\t&quot;
+            &quot;psrlw $6, %%mm3\n\t&quot;
+            &quot;packuswb %%mm3, %%mm2\n\t&quot;
+            H264_CHROMA_OP(%0, %%mm2)
+            &quot;movq %%mm2, %0\n\t&quot;
+            : &quot;=m&quot; (dst[0]) : &quot;m&quot; (*rnd_reg));
+        dst+= stride;
+    }
+}
+
+static void H264_CHROMA_MC4_TMPL(uint8_t *dst/*align 4*/, uint8_t *src/*align 1*/, int stride, int h, int x, int y)
+{
+    asm volatile(
+        &quot;pxor   %%mm7, %%mm7        \n\t&quot;
+        &quot;movd %5, %%mm2             \n\t&quot;
+        &quot;movd %6, %%mm3             \n\t&quot;
+        &quot;movq &quot;MANGLE(ff_pw_8)&quot;, %%mm4\n\t&quot;
+        &quot;movq &quot;MANGLE(ff_pw_8)&quot;, %%mm5\n\t&quot;
+        &quot;punpcklwd %%mm2, %%mm2     \n\t&quot;
+        &quot;punpcklwd %%mm3, %%mm3     \n\t&quot;
+        &quot;punpcklwd %%mm2, %%mm2     \n\t&quot;
+        &quot;punpcklwd %%mm3, %%mm3     \n\t&quot;
+        &quot;psubw %%mm2, %%mm4         \n\t&quot;
+        &quot;psubw %%mm3, %%mm5         \n\t&quot;
+
+        &quot;movd  (%1), %%mm0          \n\t&quot;
+        &quot;movd 1(%1), %%mm6          \n\t&quot;
+        &quot;add %3, %1                 \n\t&quot;
+        &quot;punpcklbw %%mm7, %%mm0     \n\t&quot;
+        &quot;punpcklbw %%mm7, %%mm6     \n\t&quot;
+        &quot;pmullw %%mm4, %%mm0        \n\t&quot;
+        &quot;pmullw %%mm2, %%mm6        \n\t&quot;
+        &quot;paddw %%mm0, %%mm6         \n\t&quot;
+
+        &quot;1:                         \n\t&quot;
+        &quot;movd  (%1), %%mm0          \n\t&quot;
+        &quot;movd 1(%1), %%mm1          \n\t&quot;
+        &quot;add %3, %1                 \n\t&quot;
+        &quot;punpcklbw %%mm7, %%mm0     \n\t&quot;
+        &quot;punpcklbw %%mm7, %%mm1     \n\t&quot;
+        &quot;pmullw %%mm4, %%mm0        \n\t&quot;
+        &quot;pmullw %%mm2, %%mm1        \n\t&quot;
+        &quot;paddw %%mm0, %%mm1         \n\t&quot;
+        &quot;movq %%mm1, %%mm0          \n\t&quot;
+        &quot;pmullw %%mm5, %%mm6        \n\t&quot;
+        &quot;pmullw %%mm3, %%mm1        \n\t&quot;
+        &quot;paddw %4, %%mm6            \n\t&quot;
+        &quot;paddw %%mm6, %%mm1         \n\t&quot;
+        &quot;psrlw $6, %%mm1            \n\t&quot;
+        &quot;packuswb %%mm1, %%mm1      \n\t&quot;
+        H264_CHROMA_OP4((%0), %%mm1, %%mm6)
+        &quot;movd %%mm1, (%0)           \n\t&quot;
+        &quot;add %3, %0                 \n\t&quot;
+        &quot;movd  (%1), %%mm6          \n\t&quot;
+        &quot;movd 1(%1), %%mm1          \n\t&quot;
+        &quot;add %3, %1                 \n\t&quot;
+        &quot;punpcklbw %%mm7, %%mm6     \n\t&quot;
+        &quot;punpcklbw %%mm7, %%mm1     \n\t&quot;
+        &quot;pmullw %%mm4, %%mm6        \n\t&quot;
+        &quot;pmullw %%mm2, %%mm1        \n\t&quot;
+        &quot;paddw %%mm6, %%mm1         \n\t&quot;
+        &quot;movq %%mm1, %%mm6          \n\t&quot;
+        &quot;pmullw %%mm5, %%mm0        \n\t&quot;
+        &quot;pmullw %%mm3, %%mm1        \n\t&quot;
+        &quot;paddw %4, %%mm0            \n\t&quot;
+        &quot;paddw %%mm0, %%mm1         \n\t&quot;
+        &quot;psrlw $6, %%mm1            \n\t&quot;
+        &quot;packuswb %%mm1, %%mm1      \n\t&quot;
+        H264_CHROMA_OP4((%0), %%mm1, %%mm0)
+        &quot;movd %%mm1, (%0)           \n\t&quot;
+        &quot;add %3, %0                 \n\t&quot;
+        &quot;sub $2, %2                 \n\t&quot;
+        &quot;jnz 1b                     \n\t&quot;
+        : &quot;+r&quot;(dst), &quot;+r&quot;(src), &quot;+r&quot;(h)
+        : &quot;r&quot;((x86_reg)stride), &quot;m&quot;(ff_pw_32), &quot;m&quot;(x), &quot;m&quot;(y)
+    );
+}
+
+#ifdef H264_CHROMA_MC2_TMPL
+static void H264_CHROMA_MC2_TMPL(uint8_t *dst/*align 2*/, uint8_t *src/*align 1*/, int stride, int h, int x, int y)
+{
+    int tmp = ((1&lt;&lt;16)-1)*x + 8;
+    int CD= tmp*y;
+    int AB= (tmp&lt;&lt;3) - CD;
+    asm volatile(
+        /* mm5 = {A,B,A,B} */
+        /* mm6 = {C,D,C,D} */
+        &quot;movd %0, %%mm5\n\t&quot;
+        &quot;movd %1, %%mm6\n\t&quot;
+        &quot;punpckldq %%mm5, %%mm5\n\t&quot;
+        &quot;punpckldq %%mm6, %%mm6\n\t&quot;
+        &quot;pxor %%mm7, %%mm7\n\t&quot;
+        /* mm0 = src[0,1,1,2] */
+        &quot;movd %2, %%mm2\n\t&quot;
+        &quot;punpcklbw %%mm7, %%mm2\n\t&quot;
+        &quot;pshufw $0x94, %%mm2, %%mm2\n\t&quot;
+        :: &quot;r&quot;(AB), &quot;r&quot;(CD), &quot;m&quot;(src[0]));
+
+
+    asm volatile(
+        &quot;1:\n\t&quot;
+        &quot;add %4, %1\n\t&quot;
+        /* mm1 = A * src[0,1] + B * src[1,2] */
+        &quot;movq    %%mm2, %%mm1\n\t&quot;
+        &quot;pmaddwd %%mm5, %%mm1\n\t&quot;
+        /* mm0 = src[0,1,1,2] */
+        &quot;movd (%1), %%mm0\n\t&quot;
+        &quot;punpcklbw %%mm7, %%mm0\n\t&quot;
+        &quot;pshufw $0x94, %%mm0, %%mm0\n\t&quot;
+        /* mm1 += C * src[0,1] + D * src[1,2] */
+        &quot;movq    %%mm0, %%mm2\n\t&quot;
+        &quot;pmaddwd %%mm6, %%mm0\n\t&quot;
+        &quot;paddw      %3, %%mm1\n\t&quot;
+        &quot;paddw   %%mm0, %%mm1\n\t&quot;
+        /* dst[0,1] = pack((mm1 + 32) &gt;&gt; 6) */
+        &quot;psrlw $6, %%mm1\n\t&quot;
+        &quot;packssdw %%mm7, %%mm1\n\t&quot;
+        &quot;packuswb %%mm7, %%mm1\n\t&quot;
+        H264_CHROMA_OP4((%0), %%mm1, %%mm3)
+        &quot;movd %%mm1, %%esi\n\t&quot;
+        &quot;movw %%si, (%0)\n\t&quot;
+        &quot;add %4, %0\n\t&quot;
+        &quot;sub $1, %2\n\t&quot;
+        &quot;jnz 1b\n\t&quot;
+        : &quot;+r&quot; (dst), &quot;+r&quot;(src), &quot;+r&quot;(h)
+        : &quot;m&quot; (ff_pw_32), &quot;r&quot;((x86_reg)stride)
+        : &quot;%esi&quot;);
+
+}
+#endif
+

Added: haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/dsputil_h264_template_ssse3.c
===================================================================
--- haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/dsputil_h264_template_ssse3.c	2008-09-15 14:04:37 UTC (rev 27549)
+++ haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec/i386/dsputil_h264_template_ssse3.c	2008-09-15 14:04:59 UTC (rev 27550)
@@ -0,0 +1,208 @@
+/*
+ * Copyright (c) 2008 Loren Merritt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * SSSE3 optimized version of (put|avg)_h264_chroma_mc8.
+ * H264_CHROMA_MC8_TMPL must be defined to the desired function name
+ * H264_CHROMA_MC8_MV0 must be defined to a (put|avg)_pixels8 function
+ * AVG_OP must be defined to empty for put and the identify for avg
+ */
+static void H264_CHROMA_MC8_TMPL(uint8_t *dst/*align 8*/, uint8_t *src/*align 1*/, int stride, int h, int x, int y, int rnd)
+{
+    if(y==0 &amp;&amp; x==0) {
+        /* no filter needed */
+        H264_CHROMA_MC8_MV0(dst, src, stride, h);
+        return;
+    }
+
+    assert(x&lt;8 &amp;&amp; y&lt;8 &amp;&amp; x&gt;=0 &amp;&amp; y&gt;=0);
+
+    if(y==0 || x==0)
+    {
+        /* 1 dimensional filter only */
+        asm volatile(
+            &quot;movd %0, %%xmm7 \n\t&quot;
+            &quot;movq %1, %%xmm6 \n\t&quot;
+            &quot;pshuflw $0, %%xmm7, %%xmm7 \n\t&quot;
+            &quot;movlhps %%xmm6, %%xmm6 \n\t&quot;
+            &quot;movlhps %%xmm7, %%xmm7 \n\t&quot;

[... truncated: 31269 lines follow ...]

</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="011898.html">[Haiku-commits] r27549 -	haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec
</A></li>
	<LI>Next message: <A HREF="011861.html">[Haiku-commits] r27551 -	haiku/trunk/src/add-ons/media/plugins/avcodec/libavcodec
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#11860">[ date ]</a>
              <a href="thread.html#11860">[ thread ]</a>
              <a href="subject.html#11860">[ subject ]</a>
              <a href="author.html#11860">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/haiku-commits">More information about the Haiku-commits
mailing list</a><br>
</body></html>
