<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Haiku-commits] r27100 - in haiku/trunk: headers/private/kernel	headers/private/kernel/slab src/system/kernel/slab	src/system/kernel/vm
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/haiku-commits/2008-August/index.html" >
   <LINK REL="made" HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r27100%20-%20in%20haiku/trunk%3A%20headers/private/kernel%0A%09headers/private/kernel/slab%20src/system/kernel/slab%0A%09src/system/kernel/vm&In-Reply-To=%3C200808210321.m7L3LgPX001889%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="011159.html">
   <LINK REL="Next"  HREF="011161.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Haiku-commits] r27100 - in haiku/trunk: headers/private/kernel	headers/private/kernel/slab src/system/kernel/slab	src/system/kernel/vm</H1>
    <B>bonefish at mail.berlios.de</B> 
    <A HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r27100%20-%20in%20haiku/trunk%3A%20headers/private/kernel%0A%09headers/private/kernel/slab%20src/system/kernel/slab%0A%09src/system/kernel/vm&In-Reply-To=%3C200808210321.m7L3LgPX001889%40sheep.berlios.de%3E"
       TITLE="[Haiku-commits] r27100 - in haiku/trunk: headers/private/kernel	headers/private/kernel/slab src/system/kernel/slab	src/system/kernel/vm">bonefish at mail.berlios.de
       </A><BR>
    <I>Thu Aug 21 05:21:42 CEST 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="011159.html">[Haiku-commits] r27099 -	haiku/trunk/src/system/kernel/device_manager
</A></li>
        <LI>Next message: <A HREF="011161.html">[Haiku-commits] r27101 - haiku/trunk/src/system/kernel/vm
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#11160">[ date ]</a>
              <a href="thread.html#11160">[ thread ]</a>
              <a href="subject.html#11160">[ subject ]</a>
              <a href="author.html#11160">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: bonefish
Date: 2008-08-21 05:21:37 +0200 (Thu, 21 Aug 2008)
New Revision: 27100
ViewCVS: <A HREF="http://svn.berlios.de/viewcvs/haiku?rev=27100&amp;view=rev">http://svn.berlios.de/viewcvs/haiku?rev=27100&amp;view=rev</A>

Modified:
   haiku/trunk/headers/private/kernel/slab/Slab.h
   haiku/trunk/headers/private/kernel/vm.h
   haiku/trunk/src/system/kernel/slab/Slab.cpp
   haiku/trunk/src/system/kernel/vm/vm.cpp
Log:
Added object_cache_set_minimum_reserve() which sets the minimal number
of free objects an object cache should try to have ready. If the number
of free objects drops below the threshold, a new urgent priority thread
is asked to asynchronously resize the object cache (pretty similar to
the heap grower thread). Such a mechanism is necessary for code paths
that are supposed to free pages, but may need memory themselves (like
the swap support).


Modified: haiku/trunk/headers/private/kernel/slab/Slab.h
===================================================================
--- haiku/trunk/headers/private/kernel/slab/Slab.h	2008-08-21 03:11:47 UTC (rev 27099)
+++ haiku/trunk/headers/private/kernel/slab/Slab.h	2008-08-21 03:21:37 UTC (rev 27100)
@@ -46,6 +46,9 @@
 
 void delete_object_cache(object_cache *cache);
 
+status_t object_cache_set_minimum_reserve(object_cache *cache,
+	size_t objectCount);
+
 void *object_cache_alloc(object_cache *cache, uint32 flags);
 void object_cache_free(object_cache *cache, void *object);
 

Modified: haiku/trunk/headers/private/kernel/vm.h
===================================================================
--- haiku/trunk/headers/private/kernel/vm.h	2008-08-21 03:11:47 UTC (rev 27099)
+++ haiku/trunk/headers/private/kernel/vm.h	2008-08-21 03:21:37 UTC (rev 27100)
@@ -40,6 +40,7 @@
 void slab_init(struct kernel_args *args, addr_t initialBase,
 	size_t initialSize);
 void slab_init_post_sem();
+void slab_init_post_thread();
 
 // to protect code regions with interrupts turned on
 void permit_page_faults(void);

Modified: haiku/trunk/src/system/kernel/slab/Slab.cpp
===================================================================
--- haiku/trunk/src/system/kernel/slab/Slab.cpp	2008-08-21 03:11:47 UTC (rev 27099)
+++ haiku/trunk/src/system/kernel/slab/Slab.cpp	2008-08-21 03:21:37 UTC (rev 27100)
@@ -11,11 +11,13 @@
 
 #include &lt;algorithm&gt;
 #include &lt;new&gt;
+#include &lt;signal.h&gt;
 #include &lt;stdlib.h&gt;
 #include &lt;string.h&gt;
 
 #include &lt;KernelExport.h&gt;
 
+#include &lt;condition_variable.h&gt;
 #include &lt;Depot.h&gt;
 #include &lt;kernel.h&gt;
 #include &lt;low_resource_manager.h&gt;
@@ -54,12 +56,14 @@
 
 struct slab : DoublyLinkedListLinkImpl&lt;slab&gt; {
 	void *pages;
-	size_t count, size;
+	size_t size;		// total number of objects
+	size_t count;		// free objects
 	size_t offset;
 	object_link *free;
 };
 
 typedef DoublyLinkedList&lt;slab&gt; SlabList;
+struct ResizeRequest;
 
 struct object_cache : DoublyLinkedListLinkImpl&lt;object_cache&gt; {
 	char name[32];
@@ -67,24 +71,30 @@
 	size_t object_size;
 	size_t cache_color_cycle;
 	SlabList empty, partial, full;
-	size_t used_count, empty_count, pressure;
+	size_t total_objects;		// total number of objects
+	size_t used_count;			// used objects
+	size_t empty_count;			// empty slabs
+	size_t pressure;
+	size_t min_object_reserve;	// minimum number of free objects
 
 	size_t slab_size;
 	size_t usage, maximum;
 	uint32 flags;
 
+	ResizeRequest *resize_request;
+
 	void *cookie;
 	object_cache_constructor constructor;
 	object_cache_destructor destructor;
 	object_cache_reclaimer reclaimer;
 
 	status_t (*allocate_pages)(object_cache *cache, void **pages,
-		uint32 flags);
+		uint32 flags, bool unlockWhileAllocating);
 	void (*free_pages)(object_cache *cache, void *pages);
 
 	object_depot depot;
 
-	virtual slab *CreateSlab(uint32 flags) = 0;
+	virtual slab *CreateSlab(uint32 flags, bool unlockWhileAllocating) = 0;
 	virtual void ReturnSlab(slab *slab) = 0;
 	virtual slab *ObjectSlab(void *object) const = 0;
 
@@ -95,12 +105,15 @@
 	virtual void UnprepareObject(slab *source, void *object) {}
 
 	virtual ~object_cache() {}
+
+	bool Lock()		{ return mutex_lock(&amp;lock) == B_OK; }
+	void Unlock()	{ mutex_unlock(&amp;lock); }
 };
 
 typedef DoublyLinkedList&lt;object_cache&gt; ObjectCacheList;
 
 struct SmallObjectCache : object_cache {
-	slab *CreateSlab(uint32 flags);
+	slab *CreateSlab(uint32 flags, bool unlockWhileAllocating);
 	void ReturnSlab(slab *slab);
 	slab *ObjectSlab(void *object) const;
 };
@@ -139,7 +152,7 @@
 	HashedObjectCache()
 		: hash_table(this) {}
 
-	slab *CreateSlab(uint32 flags);
+	slab *CreateSlab(uint32 flags, bool unlockWhileAllocating);
 	void ReturnSlab(slab *slab);
 	slab *ObjectSlab(void *object) const;
 	status_t PrepareObject(slab *source, void *object);
@@ -162,7 +175,23 @@
 	struct depot_magazine *loaded, *previous;
 };
 
+struct ResizeRequest : DoublyLinkedListLinkImpl&lt;ResizeRequest&gt; {
+	ResizeRequest(object_cache* cache)
+		:
+		cache(cache),
+		pending(false),
+		delete_when_done(false)
+	{
+	}
 
+	object_cache*	cache;
+	bool			pending;
+	bool			delete_when_done;
+};
+
+typedef DoublyLinkedList&lt;ResizeRequest&gt; ResizeRequestQueue;
+
+
 static ObjectCacheList sObjectCaches;
 static mutex sObjectCacheListLock = MUTEX_INITIALIZER(&quot;object cache list&quot;);
 
@@ -171,11 +200,16 @@
 
 
 static status_t object_cache_reserve_internal(object_cache *cache,
-	size_t object_count, uint32 flags);
+	size_t object_count, uint32 flags, bool unlockWhileAllocating);
 static depot_magazine *alloc_magazine();
 static void free_magazine(depot_magazine *magazine);
 
+static mutex sResizeRequestsLock
+	= MUTEX_INITIALIZER(&quot;object cache resize requests&quot;);
+static ResizeRequestQueue sResizeRequests;
+static ConditionVariable sResizeRequestsCondition;
 
+
 #if OBJECT_CACHE_TRACING
 
 
@@ -405,7 +439,8 @@
 
 
 static status_t
-area_allocate_pages(object_cache *cache, void **pages, uint32 flags)
+area_allocate_pages(object_cache *cache, void **pages, uint32 flags,
+	bool unlockWhileAllocating)
 {
 	TRACE_CACHE(cache, &quot;allocate pages (%lu, 0x0%lx)&quot;, cache-&gt;slab_size, flags);
 
@@ -418,11 +453,18 @@
 		&amp;&amp; cache-&gt;slab_size != B_PAGE_SIZE)
 		addressSpec = B_ANY_KERNEL_BLOCK_ADDRESS;
 
+	if (unlockWhileAllocating)
+		cache-&gt;Unlock();
+
 	// if we are allocating, it is because we need the pages immediatly
 	// so we lock them. when moving the slab to the empty list we should
 	// unlock them, and lock them again when getting one from the empty list.
 	area_id areaId = create_area(cache-&gt;name, pages, addressSpec,
 		cache-&gt;slab_size, lock, B_KERNEL_READ_AREA | B_KERNEL_WRITE_AREA);
+
+	if (unlockWhileAllocating)
+		cache-&gt;Lock();
+
 	if (areaId &lt; 0)
 		return areaId;
 
@@ -451,14 +493,21 @@
 
 
 static status_t
-early_allocate_pages(object_cache *cache, void **pages, uint32 flags)
+early_allocate_pages(object_cache *cache, void **pages, uint32 flags,
+	bool unlockWhileAllocating)
 {
 	TRACE_CACHE(cache, &quot;early allocate pages (%lu, 0x0%lx)&quot;, cache-&gt;slab_size,
 		flags);
 
+	if (unlockWhileAllocating)
+		cache-&gt;Unlock();
+
 	addr_t base = vm_allocate_early(sKernelArgs, cache-&gt;slab_size,
 		cache-&gt;slab_size, B_KERNEL_READ_AREA | B_KERNEL_WRITE_AREA);
 
+	if (unlockWhileAllocating)
+		cache-&gt;Lock();
+
 	*pages = (void *)base;
 
 	cache-&gt;usage += cache-&gt;slab_size;
@@ -558,15 +607,19 @@
 		cache-&gt;object_size);
 
 	cache-&gt;cache_color_cycle = 0;
+	cache-&gt;total_objects = 0;
 	cache-&gt;used_count = 0;
 	cache-&gt;empty_count = 0;
 	cache-&gt;pressure = 0;
+	cache-&gt;min_object_reserve = 0;
 
 	cache-&gt;usage = 0;
 	cache-&gt;maximum = maximum;
 
 	cache-&gt;flags = flags;
 
+	cache-&gt;resize_request = NULL;
+
 	// TODO: depot destruction is obviously broken
 	// no gain in using the depot in single cpu setups
 	//if (smp_get_num_cpus() == 1)
@@ -634,7 +687,68 @@
 }
 
 
+static status_t
+object_cache_resizer(void*)
+{
+	while (true) {
+		MutexLocker locker(sResizeRequestsLock);
+
+		// wait for the next request
+		while (sResizeRequests.IsEmpty()) {
+			ConditionVariableEntry entry;
+			sResizeRequestsCondition.Add(&amp;entry);
+			locker.Unlock();
+			entry.Wait();
+			locker.Lock();
+		}
+
+		ResizeRequest* request = sResizeRequests.RemoveHead();
+
+		locker.Unlock();
+
+		// resize the cache, if necessary
+
+		object_cache* cache = request-&gt;cache;
+
+		MutexLocker cacheLocker(cache-&gt;lock);
+
+		size_t freeObjects = cache-&gt;total_objects - cache-&gt;used_count;
+
+		while (freeObjects &lt; cache-&gt;min_object_reserve) {
+			status_t error = object_cache_reserve_internal(cache,
+				cache-&gt;min_object_reserve - freeObjects, 0, true);
+			if (error != B_OK) {
+				dprintf(&quot;object cache resizer: Failed to resize object cache &quot;
+					&quot;%p!\n&quot;, cache);
+				break;
+			}
+
+			freeObjects = cache-&gt;total_objects - cache-&gt;used_count;
+		}
+
+		request-&gt;pending = false;
+
+		if (request-&gt;delete_when_done)
+			delete request;
+	}
+}
+
+
 static void
+increase_object_reserve(object_cache* cache)
+{
+	if (cache-&gt;resize_request-&gt;pending)
+		return;
+
+	cache-&gt;resize_request-&gt;pending = true;
+
+	MutexLocker locker(sResizeRequestsLock);
+	sResizeRequests.Add(cache-&gt;resize_request);
+	sResizeRequestsCondition.NotifyAll();
+}
+
+
+static void
 delete_cache(object_cache *cache)
 {
 	cache-&gt;~object_cache();
@@ -762,6 +876,35 @@
 }
 
 
+status_t
+object_cache_set_minimum_reserve(object_cache *cache, size_t objectCount)
+{
+	MutexLocker _(cache-&gt;lock);
+
+	if (cache-&gt;min_object_reserve == objectCount)
+		return B_OK;
+
+	if (cache-&gt;min_object_reserve == 0) {
+		cache-&gt;resize_request = new(std::nothrow) ResizeRequest(cache);
+		if (cache-&gt;resize_request == NULL)
+			return B_NO_MEMORY;
+	} else if (cache-&gt;min_object_reserve == 0) {
+		if (cache-&gt;resize_request-&gt;pending)
+			cache-&gt;resize_request-&gt;delete_when_done = true;
+		else
+			delete cache-&gt;resize_request;
+
+		cache-&gt;resize_request = NULL;
+	}
+
+	cache-&gt;min_object_reserve = objectCount;
+
+	increase_object_reserve(cache);
+
+	return B_OK;
+}
+
+
 void *
 object_cache_alloc(object_cache *cache, uint32 flags)
 {
@@ -778,7 +921,7 @@
 
 	if (cache-&gt;partial.IsEmpty()) {
 		if (cache-&gt;empty.IsEmpty()) {
-			if (object_cache_reserve_internal(cache, 1, flags) &lt; B_OK) {
+			if (object_cache_reserve_internal(cache, 1, flags, false) &lt; B_OK) {
 				T(Alloc(cache, flags, NULL));
 				return NULL;
 			}
@@ -799,6 +942,9 @@
 	source-&gt;count--;
 	cache-&gt;used_count++;
 
+	if (cache-&gt;total_objects - cache-&gt;used_count &lt; cache-&gt;min_object_reserve)
+		increase_object_reserve(cache);
+
 	REMOVE_PARANOIA_CHECK(PARANOIA_SUSPICIOUS, source, &amp;link-&gt;next,
 		sizeof(void*));
 
@@ -839,7 +985,9 @@
 	if (source-&gt;count == source-&gt;size) {
 		cache-&gt;partial.Remove(source);
 
-		if (cache-&gt;empty_count &lt; cache-&gt;pressure) {
+		if (cache-&gt;empty_count &lt; cache-&gt;pressure
+			&amp;&amp; cache-&gt;total_objects - cache-&gt;used_count - source-&gt;size
+				&gt;= cache-&gt;min_object_reserve) {
 			cache-&gt;empty_count++;
 			cache-&gt;empty.Add(source);
 		} else {
@@ -871,14 +1019,16 @@
 
 
 static status_t
-object_cache_reserve_internal(object_cache *cache, size_t object_count,
-	uint32 flags)
+object_cache_reserve_internal(object_cache *cache, size_t objectCount,
+	uint32 flags, bool unlockWhileAllocating)
 {
-	size_t numBytes = object_count * cache-&gt;object_size;
+	size_t numBytes = objectCount * cache-&gt;object_size;
 	size_t slabCount = ((numBytes - 1) / cache-&gt;slab_size) + 1;
+		// TODO: This also counts the unusable space of each slab, which can
+		// sum up.
 
 	while (slabCount &gt; 0) {
-		slab *newSlab = cache-&gt;CreateSlab(flags);
+		slab *newSlab = cache-&gt;CreateSlab(flags, unlockWhileAllocating);
 		if (newSlab == NULL)
 			return B_NO_MEMORY;
 
@@ -900,7 +1050,7 @@
 	T(Reserve(cache, objectCount, flags));
 
 	MutexLocker _(cache-&gt;lock);
-	return object_cache_reserve_internal(cache, objectCount, flags);
+	return object_cache_reserve_internal(cache, objectCount, flags, false);
 }
 
 
@@ -921,6 +1071,7 @@
 	slab-&gt;pages = pages;
 	slab-&gt;count = slab-&gt;size = byteCount / object_size;
 	slab-&gt;free = NULL;
+	total_objects += slab-&gt;size;
 
 	size_t spareBytes = byteCount - (slab-&gt;size * object_size);
 	slab-&gt;offset = cache_color_cycle;
@@ -983,6 +1134,8 @@
 	if (slab-&gt;count != slab-&gt;size)
 		panic(&quot;cache: destroying a slab which isn't empty.&quot;);
 
+	total_objects -= slab-&gt;size;
+
 	DELETE_PARANOIA_CHECK_SET(slab);
 
 	uint8 *data = ((uint8 *)slab-&gt;pages) + slab-&gt;offset;
@@ -1022,14 +1175,14 @@
 
 
 slab *
-SmallObjectCache::CreateSlab(uint32 flags)
+SmallObjectCache::CreateSlab(uint32 flags, bool unlockWhileAllocating)
 {
 	if (!check_cache_quota(this))
 		return NULL;
 
 	void *pages;
 
-	if (allocate_pages(this, &amp;pages, flags) &lt; B_OK)
+	if (allocate_pages(this, &amp;pages, flags, unlockWhileAllocating) &lt; B_OK)
 		return NULL;
 
 	return InitSlab(slab_in_pages(pages, slab_size), pages,
@@ -1082,18 +1235,25 @@
 
 
 slab *
-HashedObjectCache::CreateSlab(uint32 flags)
+HashedObjectCache::CreateSlab(uint32 flags, bool unlockWhileAllocating)
 {
 	if (!check_cache_quota(this))
 		return NULL;
 
+	if (unlockWhileAllocating)
+		Unlock();
+
 	slab *slab = allocate_slab(flags);
+
+	if (unlockWhileAllocating)
+		Lock();
+
 	if (slab == NULL)
 		return NULL;
 
 	void *pages;
 
-	if (allocate_pages(this, &amp;pages, flags) == B_OK) {
+	if (allocate_pages(this, &amp;pages, flags, unlockWhileAllocating) == B_OK) {
 		if (InitSlab(slab, pages, slab_size))
 			return slab;
 
@@ -1474,3 +1634,20 @@
 	block_allocator_init_rest();
 }
 
+
+void
+slab_init_post_thread()
+{
+	new(&amp;sResizeRequests) ResizeRequestQueue;
+	sResizeRequestsCondition.Init(&amp;sResizeRequests, &quot;object cache resizer&quot;);
+
+	thread_id objectCacheResizer = spawn_kernel_thread(object_cache_resizer,
+		&quot;object cache resizer&quot;, B_URGENT_PRIORITY, NULL);
+	if (objectCacheResizer &lt; 0) {
+		panic(&quot;slab_init_post_thread(): failed to spawn object cache resizer &quot;
+			&quot;thread\n&quot;);
+		return;
+	}
+
+	send_signal_etc(objectCacheResizer, SIGCONT, B_DO_NOT_RESCHEDULE);
+}

Modified: haiku/trunk/src/system/kernel/vm/vm.cpp
===================================================================
--- haiku/trunk/src/system/kernel/vm/vm.cpp	2008-08-21 03:11:47 UTC (rev 27099)
+++ haiku/trunk/src/system/kernel/vm/vm.cpp	2008-08-21 03:21:37 UTC (rev 27100)
@@ -4049,6 +4049,7 @@
 {
 	vm_page_init_post_thread(args);
 	vm_daemon_init();
+	slab_init_post_thread();
 	return heap_init_post_thread();
 }
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="011159.html">[Haiku-commits] r27099 -	haiku/trunk/src/system/kernel/device_manager
</A></li>
	<LI>Next message: <A HREF="011161.html">[Haiku-commits] r27101 - haiku/trunk/src/system/kernel/vm
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#11160">[ date ]</a>
              <a href="thread.html#11160">[ thread ]</a>
              <a href="subject.html#11160">[ subject ]</a>
              <a href="author.html#11160">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/haiku-commits">More information about the Haiku-commits
mailing list</a><br>
</body></html>
