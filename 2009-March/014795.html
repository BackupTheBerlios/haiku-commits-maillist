<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Haiku-commits] r29478 - haiku/trunk/src/system/kernel/vm
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/haiku-commits/2009-March/index.html" >
   <LINK REL="made" HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r29478%20-%20haiku/trunk/src/system/kernel/vm&In-Reply-To=%3C200903121218.n2CCISYm009008%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="014794.html">
   <LINK REL="Next"  HREF="014796.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Haiku-commits] r29478 - haiku/trunk/src/system/kernel/vm</H1>
    <B>axeld at BerliOS</B> 
    <A HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r29478%20-%20haiku/trunk/src/system/kernel/vm&In-Reply-To=%3C200903121218.n2CCISYm009008%40sheep.berlios.de%3E"
       TITLE="[Haiku-commits] r29478 - haiku/trunk/src/system/kernel/vm">axeld at mail.berlios.de
       </A><BR>
    <I>Thu Mar 12 13:18:28 CET 2009</I>
    <P><UL>
        <LI>Previous message: <A HREF="014794.html">[Haiku-commits] r29477 - haiku/trunk/src/apps/mandelbrot
</A></li>
        <LI>Next message: <A HREF="014796.html">[Haiku-commits] r29479 -	haiku/trunk/src/add-ons/kernel/file_systems/bfs
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#14795">[ date ]</a>
              <a href="thread.html#14795">[ thread ]</a>
              <a href="subject.html#14795">[ subject ]</a>
              <a href="author.html#14795">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: axeld
Date: 2009-03-12 13:18:25 +0100 (Thu, 12 Mar 2009)
New Revision: 29478
ViewCVS: <A HREF="http://svn.berlios.de/viewcvs/haiku?rev=29478&amp;view=rev">http://svn.berlios.de/viewcvs/haiku?rev=29478&amp;view=rev</A>

Modified:
   haiku/trunk/src/system/kernel/vm/vm.cpp
Log:
* Cleanup, no functional change.


Modified: haiku/trunk/src/system/kernel/vm/vm.cpp
===================================================================
--- haiku/trunk/src/system/kernel/vm/vm.cpp	2009-03-12 11:27:22 UTC (rev 29477)
+++ haiku/trunk/src/system/kernel/vm/vm.cpp	2009-03-12 12:18:25 UTC (rev 29478)
@@ -191,7 +191,7 @@
 
 #define AREA_HASH_TABLE_SIZE 1024
 static area_id sNextAreaID = 1;
-static hash_table *sAreaHash;
+static hash_table* sAreaHash;
 static rw_lock sAreaHashLock = RW_LOCK_INITIALIZER(&quot;area hash&quot;);
 static mutex sMappingLock = MUTEX_INITIALIZER(&quot;page mappings&quot;);
 static mutex sAreaCacheLock = MUTEX_INITIALIZER(&quot;area-&gt;cache&quot;);
@@ -216,14 +216,14 @@
 
 
 // function declarations
-static void delete_area(vm_address_space *addressSpace, vm_area *area);
-static vm_address_space *get_address_space_by_area_id(area_id id);
-static status_t vm_soft_fault(vm_address_space *addressSpace, addr_t address,
+static void delete_area(vm_address_space* addressSpace, vm_area* area);
+static vm_address_space* get_address_space_by_area_id(area_id id);
+static status_t vm_soft_fault(vm_address_space* addressSpace, addr_t address,
 	bool isWrite, bool isUser);
-static status_t map_backing_store(vm_address_space *addressSpace,
-	vm_cache *cache, void **_virtualAddress, off_t offset, addr_t size,
+static status_t map_backing_store(vm_address_space* addressSpace,
+	vm_cache* cache, void** _virtualAddress, off_t offset, addr_t size,
 	uint32 addressSpec, int wiring, int protection, int mapping,
-	vm_area **_area, const char *areaName, bool unmapAddressRange, bool kernel);
+	vm_area** _area, const char* areaName, bool unmapAddressRange, bool kernel);
 
 
 //	#pragma mark -
@@ -313,7 +313,7 @@
 	rw_lock_read_lock(&amp;fSpace-&gt;lock);
 
 	rw_lock_read_lock(&amp;sAreaHashLock);
-	area = (vm_area *)hash_lookup(sAreaHash, &amp;areaID);
+	area = (vm_area*)hash_lookup(sAreaHash, &amp;areaID);
 	rw_lock_read_unlock(&amp;sAreaHashLock);
 
 	if (area == NULL || area-&gt;address_space != fSpace) {
@@ -415,7 +415,7 @@
 {
 	rw_lock_read_lock(&amp;sAreaHashLock);
 
-	area = (vm_area *)hash_lookup(sAreaHash, &amp;areaID);
+	area = (vm_area*)hash_lookup(sAreaHash, &amp;areaID);
 	if (area != NULL
 		&amp;&amp; (area-&gt;address_space-&gt;id == team
 			|| (allowKernel &amp;&amp; team == vm_kernel_address_space_id()))) {
@@ -434,7 +434,7 @@
 	rw_lock_write_lock(&amp;fSpace-&gt;lock);
 
 	rw_lock_read_lock(&amp;sAreaHashLock);
-	area = (vm_area *)hash_lookup(sAreaHash, &amp;areaID);
+	area = (vm_area*)hash_lookup(sAreaHash, &amp;areaID);
 	rw_lock_read_unlock(&amp;sAreaHashLock);
 
 	if (area == NULL) {
@@ -705,7 +705,7 @@
 
 		// check whether the area is gone in the meantime
 		rw_lock_read_lock(&amp;sAreaHashLock);
-		area = (vm_area *)hash_lookup(sAreaHash, &amp;areaID);
+		area = (vm_area*)hash_lookup(sAreaHash, &amp;areaID);
 		rw_lock_read_unlock(&amp;sAreaHashLock);
 
 		if (area == NULL) {
@@ -724,7 +724,7 @@
 		// ... unless we're supposed to check the areas' &quot;no_cache_change&quot; flag
 		bool yield = false;
 		if (done &amp;&amp; checkNoCacheChange) {
-			for (vm_area *tempArea = cache-&gt;areas; tempArea != NULL;
+			for (vm_area* tempArea = cache-&gt;areas; tempArea != NULL;
 					tempArea = tempArea-&gt;cache_next) {
 				if (tempArea-&gt;no_cache_change) {
 					done = false;
@@ -889,10 +889,10 @@
 
 
 static int
-area_compare(void *_area, const void *key)
+area_compare(void* _area, const void* key)
 {
-	vm_area *area = (vm_area *)_area;
-	const area_id *id = (const area_id *)key;
+	vm_area* area = (vm_area*)_area;
+	const area_id* id = (const area_id*)key;
 
 	if (area-&gt;id == *id)
 		return 0;
@@ -902,10 +902,10 @@
 
 
 static uint32
-area_hash(void *_area, const void *key, uint32 range)
+area_hash(void* _area, const void* key, uint32 range)
 {
-	vm_area *area = (vm_area *)_area;
-	const area_id *id = (const area_id *)key;
+	vm_area* area = (vm_area*)_area;
+	const area_id* id = (const area_id*)key;
 
 	if (area != NULL)
 		return area-&gt;id % range;
@@ -914,14 +914,14 @@
 }
 
 
-static vm_address_space *
+static vm_address_space*
 get_address_space_by_area_id(area_id id)
 {
 	vm_address_space* addressSpace = NULL;
 
 	rw_lock_read_lock(&amp;sAreaHashLock);
 
-	vm_area *area = (vm_area *)hash_lookup(sAreaHash, &amp;id);
+	vm_area* area = (vm_area*)hash_lookup(sAreaHash, &amp;id);
 	if (area != NULL) {
 		addressSpace = area-&gt;address_space;
 		atomic_add(&amp;addressSpace-&gt;ref_count, 1);
@@ -934,12 +934,12 @@
 
 
 //! You need to have the address space locked when calling this function
-static vm_area *
+static vm_area*
 lookup_area(vm_address_space* addressSpace, area_id id)
 {
 	rw_lock_read_lock(&amp;sAreaHashLock);
 
-	vm_area *area = (vm_area *)hash_lookup(sAreaHash, &amp;id);
+	vm_area* area = (vm_area*)hash_lookup(sAreaHash, &amp;id);
 	if (area != NULL &amp;&amp; area-&gt;address_space != addressSpace)
 		area = NULL;
 
@@ -949,10 +949,10 @@
 }
 
 
-static vm_area *
-create_reserved_area_struct(vm_address_space *addressSpace, uint32 flags)
+static vm_area*
+create_reserved_area_struct(vm_address_space* addressSpace, uint32 flags)
 {
-	vm_area *reserved = (vm_area *)malloc_nogrow(sizeof(vm_area));
+	vm_area* reserved = (vm_area*)malloc_nogrow(sizeof(vm_area));
 	if (reserved == NULL)
 		return NULL;
 
@@ -966,8 +966,8 @@
 }
 
 
-static vm_area *
-create_area_struct(vm_address_space *addressSpace, const char *name,
+static vm_area*
+create_area_struct(vm_address_space* addressSpace, const char* name,
 	uint32 wiring, uint32 protection)
 {
 	// restrict the area name to B_OS_NAME_LENGTH
@@ -975,11 +975,11 @@
 	if (length &gt; B_OS_NAME_LENGTH)
 		length = B_OS_NAME_LENGTH;
 
-	vm_area *area = (vm_area *)malloc_nogrow(sizeof(vm_area));
+	vm_area* area = (vm_area*)malloc_nogrow(sizeof(vm_area));
 	if (area == NULL)
 		return NULL;
 
-	area-&gt;name = (char *)malloc_nogrow(length);
+	area-&gt;name = (char*)malloc_nogrow(length);
 	if (area-&gt;name == NULL) {
 		free(area);
 		return NULL;
@@ -1008,16 +1008,16 @@
 }
 
 
-/**	Finds a reserved area that covers the region spanned by \a start and
- *	\a size, inserts the \a area into that region and makes sure that
- *	there are reserved regions for the remaining parts.
- */
-
+/*!	Finds a reserved area that covers the region spanned by \a start and
+	\a size, inserts the \a area into that region and makes sure that
+	there are reserved regions for the remaining parts.
+*/
 static status_t
-find_reserved_area(vm_address_space *addressSpace, addr_t start,
-	addr_t size, vm_area *area)
+find_reserved_area(vm_address_space* addressSpace, addr_t start,
+	addr_t size, vm_area* area)
 {
-	vm_area *next, *last = NULL;
+	vm_area* last = NULL;
+	vm_area* next;
 
 	next = addressSpace-&gt;areas;
 	while (next) {
@@ -1068,7 +1068,7 @@
 	} else {
 		// the area splits the reserved range into two separate ones
 		// we need a new reserved area to cover this space
-		vm_area *reserved = create_reserved_area_struct(addressSpace,
+		vm_area* reserved = create_reserved_area_struct(addressSpace,
 			next-&gt;protection);
 		if (reserved == NULL)
 			return B_NO_MEMORY;
@@ -1095,11 +1095,11 @@
 
 /*!	Must be called with this address space's sem held */
 static status_t
-find_and_insert_area_slot(vm_address_space *addressSpace, addr_t start,
-	addr_t size, addr_t end, uint32 addressSpec, vm_area *area)
+find_and_insert_area_slot(vm_address_space* addressSpace, addr_t start,
+	addr_t size, addr_t end, uint32 addressSpec, vm_area* area)
 {
-	vm_area *last = NULL;
-	vm_area *next;
+	vm_area* last = NULL;
+	vm_area* next;
 	bool foundSpot = false;
 
 	TRACE((&quot;find_and_insert_area_slot: address space %p, start 0x%lx, &quot;
@@ -1120,7 +1120,7 @@
 
 		// There was no reserved area, and the slot doesn't seem to be used
 		// already
-		// ToDo: this could be further optimized.
+		// TODO: this could be further optimized.
 	}
 
 	size_t alignment = B_PAGE_SIZE;
@@ -1184,12 +1184,13 @@
 				// We didn't find a free spot - if there were any reserved areas
 				// with the RESERVED_AVOID_BASE flag set, we can now test those
 				// for free space
-				// ToDo: it would make sense to start with the biggest of them
+				// TODO: it would make sense to start with the biggest of them
 				next = addressSpace-&gt;areas;
 				last = NULL;
 				for (last = NULL; next; next = next-&gt;address_space_next,
 						last = next) {
-					// ToDo: take free space after the reserved area into account!
+					// TODO: take free space after the reserved area into
+					// account!
 					if (next-&gt;base == ROUNDUP(next-&gt;base, alignment)
 						&amp;&amp; next-&gt;size == size) {
 						// The reserved area is entirely covered, and thus,
@@ -1268,7 +1269,8 @@
 				}
 			} else {
 				if (next) {
-					if (last-&gt;base + last-&gt;size &lt;= start &amp;&amp; next-&gt;base &gt;= start + size) {
+					if (last-&gt;base + last-&gt;size &lt;= start
+						&amp;&amp; next-&gt;base &gt;= start + size) {
 						foundSpot = true;
 						area-&gt;base = start;
 						break;
@@ -1307,8 +1309,8 @@
 	You need to hold the vm_address_space semaphore.
 */
 static status_t
-insert_area(vm_address_space *addressSpace, void **_address,
-	uint32 addressSpec, addr_t size, vm_area *area)
+insert_area(vm_address_space* addressSpace, void** _address,
+	uint32 addressSpec, addr_t size, vm_area* area)
 {
 	addr_t searchBase, searchEnd;
 	status_t status;
@@ -1342,9 +1344,9 @@
 	status = find_and_insert_area_slot(addressSpace, searchBase, size,
 		searchEnd, addressSpec, area);
 	if (status == B_OK) {
-		// ToDo: do we have to do anything about B_ANY_KERNEL_ADDRESS
-		//		vs. B_ANY_KERNEL_BLOCK_ADDRESS here?
-		*_address = (void *)area-&gt;base;
+		// TODO: do we have to do anything about B_ANY_KERNEL_ADDRESS
+		// vs. B_ANY_KERNEL_BLOCK_ADDRESS here?
+		*_address = (void*)area-&gt;base;
 	}
 
 	return status;
@@ -1515,7 +1517,7 @@
 	The address space must be write-locked.
 */
 static status_t
-unmap_address_range(vm_address_space *addressSpace, addr_t address, addr_t size,
+unmap_address_range(vm_address_space* addressSpace, addr_t address, addr_t size,
 	bool kernel)
 {
 	size = PAGE_ALIGN(size);
@@ -1568,17 +1570,18 @@
 	Note, that in case of error your cache will be temporarily unlocked.
 */
 static status_t
-map_backing_store(vm_address_space *addressSpace, vm_cache *cache,
-	void **_virtualAddress, off_t offset, addr_t size, uint32 addressSpec,
-	int wiring, int protection, int mapping, vm_area **_area,
-	const char *areaName, bool unmapAddressRange, bool kernel)
+map_backing_store(vm_address_space* addressSpace, vm_cache* cache,
+	void** _virtualAddress, off_t offset, addr_t size, uint32 addressSpec,
+	int wiring, int protection, int mapping, vm_area** _area,
+	const char* areaName, bool unmapAddressRange, bool kernel)
 {
-	TRACE((&quot;map_backing_store: aspace %p, cache %p, *vaddr %p, offset 0x%Lx, size %lu, addressSpec %ld, wiring %d, protection %d, _area %p, area_name '%s'\n&quot;,
-		addressSpace, cache, *_virtualAddress, offset, size, addressSpec,
-		wiring, protection, _area, areaName));
+	TRACE((&quot;map_backing_store: aspace %p, cache %p, *vaddr %p, offset 0x%Lx, &quot;
+		&quot;size %lu, addressSpec %ld, wiring %d, protection %d, area %p, areaName &quot;
+		&quot;'%s'\n&quot;, addressSpace, cache, *_virtualAddress, offset, size,
+		addressSpec, wiring, protection, _area, areaName));
 	cache-&gt;AssertLocked();
 
-	vm_area *area = create_area_struct(addressSpace, areaName, wiring,
+	vm_area* area = create_area_struct(addressSpace, areaName, wiring,
 		protection);
 	if (area == NULL)
 		return B_NO_MEMORY;
@@ -1589,7 +1592,7 @@
 	// to handle the private copies of pages as they are written to
 	vm_cache* sourceCache = cache;
 	if (mapping == REGION_PRIVATE_MAP) {
-		vm_cache *newCache;
+		vm_cache* newCache;
 
 		// create an anonymous cache
 		status = VMCacheFactory::CreateAnonymousCache(newCache,
@@ -1671,7 +1674,7 @@
 
 
 status_t
-vm_unreserve_address_range(team_id team, void *address, addr_t size)
+vm_unreserve_address_range(team_id team, void* address, addr_t size)
 {
 	AddressSpaceWriteLocker locker(team);
 	if (!locker.IsLocked())
@@ -1693,7 +1696,7 @@
 		if (area-&gt;id == RESERVED_AREA_ID &amp;&amp; area-&gt;base &gt;= (addr_t)address
 			&amp;&amp; area-&gt;base + area-&gt;size &lt;= (addr_t)address + size) {
 			// remove reserved range
-			vm_area *reserved = area;
+			vm_area* reserved = area;
 			if (last)
 				last-&gt;address_space_next = reserved-&gt;address_space_next;
 			else
@@ -1714,7 +1717,7 @@
 
 
 status_t
-vm_reserve_address_range(team_id team, void **_address, uint32 addressSpec,
+vm_reserve_address_range(team_id team, void** _address, uint32 addressSpec,
 	addr_t size, uint32 flags)
 {
 	if (size == 0)
@@ -1731,7 +1734,7 @@
 		return B_BAD_TEAM_ID;
 	}
 
-	vm_area *area = create_reserved_area_struct(locker.AddressSpace(), flags);
+	vm_area* area = create_reserved_area_struct(locker.AddressSpace(), flags);
 	if (area == NULL)
 		return B_NO_MEMORY;
 
@@ -1753,13 +1756,13 @@
 
 
 area_id
-vm_create_anonymous_area(team_id team, const char *name, void **address,
+vm_create_anonymous_area(team_id team, const char* name, void** address,
 	uint32 addressSpec, addr_t size, uint32 wiring, uint32 protection,
 	uint32 flags, bool kernel)
 {
-	vm_area *area;
-	vm_cache *cache;
-	vm_page *page = NULL;
+	vm_area* area;
+	vm_cache* cache;
+	vm_page* page = NULL;
 	bool isStack = (protection &amp; B_STACK_AREA) != 0;
 	page_num_t guardPages;
 	bool canOvercommit = false;
@@ -1782,7 +1785,7 @@
 		isStack = true;
 #endif
 
-	/* check parameters */
+	// check parameters
 	switch (addressSpec) {
 		case B_ANY_ADDRESS:
 		case B_EXACT_ADDRESS:
@@ -1829,7 +1832,7 @@
 		if (status != B_OK)
 			return status;
 
-		vm_translation_map *map = &amp;locker.AddressSpace()-&gt;translation_map;
+		vm_translation_map* map = &amp;locker.AddressSpace()-&gt;translation_map;
 		reservedMapPages = map-&gt;ops-&gt;map_max_pages_need(map, 0, size - 1);
 	}
 
@@ -1851,7 +1854,7 @@
 	}
 
 	AddressSpaceWriteLocker locker;
-	vm_address_space *addressSpace;
+	vm_address_space* addressSpace;
 	status_t status;
 
 	// For full lock areas reserve the pages before locking the address
@@ -1940,7 +1943,8 @@
 			// Allocate and map all pages for this area
 
 			off_t offset = 0;
-			for (addr_t address = area-&gt;base; address &lt; area-&gt;base + (area-&gt;size - 1);
+			for (addr_t address = area-&gt;base;
+					address &lt; area-&gt;base + (area-&gt;size - 1);
 					address += B_PAGE_SIZE, offset += B_PAGE_SIZE) {
 #ifdef DEBUG_KERNEL_STACKS
 #	ifdef STACK_GROWS_DOWNWARDS
@@ -1952,7 +1956,7 @@
 #	endif
 					continue;
 #endif
-				vm_page *page = vm_page_allocate_page(PAGE_STATE_CLEAR, true);
+				vm_page* page = vm_page_allocate_page(PAGE_STATE_CLEAR, true);
 				cache-&gt;InsertPage(page, offset);
 				vm_map_page(area, page, address, protection);
 
@@ -1970,10 +1974,10 @@
 
 		case B_ALREADY_WIRED:
 		{
-			// the pages should already be mapped. This is only really useful during
-			// boot time. Find the appropriate vm_page objects and stick them in
-			// the cache object.
-			vm_translation_map *map = &amp;addressSpace-&gt;translation_map;
+			// The pages should already be mapped. This is only really useful
+			// during boot time. Find the appropriate vm_page objects and stick
+			// them in the cache object.
+			vm_translation_map* map = &amp;addressSpace-&gt;translation_map;
 			off_t offset = 0;
 
 			if (!gKernelStartup)
@@ -2009,9 +2013,9 @@
 
 		case B_CONTIGUOUS:
 		{
-			// We have already allocated our continuous pages run, so we can now just
-			// map them in the address space
-			vm_translation_map *map = &amp;addressSpace-&gt;translation_map;
+			// We have already allocated our continuous pages run, so we can now
+			// just map them in the address space
+			vm_translation_map* map = &amp;addressSpace-&gt;translation_map;
 			addr_t physicalAddress = page-&gt;physical_page_number * B_PAGE_SIZE;
 			addr_t virtualAddress = area-&gt;base;
 			off_t offset = 0;
@@ -2078,11 +2082,11 @@
 
 
 area_id
-vm_map_physical_memory(team_id team, const char *name, void **_address,
+vm_map_physical_memory(team_id team, const char* name, void** _address,
 	uint32 addressSpec, addr_t size, uint32 protection, addr_t physicalAddress)
 {
-	vm_area *area;
-	vm_cache *cache;
+	vm_area* area;
+	vm_cache* cache;
 	addr_t mapOffset;
 
 	TRACE((&quot;vm_map_physical_memory(aspace = %ld, \&quot;%s\&quot;, virtual = %p, &quot;
@@ -2135,7 +2139,7 @@
 	if (status &gt;= B_OK) {
 		// make sure our area is mapped in completely
 
-		vm_translation_map *map = &amp;locker.AddressSpace()-&gt;translation_map;
+		vm_translation_map* map = &amp;locker.AddressSpace()-&gt;translation_map;
 		size_t reservePages = map-&gt;ops-&gt;map_max_pages_need(map, area-&gt;base,
 			area-&gt;base + (size - 1));
 
@@ -2156,7 +2160,7 @@
 
 	// modify the pointer returned to be offset back into the new area
 	// the same way the physical address in was offset
-	*_address = (void *)((addr_t)*_address + mapOffset);
+	*_address = (void*)((addr_t)*_address + mapOffset);
 
 	area-&gt;cache_type = CACHE_TYPE_DEVICE;
 	return area-&gt;id;
@@ -2164,11 +2168,11 @@
 
 
 area_id
-vm_create_null_area(team_id team, const char *name, void **address,
+vm_create_null_area(team_id team, const char* name, void** address,
 	uint32 addressSpec, addr_t size)
 {
-	vm_area *area;
-	vm_cache *cache;
+	vm_area* area;
+	vm_cache* cache;
 	status_t status;
 
 	AddressSpaceWriteLocker locker(team);
@@ -2208,7 +2212,7 @@
 	The vnode has to be marked busy when calling this function.
 */
 status_t
-vm_create_vnode_cache(struct vnode *vnode, struct VMCache **cache)
+vm_create_vnode_cache(struct vnode* vnode, struct VMCache** cache)
 {
 	return VMCacheFactory::CreateVnodeCache(*cache, vnode);
 }
@@ -2226,7 +2230,7 @@
 
 	for (VMCachePagesTree::Iterator it
 				= cache-&gt;pages.GetIterator(firstPage, true, true);
-			vm_page *page = it.Next();) {
+			vm_page* page = it.Next();) {
 		if (page-&gt;cache_offset &gt;= endPage)
 			break;
 
@@ -2246,7 +2250,7 @@
 	\a offset and \a size arguments have to be page aligned.
 */
 static area_id
-_vm_map_file(team_id team, const char *name, void **_address, uint32 addressSpec,
+_vm_map_file(team_id team, const char* name, void** _address, uint32 addressSpec,
 	size_t size, uint32 protection, uint32 mapping, int fd, off_t offset,
 	bool kernel)
 {
@@ -2271,7 +2275,7 @@
 	}
 
 	// get the open flags of the FD
-	file_descriptor *descriptor = get_fd(get_current_io_context(kernel), fd);
+	file_descriptor* descriptor = get_fd(get_current_io_context(kernel), fd);
 	if (descriptor == NULL)
 		return EBADF;
 	int32 openMode = descriptor-&gt;open_mode;
@@ -2287,7 +2291,7 @@
 	}
 
 	// get the vnode for the object, this also grabs a ref to it
-	struct vnode *vnode = NULL;
+	struct vnode* vnode = NULL;
 	status_t status = vfs_get_vnode_from_fd(fd, kernel, &amp;vnode);
 	if (status &lt; B_OK)
 		return status;
@@ -2302,7 +2306,7 @@
 		if (status != B_OK)
 			return status;
 
-		vm_translation_map *map = &amp;locker.AddressSpace()-&gt;translation_map;
+		vm_translation_map* map = &amp;locker.AddressSpace()-&gt;translation_map;
 		reservedPreMapPages = map-&gt;ops-&gt;map_max_pages_need(map, 0, size - 1);
 
 		locker.Unlock();
@@ -2330,14 +2334,14 @@
 		return B_BAD_TEAM_ID;
 
 	// TODO: this only works for file systems that use the file cache
-	vm_cache *cache;
+	vm_cache* cache;
 	status = vfs_get_vnode_cache(vnode, &amp;cache, false);
 	if (status &lt; B_OK)
 		return status;
 
 	cache-&gt;Lock();
 
-	vm_area *area;
+	vm_area* area;
 	status = map_backing_store(locker.AddressSpace(), cache, _address,
 		offset, size, addressSpec, 0, protection, mapping, &amp;area, name,
 		addressSpec == B_EXACT_ADDRESS, kernel);
@@ -2361,7 +2365,7 @@
 
 
 area_id
-vm_map_file(team_id aid, const char *name, void **address, uint32 addressSpec,
+vm_map_file(team_id aid, const char* name, void** address, uint32 addressSpec,
 	addr_t size, uint32 protection, uint32 mapping, int fd, off_t offset)
 {
 	if (!arch_vm_supports_protection(protection))
@@ -2372,8 +2376,8 @@
 }
 
 
-vm_cache *
-vm_area_get_locked_cache(vm_area *area)
+vm_cache*
+vm_area_get_locked_cache(vm_area* area)
 {
 	mutex_lock(&amp;sAreaCacheLock);
 
@@ -2401,19 +2405,19 @@
 
 
 void
-vm_area_put_locked_cache(vm_cache *cache)
+vm_area_put_locked_cache(vm_cache* cache)
 {
 	cache-&gt;ReleaseRefAndUnlock();
 }
 
 
 area_id
-vm_clone_area(team_id team, const char *name, void **address,
+vm_clone_area(team_id team, const char* name, void** address,
 	uint32 addressSpec, uint32 protection, uint32 mapping, area_id sourceID,
 	bool kernel)
 {
-	vm_area *newArea = NULL;
-	vm_area *sourceArea;
+	vm_area* newArea = NULL;
+	vm_area* sourceArea;
 
 	// Check whether the source area exists and is cloneable. If so, mark it
 	// B_SHARED_AREA, so that we don't get problems with copy-on-write.
@@ -2433,12 +2437,12 @@
 	// Now lock both address spaces and actually do the cloning.
 
 	MultiAddressSpaceLocker locker;
-	vm_address_space *sourceAddressSpace;
+	vm_address_space* sourceAddressSpace;
 	status_t status = locker.AddArea(sourceID, false, &amp;sourceAddressSpace);
 	if (status != B_OK)
 		return status;
 
-	vm_address_space *targetAddressSpace;
+	vm_address_space* targetAddressSpace;
 	status = locker.AddTeam(team, true, &amp;targetAddressSpace);
 	if (status != B_OK)
 		return status;
@@ -2454,13 +2458,14 @@
 	if (!kernel &amp;&amp; (sourceArea-&gt;protection &amp; B_KERNEL_AREA) != 0)
 		return B_NOT_ALLOWED;
 
-	vm_cache *cache = vm_area_get_locked_cache(sourceArea);
+	vm_cache* cache = vm_area_get_locked_cache(sourceArea);
 
-	// ToDo: for now, B_USER_CLONEABLE is disabled, until all drivers
+	// TODO: for now, B_USER_CLONEABLE is disabled, until all drivers
 	//	have been adapted. Maybe it should be part of the kernel settings,
 	//	anyway (so that old drivers can always work).
 #if 0
-	if (sourceArea-&gt;aspace == vm_kernel_address_space() &amp;&amp; addressSpace != vm_kernel_address_space()
+	if (sourceArea-&gt;aspace == vm_kernel_address_space()
+		&amp;&amp; addressSpace != vm_kernel_address_space()
 		&amp;&amp; !(sourceArea-&gt;protection &amp; B_USER_CLONEABLE_AREA)) {
 		// kernel areas must not be cloned in userland, unless explicitly
 		// declared user-cloneable upon construction
@@ -2486,7 +2491,8 @@
 		// we need to map in everything at this point
 		if (sourceArea-&gt;cache_type == CACHE_TYPE_DEVICE) {
 			// we don't have actual pages to map but a physical area
-			vm_translation_map *map = &amp;sourceArea-&gt;address_space-&gt;translation_map;
+			vm_translation_map* map
+				= &amp;sourceArea-&gt;address_space-&gt;translation_map;
 			map-&gt;ops-&gt;lock(map);
 
 			addr_t physicalAddress;
@@ -2512,7 +2518,7 @@
 			map-&gt;ops-&gt;unlock(map);
 			vm_page_unreserve_pages(reservePages);
 		} else {
-			vm_translation_map *map = &amp;targetAddressSpace-&gt;translation_map;
+			vm_translation_map* map = &amp;targetAddressSpace-&gt;translation_map;
 			size_t reservePages = map-&gt;ops-&gt;map_max_pages_need(map,
 				newArea-&gt;base, newArea-&gt;base + (newArea-&gt;size - 1));
 			vm_page_reserve_pages(reservePages);
@@ -2542,11 +2548,11 @@
 
 //! The address space must be write locked at this point
 static void
-remove_area_from_address_space(vm_address_space *addressSpace, vm_area *area)
+remove_area_from_address_space(vm_address_space* addressSpace, vm_area* area)
 {
-	vm_area *temp, *last = NULL;
+	vm_area* temp = addressSpace-&gt;areas;
+	vm_area* last = NULL;
 
-	temp = addressSpace-&gt;areas;
 	while (temp != NULL) {
 		if (area == temp) {
 			if (last != NULL) {
@@ -2569,7 +2575,7 @@
 
 
 static void
-delete_area(vm_address_space *addressSpace, vm_area *area)
+delete_area(vm_address_space* addressSpace, vm_area* area)
 {
 	rw_lock_write_lock(&amp;sAreaHashLock);
 	hash_remove(sAreaHash, area);
@@ -2603,7 +2609,7 @@
 	TRACE((&quot;vm_delete_area(team = 0x%lx, area = 0x%lx)\n&quot;, team, id));
 
 	AddressSpaceWriteLocker locker;
-	vm_area *area;
+	vm_area* area;
 	status_t status = locker.SetFromArea(team, id, area);
 	if (status &lt; B_OK)
 		return status;
@@ -2627,7 +2633,7 @@
 static status_t
 vm_copy_on_write_area(vm_cache* lowerCache)
 {
-	vm_cache *upperCache;
+	vm_cache* upperCache;
 
 	TRACE((&quot;vm_copy_on_write_area(cache = %p)\n&quot;, lowerCache));
 
@@ -2653,7 +2659,7 @@
 	upperCache-&gt;areas = lowerCache-&gt;areas;
 	lowerCache-&gt;areas = NULL;
 
-	for (vm_area *tempArea = upperCache-&gt;areas; tempArea != NULL;
+	for (vm_area* tempArea = upperCache-&gt;areas; tempArea != NULL;
 			tempArea = tempArea-&gt;cache_next) {
 		ASSERT(!tempArea-&gt;no_cache_change);
 
@@ -2666,19 +2672,20 @@
 
 	lowerCache-&gt;AddConsumer(upperCache);
 
-	// We now need to remap all pages from all of the cache's areas read-only, so that
-	// a copy will be created on next write access
+	// We now need to remap all pages from all of the cache's areas read-only, so
+	// that a copy will be created on next write access
 
-	for (vm_area *tempArea = upperCache-&gt;areas; tempArea != NULL;
+	for (vm_area* tempArea = upperCache-&gt;areas; tempArea != NULL;
 			tempArea = tempArea-&gt;cache_next) {
 		// The area must be readable in the same way it was previously writable
 		uint32 protection = B_KERNEL_READ_AREA;
-		if (tempArea-&gt;protection &amp; B_READ_AREA)
+		if ((tempArea-&gt;protection &amp; B_READ_AREA) != 0)
 			protection |= B_READ_AREA;
 
-		vm_translation_map *map = &amp;tempArea-&gt;address_space-&gt;translation_map;
+		vm_translation_map* map = &amp;tempArea-&gt;address_space-&gt;translation_map;
 		map-&gt;ops-&gt;lock(map);
-		map-&gt;ops-&gt;protect(map, tempArea-&gt;base, tempArea-&gt;base - 1 + tempArea-&gt;size, protection);
+		map-&gt;ops-&gt;protect(map, tempArea-&gt;base,
+			tempArea-&gt;base - 1 + tempArea-&gt;size, protection);
 		map-&gt;ops-&gt;unlock(map);
 	}
 
@@ -2689,7 +2696,7 @@
 
 
 area_id
-vm_copy_area(team_id team, const char *name, void **_address,
+vm_copy_area(team_id team, const char* name, void** _address,
 	uint32 addressSpec, uint32 protection, area_id sourceID)
 {
 	bool writableCopy = (protection &amp; (B_KERNEL_WRITE_AREA | B_WRITE_AREA)) != 0;
@@ -2704,8 +2711,8 @@
 	// Do the locking: target address space, all address spaces associated with
 	// the source cache, and the cache itself.
 	MultiAddressSpaceLocker locker;
-	vm_address_space *targetAddressSpace;
-	vm_cache *cache;
+	vm_address_space* targetAddressSpace;
+	vm_cache* cache;
 	vm_area* source;
 	status_t status = locker.AddTeam(team, true, &amp;targetAddressSpace);
 	if (status == B_OK) {
@@ -2719,7 +2726,7 @@
 
 	if (addressSpec == B_CLONE_ADDRESS) {
 		addressSpec = B_EXACT_ADDRESS;
-		*_address = (void *)source-&gt;base;
+		*_address = (void*)source-&gt;base;
 	}
 
 	bool sharedArea = (source-&gt;protection &amp; B_SHARED_AREA) != 0;
@@ -2727,7 +2734,7 @@
 	// First, create a cache on top of the source area, respectively use the
 	// existing one, if this is a shared area.
 
-	vm_area *target;
+	vm_area* target;
 	status = map_backing_store(targetAddressSpace, cache, _address,
 		source-&gt;cache_offset, source-&gt;size, addressSpec, source-&gt;wiring,
 		protection, sharedArea ? REGION_NO_PRIVATE_MAP : REGION_PRIVATE_MAP,
@@ -2758,9 +2765,9 @@
 
 //! You need to hold the cache lock when calling this function
 static int32
-count_writable_areas(vm_cache *cache, vm_area *ignoreArea)
+count_writable_areas(vm_cache* cache, vm_area* ignoreArea)
 {
-	struct vm_area *area = cache-&gt;areas;
+	struct vm_area* area = cache-&gt;areas;
 	uint32 count = 0;
 
 	for (; area != NULL; area = area-&gt;cache_next) {
@@ -2777,15 +2784,15 @@
 vm_set_area_protection(team_id team, area_id areaID, uint32 newProtection,
 	bool kernel)
 {
-	TRACE((&quot;vm_set_area_protection(team = %#lx, area = %#lx, protection = %#lx)\n&quot;,
-		team, areaID, newProtection));
+	TRACE((&quot;vm_set_area_protection(team = %#lx, area = %#lx, protection = &quot;
+		&quot;%#lx)\n&quot;, team, areaID, newProtection));
 
 	if (!arch_vm_supports_protection(newProtection))
 		return B_NOT_SUPPORTED;
 
 	// lock address spaces and cache
 	MultiAddressSpaceLocker locker;
-	vm_cache *cache;
+	vm_cache* cache;
 	vm_area* area;
 	status_t status = locker.AddAreaCacheAndLock(areaID, true, false, area,
 		&amp;cache, true);
@@ -2818,7 +2825,8 @@
 
 				status = cache-&gt;Commit(cache-&gt;page_count * B_PAGE_SIZE);
 
-				// ToDo: we may be able to join with our source cache, if count == 0
+				// TODO: we may be able to join with our source cache, if
+				// count == 0
 			}
 		}
 	} else if ((area-&gt;protection &amp; (B_WRITE_AREA | B_KERNEL_WRITE_AREA)) == 0
@@ -2844,7 +2852,7 @@
 				// a lower cache.
 				changePageProtection = false;
 
-				struct vm_translation_map *map
+				struct vm_translation_map* map
 					= &amp;area-&gt;address_space-&gt;translation_map;
 				map-&gt;ops-&gt;lock(map);
 
@@ -2865,7 +2873,7 @@
 
 	if (status == B_OK) {
 		// remap existing pages in this cache
-		struct vm_translation_map *map = &amp;area-&gt;address_space-&gt;translation_map;
+		struct vm_translation_map* map = &amp;area-&gt;address_space-&gt;translation_map;
 
 		if (changePageProtection) {
 			map-&gt;ops-&gt;lock(map);
@@ -2882,9 +2890,9 @@
 
 
 status_t
-vm_get_page_mapping(team_id team, addr_t vaddr, addr_t *paddr)
+vm_get_page_mapping(team_id team, addr_t vaddr, addr_t* paddr)
 {
-	vm_address_space *addressSpace = vm_get_address_space(team);
+	vm_address_space* addressSpace = vm_get_address_space(team);
 	if (addressSpace == NULL)
 		return B_BAD_TEAM_ID;
 
@@ -2898,7 +2906,7 @@
 
 
 static inline addr_t
-virtual_page_address(vm_area *area, vm_page *page)
+virtual_page_address(vm_area* area, vm_page* page)
 {
 	return area-&gt;base
 		+ ((page-&gt;cache_offset &lt;&lt; PAGE_SHIFT) - area-&gt;cache_offset);
@@ -2906,15 +2914,15 @@
 
 
 bool
-vm_test_map_modification(vm_page *page)
+vm_test_map_modification(vm_page* page)
 {
 	MutexLocker locker(sMappingLock);
 
 	vm_page_mappings::Iterator iterator = page-&gt;mappings.GetIterator();
-	vm_page_mapping *mapping;
+	vm_page_mapping* mapping;
 	while ((mapping = iterator.Next()) != NULL) {
-		vm_area *area = mapping-&gt;area;
-		vm_translation_map *map = &amp;area-&gt;address_space-&gt;translation_map;
+		vm_area* area = mapping-&gt;area;
+		vm_translation_map* map = &amp;area-&gt;address_space-&gt;translation_map;
 
 		addr_t physicalAddress;
 		uint32 flags;
@@ -2923,7 +2931,7 @@
 			&amp;physicalAddress, &amp;flags);
 		map-&gt;ops-&gt;unlock(map);
 
-		if (flags &amp; PAGE_MODIFIED)
+		if ((flags &amp; PAGE_MODIFIED) != 0)
 			return true;
 	}
 
@@ -2932,7 +2940,7 @@
 
 
 int32
-vm_test_map_activation(vm_page *page, bool *_modified)
+vm_test_map_activation(vm_page* page, bool* _modified)
 {
 	int32 activation = 0;
 	bool modified = false;
@@ -2940,10 +2948,10 @@
 	MutexLocker locker(sMappingLock);
 
 	vm_page_mappings::Iterator iterator = page-&gt;mappings.GetIterator();
-	vm_page_mapping *mapping;
+	vm_page_mapping* mapping;
 	while ((mapping = iterator.Next()) != NULL) {
-		vm_area *area = mapping-&gt;area;
-		vm_translation_map *map = &amp;area-&gt;address_space-&gt;translation_map;
+		vm_area* area = mapping-&gt;area;
+		vm_translation_map* map = &amp;area-&gt;address_space-&gt;translation_map;
 
 		addr_t physicalAddress;
 		uint32 flags;
@@ -2952,9 +2960,9 @@
 			&amp;physicalAddress, &amp;flags);
 		map-&gt;ops-&gt;unlock(map);
 
-		if (flags &amp; PAGE_ACCESSED)
+		if ((flags &amp; PAGE_ACCESSED) != 0)
 			activation++;
-		if (flags &amp; PAGE_MODIFIED)
+		if ((flags &amp; PAGE_MODIFIED) != 0)
 			modified = true;
 	}
 
@@ -2966,15 +2974,15 @@
 
 
 void
-vm_clear_map_flags(vm_page *page, uint32 flags)
+vm_clear_map_flags(vm_page* page, uint32 flags)
 {
 	MutexLocker locker(sMappingLock);
 
 	vm_page_mappings::Iterator iterator = page-&gt;mappings.GetIterator();
-	vm_page_mapping *mapping;
+	vm_page_mapping* mapping;
 	while ((mapping = iterator.Next()) != NULL) {
-		vm_area *area = mapping-&gt;area;
-		vm_translation_map *map = &amp;area-&gt;address_space-&gt;translation_map;
+		vm_area* area = mapping-&gt;area;
+		vm_translation_map* map = &amp;area-&gt;address_space-&gt;translation_map;
 
 		map-&gt;ops-&gt;lock(map);
 		map-&gt;ops-&gt;clear_flags(map, virtual_page_address(area, page), flags);
@@ -2988,7 +2996,7 @@
 	The accumulated page flags of all mappings can be found in \a _flags.
 */
 void
-vm_remove_all_page_mappings(vm_page *page, uint32 *_flags)
+vm_remove_all_page_mappings(vm_page* page, uint32* _flags)
 {
 	uint32 accumulatedFlags = 0;
 	MutexLocker locker(sMappingLock);
@@ -2997,10 +3005,10 @@
 	queue.MoveFrom(&amp;page-&gt;mappings);
 
 	vm_page_mappings::Iterator iterator = queue.GetIterator();
-	vm_page_mapping *mapping;
+	vm_page_mapping* mapping;
 	while ((mapping = iterator.Next()) != NULL) {
-		vm_area *area = mapping-&gt;area;
-		vm_translation_map *map = &amp;area-&gt;address_space-&gt;translation_map;
+		vm_area* area = mapping-&gt;area;
+		vm_translation_map* map = &amp;area-&gt;address_space-&gt;translation_map;
 		addr_t physicalAddress;
 		uint32 flags;
 
@@ -3033,9 +3041,9 @@
 
 
 status_t
-vm_unmap_pages(vm_area *area, addr_t base, size_t size, bool preserveModified)
+vm_unmap_pages(vm_area* area, addr_t base, size_t size, bool preserveModified)
 {
-	vm_translation_map *map = &amp;area-&gt;address_space-&gt;translation_map;
+	vm_translation_map* map = &amp;area-&gt;address_space-&gt;translation_map;
 	addr_t end = base + (size - 1);
 
 	map-&gt;ops-&gt;lock(map);
@@ -3051,7 +3059,7 @@
 			if (status &lt; B_OK || (flags &amp; PAGE_PRESENT) == 0)
 				continue;
 
-			vm_page *page = vm_lookup_page(physicalAddress / B_PAGE_SIZE);
+			vm_page* page = vm_lookup_page(physicalAddress / B_PAGE_SIZE);
 			if (page == NULL) {
 				panic(&quot;area %p looking up page failed for pa 0x%lx\n&quot;, area,
 					physicalAddress);
@@ -3074,7 +3082,7 @@

[... truncated: 1723 lines follow ...]

</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="014794.html">[Haiku-commits] r29477 - haiku/trunk/src/apps/mandelbrot
</A></li>
	<LI>Next message: <A HREF="014796.html">[Haiku-commits] r29479 -	haiku/trunk/src/add-ons/kernel/file_systems/bfs
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#14795">[ date ]</a>
              <a href="thread.html#14795">[ thread ]</a>
              <a href="subject.html#14795">[ subject ]</a>
              <a href="author.html#14795">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/haiku-commits">More information about the Haiku-commits
mailing list</a><br>
</body></html>
