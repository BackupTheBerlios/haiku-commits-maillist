<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Haiku-commits] r23939 - in haiku/trunk: headers/private/kernel	src/system/kernel src/system/kernel/vm
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/haiku-commits/2008-February/index.html" >
   <LINK REL="made" HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r23939%20-%20in%20haiku/trunk%3A%20headers/private/kernel%0A%09src/system/kernel%20src/system/kernel/vm&In-Reply-To=%3C200802102100.m1AL0F71017229%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="006041.html">
   <LINK REL="Next"  HREF="006051.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Haiku-commits] r23939 - in haiku/trunk: headers/private/kernel	src/system/kernel src/system/kernel/vm</H1>
    <B>mmlr at BerliOS</B> 
    <A HREF="mailto:haiku-commits%40lists.berlios.de?Subject=Re%3A%20%5BHaiku-commits%5D%20r23939%20-%20in%20haiku/trunk%3A%20headers/private/kernel%0A%09src/system/kernel%20src/system/kernel/vm&In-Reply-To=%3C200802102100.m1AL0F71017229%40sheep.berlios.de%3E"
       TITLE="[Haiku-commits] r23939 - in haiku/trunk: headers/private/kernel	src/system/kernel src/system/kernel/vm">mmlr at mail.berlios.de
       </A><BR>
    <I>Sun Feb 10 22:00:15 CET 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="006041.html">[Haiku-commits] r23938 - haiku/trunk/src/servers/app/drawing
</A></li>
        <LI>Next message: <A HREF="006051.html">[Haiku-commits] r23939 - in haiku/trunk: headers/private/kernel src/system/kernel  src/system/kernel/vm
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#6045">[ date ]</a>
              <a href="thread.html#6045">[ thread ]</a>
              <a href="subject.html#6045">[ subject ]</a>
              <a href="author.html#6045">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: mmlr
Date: 2008-02-10 22:00:13 +0100 (Sun, 10 Feb 2008)
New Revision: 23939
ViewCVS: <A HREF="http://svn.berlios.de/viewcvs/haiku?rev=23939&amp;view=rev">http://svn.berlios.de/viewcvs/haiku?rev=23939&amp;view=rev</A>

Added:
   haiku/trunk/headers/private/kernel/heap.h
   haiku/trunk/src/system/kernel/heap.cpp
Removed:
   haiku/trunk/headers/private/kernel/memheap.h
   haiku/trunk/src/system/kernel/heap.c
Modified:
   haiku/trunk/src/system/kernel/Jamfile
   haiku/trunk/src/system/kernel/vm/vm.cpp
Log:
Complete rework of the heap implementation. Freelists are now part of the pages
and pages are now kept in lists as well. This allows to return free pages once
a bin does not need them anymore. Partially filled pages are kept in a sorted
linked list so that allocation will always happen on the fullest page - this
favours having full pages and makes it more likely lightly used pages will get
completely empty so they can be returned. Generally this now goes more in the
direction of a slab allocator.
The allocation logic has been extracted, so a heap is now simply attachable to
a region of memory. This allows for multiple heaps and for dynamic growing. In
case the allocator runs out of free pages, an asynchronous growing thread is
notified to create a new area and attach a new heap to it.
By default the kernel heap is now set to 16MB and grows by 8MB each time all
heaps run full.
This should solve quite a few issues, like certain bins just claiming all pages
so that even if there is free space nothing can be allocated. Also it obviously
does aways with filling the heap page by page until it overgrows.
I think this is now a well performing and scalable allocator we can live with
for quite some time. It is well tested under emulation and real hardware and
performs as expected. If problems come up there is an extensive sanity checker
that can be enabled by PARANOID_VALIDATION that covers most aspects of the
allocator. For normal operation this is not necessary though and is therefore
disabled by default.

Added: haiku/trunk/headers/private/kernel/heap.h
===================================================================
--- haiku/trunk/headers/private/kernel/heap.h	2008-02-09 18:08:34 UTC (rev 23938)
+++ haiku/trunk/headers/private/kernel/heap.h	2008-02-10 21:00:13 UTC (rev 23939)
@@ -0,0 +1,34 @@
+/*
+ * Copyright 2002-2006, Axel D&#246;rfler, <A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">axeld at pinc-software.de.</A>
+ * Distributed under the terms of the MIT License.
+ *
+ * Copyright 2001-2002, Travis Geiselbrecht. All rights reserved.
+ * Distributed under the terms of the NewOS License.
+ */
+#ifndef _KERNEL_HEAP_H
+#define _KERNEL_HEAP_H
+
+#include &lt;OS.h&gt;
+
+// allocate 16MB initial heap for the kernel
+#define INITIAL_HEAP_SIZE	16 * 1024 * 1024
+// grow by another 8MB each time the heap runs out of memory
+#define HEAP_GROW_SIZE		8 * 1024 * 1024
+
+
+
+#ifdef __cplusplus
+extern &quot;C&quot; {
+#endif
+
+void *memalign(size_t alignment, size_t size);
+
+status_t heap_init(addr_t heapBase, size_t heapSize);
+status_t heap_init_post_sem();
+status_t heap_init_post_thread();
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif	/* _KERNEL_MEMHEAP_H */

Deleted: haiku/trunk/headers/private/kernel/memheap.h

Modified: haiku/trunk/src/system/kernel/Jamfile
===================================================================
--- haiku/trunk/src/system/kernel/Jamfile	2008-02-09 18:08:34 UTC (rev 23938)
+++ haiku/trunk/src/system/kernel/Jamfile	2008-02-10 21:00:13 UTC (rev 23939)
@@ -21,7 +21,7 @@
 	condition_variable.cpp
 	cpu.c
 	elf.cpp
-	heap.c
+	heap.cpp
 	image.c
 	int.c
 	kernel_daemon.c

Deleted: haiku/trunk/src/system/kernel/heap.c

Added: haiku/trunk/src/system/kernel/heap.cpp
===================================================================
--- haiku/trunk/src/system/kernel/heap.cpp	2008-02-09 18:08:34 UTC (rev 23938)
+++ haiku/trunk/src/system/kernel/heap.cpp	2008-02-10 21:00:13 UTC (rev 23939)
@@ -0,0 +1,921 @@
+/*
+ * Copyright 2008, Michael Lotz, <A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">mmlr at mlotz.ch.</A>
+ * Distributed under the terms of the MIT License.
+ *
+ * Copyright 2002-2006, Axel D&#246;rfler, <A HREF="https://lists.berlios.de/mailman/listinfo/haiku-commits">axeld at pinc-software.de.</A>
+ * Distributed under the terms of the MIT License.
+ *
+ * Copyright 2001, Travis Geiselbrecht. All rights reserved.
+ * Distributed under the terms of the NewOS License.
+ */
+#include &lt;debug.h&gt;
+#include &lt;heap.h&gt;
+#include &lt;int.h&gt;
+#include &lt;kernel.h&gt;
+#include &lt;lock.h&gt;
+#include &lt;malloc.h&gt;
+#include &lt;signal.h&gt;
+#include &lt;string.h&gt;
+#include &lt;vm.h&gt;
+
+//#define TRACE_HEAP
+#ifdef TRACE_HEAP
+#	define TRACE(x) dprintf x
+#else
+#	define TRACE(x) ;
+#endif
+
+// initialize newly allocated memory with something non zero
+#define PARANOID_KMALLOC 1
+// check for double free, and fill freed memory with 0xdeadbeef
+#define PARANOID_KFREE 1
+// validate sanity of the heap after each operation (slow!)
+#define PARANOID_VALIDATION 0
+
+typedef struct heap_page_s {
+	uint16			index;
+	uint16			bin_index : 5;
+	uint16			free_count : 10;
+	uint16			in_use : 1;
+	heap_page_s *	next;
+	heap_page_s *	prev;
+	uint16			empty_index;
+	addr_t *		free_list;
+} heap_page;
+
+// used for bin == bin_count allocations
+#define allocation_id free_count
+
+typedef struct heap_bin_s {
+	uint32		element_size;
+	uint16		max_free_count;
+	heap_page *	page_list; // sorted so that the desired page is always first
+} heap_bin;
+
+typedef struct heap_allocator_s {
+	addr_t		base;
+	size_t		size;
+	mutex		lock;
+	vint32		large_alloc_id;
+
+	uint32		bin_count;
+	uint32		page_count;
+	heap_page *	free_pages;
+
+	heap_bin *	bins;
+	heap_page *	page_table;
+
+	heap_allocator_s *	next;
+} heap_allocator;
+
+static heap_allocator *sHeapList = NULL;
+static heap_allocator *sLastGrowRequest = NULL;
+static sem_id sHeapGrowSem = -1;
+static sem_id sHeapGrownNotify = -1;
+
+
+static void
+dump_page(heap_page *page)
+{
+	uint32 count = 0;
+	for (addr_t *temp = page-&gt;free_list; temp != NULL; temp = (addr_t *)*temp)
+		count++;
+
+	dprintf(&quot;\t\tpage %p: bin_index: %u; free_count: %u; empty_index: %u; free_list %p (%lu entr%s)\n&quot;,
+		page, page-&gt;bin_index, page-&gt;free_count, page-&gt;empty_index,
+		page-&gt;free_list, count, count == 1 ? &quot;y&quot; : &quot;ies&quot;);
+}
+
+
+static void
+dump_bin(heap_bin *bin)
+{
+	dprintf(&quot;\telement_size: %lu; max_free_count: %u; page_list %p;\n&quot;,
+		bin-&gt;element_size, bin-&gt;max_free_count, bin-&gt;page_list);
+
+	for (heap_page *temp = bin-&gt;page_list; temp != NULL; temp = temp-&gt;next)
+		dump_page(temp);
+}
+
+
+static void
+dump_allocator(heap_allocator *heap)
+{
+	uint32 count = 0;
+	for (heap_page *page = heap-&gt;free_pages; page != NULL; page = page-&gt;next)
+		count++;
+
+	dprintf(&quot;allocator %p: base: 0x%08lx; size: %lu; bin_count: %lu; free_pages: %p (%lu entr%s)\n&quot;, heap,
+		heap-&gt;base, heap-&gt;size, heap-&gt;bin_count, heap-&gt;free_pages, count,
+		count == 1 ? &quot;y&quot; : &quot;ies&quot;);
+
+	for (uint32 i = 0; i &lt; heap-&gt;bin_count; i++)
+		dump_bin(&amp;heap-&gt;bins[i]);
+
+	dprintf(&quot;\n&quot;);
+}
+
+
+static int
+dump_heap_list(int argc, char **argv)
+{
+	heap_allocator *heap = sHeapList;
+	while (heap) {
+		dump_allocator(heap);
+		heap = heap-&gt;next;
+	}
+
+	return 0;
+}
+
+
+#if PARANOID_VALIDATION
+static void
+heap_validate_heap(heap_allocator *heap)
+{
+	mutex_lock(&amp;heap-&gt;lock);
+
+	// validate the free pages list
+	uint32 freePageCount = 0;
+	heap_page *lastPage = NULL;
+	heap_page *page = heap-&gt;free_pages;
+	while (page) {
+		if ((addr_t)page &lt; (addr_t)&amp;heap-&gt;page_table[0]
+			|| (addr_t)page &gt;= (addr_t)&amp;heap-&gt;page_table[heap-&gt;page_count])
+			panic(&quot;free page is not part of the page table\n&quot;);
+
+		if (page-&gt;index &gt;= heap-&gt;page_count)
+			panic(&quot;free page has invalid index\n&quot;);
+
+		if ((addr_t)&amp;heap-&gt;page_table[page-&gt;index] != (addr_t)page)
+			panic(&quot;free page index does not lead to target page\n&quot;);
+
+		if (page-&gt;prev != lastPage)
+			panic(&quot;free page entry has invalid prev link\n&quot;);
+
+		if (page-&gt;in_use)
+			panic(&quot;free page marked as in use\n&quot;);
+
+		lastPage = page;
+		page = page-&gt;next;
+		freePageCount++;
+	}
+
+	// validate the page table
+	uint32 usedPageCount = 0;
+	for (uint32 i = 0; i &lt; heap-&gt;page_count; i++) {
+		if (heap-&gt;page_table[i].in_use)
+			usedPageCount++;
+	}
+
+	if (freePageCount + usedPageCount != heap-&gt;page_count) {
+		panic(&quot;free pages and used pages do not add up (%lu + %lu != %lu)\n&quot;,
+			freePageCount, usedPageCount, heap-&gt;page_count);
+	}
+
+	// validate the bins
+	for (uint32 i = 0; i &lt; heap-&gt;bin_count; i++) {
+		heap_bin *bin = &amp;heap-&gt;bins[i];
+
+		lastPage = NULL;
+		page = bin-&gt;page_list;
+		int32 lastFreeCount = 0;
+		while (page) {
+			if ((addr_t)page &lt; (addr_t)&amp;heap-&gt;page_table[0]
+				|| (addr_t)page &gt;= (addr_t)&amp;heap-&gt;page_table[heap-&gt;page_count])
+				panic(&quot;used page is not part of the page table\n&quot;);
+
+			if (page-&gt;index &gt;= heap-&gt;page_count)
+				panic(&quot;used page has invalid index\n&quot;);
+
+			if ((addr_t)&amp;heap-&gt;page_table[page-&gt;index] != (addr_t)page)
+				panic(&quot;used page index does not lead to target page\n&quot;);
+
+			if (page-&gt;prev != lastPage)
+				panic(&quot;used page entry has invalid prev link (%p vs %p bin %lu)\n&quot;,
+					page-&gt;prev, lastPage, i);
+
+			if (!page-&gt;in_use)
+				panic(&quot;used page marked as not in use\n&quot;);
+
+			if (page-&gt;bin_index != i)
+				panic(&quot;used page with bin index %u in page list of bin %lu\n&quot;,
+					page-&gt;bin_index, i);
+
+			if (page-&gt;free_count &lt; lastFreeCount)
+				panic(&quot;ordering of bin page list broken\n&quot;);
+
+			// validate the free list
+			uint32 freeSlotsCount = 0;
+			addr_t *element = page-&gt;free_list;
+			addr_t pageBase = heap-&gt;base + page-&gt;index * B_PAGE_SIZE;
+			while (element) {
+				if ((addr_t)element &lt; pageBase
+					|| (addr_t)element &gt;= pageBase + B_PAGE_SIZE)
+					panic(&quot;free list entry out of page range\n&quot;);
+
+				if (((addr_t)element - pageBase) % bin-&gt;element_size != 0)
+					panic(&quot;free list entry not on a element boundary\n&quot;);
+
+				element = (addr_t *)*element;
+				freeSlotsCount++;
+			}
+
+			uint32 slotCount = bin-&gt;max_free_count;
+			if (page-&gt;empty_index &gt; slotCount)
+				panic(&quot;empty index beyond slot count (%u with %lu slots)\n&quot;,
+					page-&gt;empty_index, slotCount);
+
+			freeSlotsCount += (slotCount - page-&gt;empty_index);
+			if (freeSlotsCount &gt; slotCount)
+				panic(&quot;more free slots than fit into the page\n&quot;);
+
+			lastPage = page;
+			lastFreeCount = page-&gt;free_count;
+			page = page-&gt;next;
+		}
+	}
+
+	mutex_unlock(&amp;heap-&gt;lock);
+}
+#endif
+
+
+heap_allocator *
+heap_attach(addr_t base, size_t size, bool postSem)
+{
+	heap_allocator *heap = (heap_allocator *)base;
+	base += sizeof(heap_allocator);
+	size -= sizeof(heap_allocator);
+
+	size_t binSizes[] = { 16, 32, 64, 96, 128, 192, 256, 384, 512, 1024, 2048, B_PAGE_SIZE };
+	uint32 binCount = sizeof(binSizes) / sizeof(binSizes[0]);
+	heap-&gt;bin_count = binCount;
+	heap-&gt;bins = (heap_bin *)base;
+	base += binCount * sizeof(heap_bin);
+	size -= binCount * sizeof(heap_bin);
+
+	for (uint32 i = 0; i &lt; binCount; i++) {
+		heap_bin *bin = &amp;heap-&gt;bins[i];
+		bin-&gt;element_size = binSizes[i];
+		bin-&gt;max_free_count = B_PAGE_SIZE / binSizes[i];
+		bin-&gt;page_list = NULL;
+	}
+
+	uint32 pageCount = size / B_PAGE_SIZE;
+	size_t pageTableSize = pageCount * sizeof(heap_page);
+	heap-&gt;page_table = (heap_page *)base;
+	base += pageTableSize;
+	size -= pageTableSize;
+
+	// the rest is now actually usable memory (rounded to the next page)
+	heap-&gt;base = (addr_t)(base + B_PAGE_SIZE - 1) / B_PAGE_SIZE * B_PAGE_SIZE;
+	heap-&gt;size = (size_t)(size / B_PAGE_SIZE) * B_PAGE_SIZE;
+
+	// now we know the real page count
+	pageCount = heap-&gt;size / B_PAGE_SIZE;
+	heap-&gt;page_count = pageCount;
+
+	// zero out the heap alloc table at the base of the heap
+	memset((void *)heap-&gt;page_table, 0, pageTableSize);
+	for (uint32 i = 0; i &lt; pageCount; i++)
+		heap-&gt;page_table[i].index = i;
+
+	// add all pages up into the free pages list
+	for (uint32 i = 1; i &lt; pageCount; i++) {
+		heap-&gt;page_table[i - 1].next = &amp;heap-&gt;page_table[i];
+		heap-&gt;page_table[i].prev = &amp;heap-&gt;page_table[i - 1];
+	}
+	heap-&gt;free_pages = &amp;heap-&gt;page_table[0];
+	heap-&gt;page_table[0].prev = NULL;
+
+	if (postSem) {
+		if (mutex_init(&amp;heap-&gt;lock, &quot;heap_mutex&quot;) &lt; 0) {
+			panic(&quot;heap_attach(): error creating heap mutex\n&quot;);
+			return NULL;
+		}
+	} else {
+		// pre-init the mutex to at least fall through any semaphore calls
+		heap-&gt;lock.sem = -1;
+		heap-&gt;lock.holder = -1;
+	}
+
+	heap-&gt;next = NULL;
+	dprintf(&quot;heap_attach: attached to %p - usable range 0x%08lx - 0x%08lx\n&quot;,
+		heap, heap-&gt;base, heap-&gt;base + heap-&gt;size);
+	return heap;
+}
+
+
+static inline uint32
+heap_next_alloc_id(heap_allocator *heap)
+{
+	return atomic_add(&amp;heap-&gt;large_alloc_id, 1) &amp; ((1 &lt;&lt; 9) - 1);
+}
+
+
+static inline void
+heap_link_page(heap_page *page, heap_page **list)
+{
+	page-&gt;prev = NULL;
+	page-&gt;next = *list;
+	if (page-&gt;next)
+		page-&gt;next-&gt;prev = page;
+	*list = page;
+}
+
+
+static inline void
+heap_unlink_page(heap_page *page, heap_page **list)
+{
+	if (page-&gt;prev)
+		page-&gt;prev-&gt;next = page-&gt;next;
+	if (page-&gt;next)
+		page-&gt;next-&gt;prev = page-&gt;prev;
+	if (list &amp;&amp; *list == page) {
+		*list = page-&gt;next;
+		if (page-&gt;next)
+			page-&gt;next-&gt;prev = NULL;
+	}
+}
+
+
+static void *
+heap_raw_alloc(heap_allocator *heap, size_t size, uint32 binIndex)
+{
+	heap_bin *bin = NULL;
+	if (binIndex &lt; heap-&gt;bin_count)
+		bin = &amp;heap-&gt;bins[binIndex];
+
+	if (bin &amp;&amp; bin-&gt;page_list != NULL) {
+		// we have a page where we have a free slot
+		void *address = NULL;
+		heap_page *page = bin-&gt;page_list;
+		if (page-&gt;free_list) {
+			// there's a previously freed entry we can use
+			address = page-&gt;free_list;
+			page-&gt;free_list = (addr_t *)*page-&gt;free_list;
+		} else {
+			// the page hasn't been fully allocated so use the next empty_index
+			address = (void *)(heap-&gt;base + page-&gt;index * B_PAGE_SIZE
+				+ page-&gt;empty_index * bin-&gt;element_size);
+			page-&gt;empty_index++;
+		}
+
+		page-&gt;free_count--;
+		if (page-&gt;free_count == 0) {
+			// the page is now full so we remove it from the page_list
+			bin-&gt;page_list = page-&gt;next;
+			if (page-&gt;next)
+				page-&gt;next-&gt;prev = NULL;
+			page-&gt;next = page-&gt;prev = NULL;
+		}
+
+		return address;
+	}
+
+	// we don't have anything free right away, we must allocate a new page
+	if (heap-&gt;free_pages == NULL) {
+		// there are no free pages anymore, we ran out of memory
+		TRACE((&quot;heap %p: no free pages to allocate %lu bytes\n&quot;, heap, size));
+		return NULL;
+	}
+
+	if (bin) {
+		// small allocation, just grab the next free page
+		heap_page *page = heap-&gt;free_pages;
+		heap-&gt;free_pages = page-&gt;next;
+		if (page-&gt;next)
+			page-&gt;next-&gt;prev = NULL;
+
+		page-&gt;in_use = 1;
+		page-&gt;bin_index = binIndex;
+		page-&gt;free_count = bin-&gt;max_free_count - 1;
+		page-&gt;empty_index = 1;
+		page-&gt;free_list = NULL;
+		page-&gt;next = page-&gt;prev = NULL;
+
+		if (page-&gt;free_count &gt; 0) {
+			// by design there are no other pages in the bins page list
+			bin-&gt;page_list = page;
+		}
+
+		// we return the first slot in this page
+		return (void *)(heap-&gt;base + page-&gt;index * B_PAGE_SIZE);
+	}
+
+	// large allocation, we must search for contiguous slots
+	bool found = false;
+	int32 first = -1;
+	for (uint32 i = 0; i &lt; heap-&gt;page_count; i++) {
+		if (heap-&gt;page_table[i].in_use) {
+			first = -1;
+			continue;
+		}
+
+		if (first &gt; 0) {
+			if ((1 + i - first) * B_PAGE_SIZE &gt;= size) {
+				found = true;
+				break;
+			}
+		} else
+			first = i;
+	}
+
+	if (!found) {
+		TRACE((&quot;heap %p: found no contiguous pages to allocate %ld bytes\n&quot;, heap, size));
+		return NULL;
+	}
+
+	uint32 allocationID = heap_next_alloc_id(heap);
+	uint32 pageCount = (size + B_PAGE_SIZE - 1) / B_PAGE_SIZE;
+	for (uint32 i = first; i &lt; first + pageCount; i++) {
+		heap_page *page = &amp;heap-&gt;page_table[i];
+		page-&gt;in_use = 1;
+		page-&gt;bin_index = binIndex;
+
+		heap_unlink_page(page, &amp;heap-&gt;free_pages);
+
+		page-&gt;next = page-&gt;prev = NULL;
+		page-&gt;free_list = NULL;
+		page-&gt;allocation_id = allocationID;
+	}
+
+	return (void *)(heap-&gt;base + first * B_PAGE_SIZE);
+}
+
+
+#if DEBUG
+static bool
+is_valid_alignment(size_t number)
+{
+	// this cryptic line accepts zero and all powers of two
+	return ((~number + 1) | ((number &lt;&lt; 1) - 1)) == ~0UL;
+}
+#endif
+
+
+static void *
+heap_memalign(heap_allocator *heap, size_t alignment, size_t size,
+	bool *shouldGrow)
+{
+	TRACE((&quot;memalign(alignment = %lu, size = %lu)\n&quot;, alignment, size));
+
+#if DEBUG
+	if (!is_valid_alignment(alignment))
+		panic(&quot;memalign() with an alignment which is not a power of 2\n&quot;);
+#endif
+
+	mutex_lock(&amp;heap-&gt;lock);
+
+	// ToDo: that code &quot;aligns&quot; the buffer because the bins are always
+	//	aligned on their bin size
+	if (size &lt; alignment)
+		size = alignment;
+
+	uint32 binIndex;
+	for (binIndex = 0; binIndex &lt; heap-&gt;bin_count; binIndex++) {
+		if (size &lt;= heap-&gt;bins[binIndex].element_size)
+			break;
+	}
+
+	void *address = heap_raw_alloc(heap, size, binIndex);
+
+	TRACE((&quot;memalign(): asked to allocate %lu bytes, returning pointer %p\n&quot;, size, address));
+
+	if (heap-&gt;next == NULL) {
+		// suggest growing if we are the last heap and we have
+		// less than three free pages left
+		*shouldGrow = (heap-&gt;free_pages == NULL
+			|| heap-&gt;free_pages-&gt;next == NULL
+			|| heap-&gt;free_pages-&gt;next-&gt;next == NULL);
+	}
+
+	mutex_unlock(&amp;heap-&gt;lock);
+	if (address == NULL)
+		return address;
+
+#if PARANOID_KFREE
+	// make sure 0xdeadbeef is cleared if we do not overwrite the memory
+	// and the user does not clear it
+	((uint32 *)address)[1] = 0xcccccccc;
+#endif
+
+#if PARANOID_KMALLOC
+	memset(address, 0xcc, size);
+#endif
+
+	return address;
+}
+
+
+static status_t
+heap_free(heap_allocator *heap, void *address)
+{
+	if (address == NULL)
+		return B_OK;
+
+	if ((addr_t)address &lt; heap-&gt;base
+		|| (addr_t)address &gt;= heap-&gt;base + heap-&gt;size) {
+		// this address does not belong to us
+		return B_ENTRY_NOT_FOUND;
+	}
+
+	mutex_lock(&amp;heap-&gt;lock);
+
+	TRACE((&quot;free(): asked to free at ptr = %p\n&quot;, address));
+
+	heap_page *page = &amp;heap-&gt;page_table[((addr_t)address - heap-&gt;base) / B_PAGE_SIZE];
+
+	TRACE((&quot;free(): page %p: bin_index %d, free_count %d\n&quot;, page, page-&gt;bin_index, page-&gt;free_count));
+
+	if (page-&gt;bin_index &gt; heap-&gt;bin_count) {
+		panic(&quot;free(): page %p: invalid bin_index %d\n&quot;, page, page-&gt;bin_index);
+		mutex_unlock(&amp;heap-&gt;lock);
+		return B_ERROR;
+	}
+
+	if (page-&gt;bin_index &lt; heap-&gt;bin_count) {
+		// small allocation
+		heap_bin *bin = &amp;heap-&gt;bins[page-&gt;bin_index];
+		if (((addr_t)address - heap-&gt;base - page-&gt;index * B_PAGE_SIZE) % bin-&gt;element_size != 0) {
+			panic(&quot;free(): passed invalid pointer %p supposed to be in bin for element size %ld\n&quot;, address, bin-&gt;element_size);
+			mutex_unlock(&amp;heap-&gt;lock);
+			return B_ERROR;
+		}
+
+#if PARANOID_KFREE
+		if (((uint32 *)address)[1] == 0xdeadbeef) {
+			// This block looks like it was freed already, walk the free list
+			// on this page to make sure this address doesn't exist.
+			for (addr_t *temp = page-&gt;free_list; temp != NULL; temp = (addr_t *)*temp) {
+				if (temp == address) {
+					panic(&quot;free(): address %p already exists in page free list\n&quot;, address);
+					mutex_unlock(&amp;heap-&gt;lock);
+					return B_ERROR;
+				}
+			}
+		}
+
+		uint32 *dead = (uint32 *)address;
+		if (bin-&gt;element_size % 4 != 0) {
+			panic(&quot;free(): didn't expect a bin element size that is not a multiple of 4\n&quot;);
+			mutex_unlock(&amp;heap-&gt;lock);
+			return B_ERROR;
+		}
+
+		// the first 4 bytes are overwritten with the next free list pointer later
+		for (uint32 i = 1; i &lt; bin-&gt;element_size / sizeof(uint32); i++)
+			dead[i] = 0xdeadbeef;
+#endif
+
+		// add the address to the page free list
+		*(addr_t *)address = (addr_t)page-&gt;free_list;
+		page-&gt;free_list = (addr_t *)address;
+		page-&gt;free_count++;
+
+		if (page-&gt;free_count == bin-&gt;max_free_count) {
+			// we are now empty, remove the page from the bin list
+			heap_unlink_page(page, &amp;bin-&gt;page_list);
+			page-&gt;in_use = 0;
+			heap_link_page(page, &amp;heap-&gt;free_pages);
+		} else if (page-&gt;free_count == 1) {
+			// we need to add ourselfs to the page list of the bin
+			heap_link_page(page, &amp;bin-&gt;page_list);
+		} else {
+			// we might need to move back in the free pages list
+			if (page-&gt;next &amp;&amp; page-&gt;next-&gt;free_count &lt; page-&gt;free_count) {
+				// move ourselfs so the list stays ordered
+				heap_page *insert = page-&gt;next;
+				while (insert-&gt;next
+					&amp;&amp; insert-&gt;next-&gt;free_count &lt; page-&gt;free_count)
+					insert = insert-&gt;next;
+
+				heap_unlink_page(page, &amp;bin-&gt;page_list);
+
+				page-&gt;prev = insert;
+				page-&gt;next = insert-&gt;next;
+				if (page-&gt;next)
+					page-&gt;next-&gt;prev = page;
+				insert-&gt;next = page;
+			}
+		}
+	} else {
+		// large allocation, just return the pages to the page free list
+		uint32 allocationID = page-&gt;allocation_id;
+		uint32 maxPages = heap-&gt;page_count - page-&gt;index;
+		for (uint32 i = 0; i &lt; maxPages; i++) {
+			// loop until we find the end of this allocation
+			if (!page[i].in_use || page[i].bin_index != heap-&gt;bin_count
+				|| page[i].allocation_id != allocationID)
+				break;
+
+			// this page still belongs to the same allocation
+			page[i].in_use = 0;
+			page[i].allocation_id = 0;
+
+			// return it to the free list
+			heap_link_page(&amp;page[i], &amp;heap-&gt;free_pages);
+		}
+	}
+
+	mutex_unlock(&amp;heap-&gt;lock);
+	return B_OK;
+}
+
+
+static status_t
+heap_realloc(heap_allocator *heap, void *address, void **newAddress,
+	size_t newSize)
+{
+	if ((addr_t)address &lt; heap-&gt;base
+		|| (addr_t)address &gt;= heap-&gt;base + heap-&gt;size) {
+		// this address does not belong to us
+		return B_ENTRY_NOT_FOUND;
+	}
+
+	mutex_lock(&amp;heap-&gt;lock);
+	TRACE((&quot;realloc(address = %p, newSize = %lu)\n&quot;, address, newSize));
+
+	heap_page *page = &amp;heap-&gt;page_table[((addr_t)address - heap-&gt;base) / B_PAGE_SIZE];
+	if (page-&gt;bin_index &gt; heap-&gt;bin_count) {
+		panic(&quot;realloc(): page %p: invalid bin_index %d\n&quot;, page, page-&gt;bin_index);
+		mutex_unlock(&amp;heap-&gt;lock);
+		return B_ERROR;
+	}
+
+	// find out the size of the old allocation first
+	size_t minSize = 0;
+	size_t maxSize = 0;
+	if (page-&gt;bin_index &lt; heap-&gt;bin_count) {
+		// this was a small allocation
+		heap_bin *bin = &amp;heap-&gt;bins[page-&gt;bin_index];
+		maxSize = bin-&gt;element_size;
+		if (page-&gt;bin_index &gt; 0)
+			minSize = heap-&gt;bins[page-&gt;bin_index - 1].element_size + 1;
+	} else {
+		// this was a large allocation
+		uint32 allocationID = page-&gt;allocation_id;
+		uint32 maxPages = heap-&gt;page_count - page-&gt;index;
+		maxSize = B_PAGE_SIZE;
+		for (uint32 i = 1; i &lt; maxPages; i++) {
+			if (!page[i].in_use || page[i].bin_index != heap-&gt;bin_count
+				|| page[i].allocation_id != allocationID)
+				break;
+
+			minSize += B_PAGE_SIZE;
+			maxSize += B_PAGE_SIZE;
+		}
+	}
+
+	mutex_unlock(&amp;heap-&gt;lock);
+
+	// does the new allocation simply fit in the old allocation?
+	if (newSize &gt; minSize &amp;&amp; newSize &lt;= maxSize) {
+		*newAddress = address;
+		return B_OK;
+	}
+
+	// if not, allocate a new chunk of memory
+	*newAddress = malloc(newSize);
+	if (*newAddress == NULL) {
+		// we tried but it didn't work out, but still the operation is done
+		return B_OK;
+	}
+
+	// copy the old data and free the old allocation
+	memcpy(*newAddress, address, min_c(maxSize, newSize));
+	free(address);
+	return B_OK;
+}
+
+
+//	#pragma mark -
+
+
+static int32
+heap_grow_thread(void *)
+{
+	heap_allocator *heap = sHeapList;
+	while (true) {
+		// wait for a request to grow the heap list
+		if (acquire_sem(sHeapGrowSem) &lt; B_OK)
+			continue;
+
+		// find the last heap
+		while (heap-&gt;next)
+			heap = heap-&gt;next;
+
+		if (sLastGrowRequest != heap) {
+			// we have already grown since the latest request, just ignore
+			continue;
+		}
+
+		TRACE((&quot;heap_grower: kernel heap will run out of memory soon, allocating new one\n&quot;));
+		void *heapAddress = NULL;
+		area_id heapArea = create_area(&quot;additional heap&quot;, &amp;heapAddress,
+			B_ANY_KERNEL_BLOCK_ADDRESS, HEAP_GROW_SIZE, B_FULL_LOCK,
+			B_KERNEL_READ_AREA | B_KERNEL_WRITE_AREA);
+		if (heapArea &lt; B_OK) {
+			panic(&quot;heap_grower: couldn't allocate additional heap area\n&quot;);
+			continue;
+		}
+
+		heap_allocator *newHeap = heap_attach((addr_t)heapAddress,
+			HEAP_GROW_SIZE, true);
+		if (newHeap == NULL) {
+			panic(&quot;heap_grower: could not attach additional heap!\n&quot;);
+			delete_area(heapArea);
+			continue;
+		}
+
+#if PARANOID_VALIDATION
+		heap_validate_heap(newHeap);
+#endif
+		heap-&gt;next = newHeap;
+		TRACE((&quot;heap_grower: new heap linked in\n&quot;));
+
+		// notify anyone waiting for this request
+		release_sem_etc(sHeapGrownNotify, -1, B_RELEASE_ALL);
+	}
+
+	return 0;
+}
+
+
+status_t
+heap_init(addr_t base, size_t size)
+{
+	sHeapList = heap_attach(base, size, false);
+
+	// set up some debug commands
+	add_debugger_command(&quot;heap&quot;, &amp;dump_heap_list, &quot;dump stats about the kernel heap(s)&quot;);
+	return B_OK;
+}
+
+
+status_t
+heap_init_post_sem()
+{
+	// create the lock for the initial heap
+	if (mutex_init(&amp;sHeapList-&gt;lock, &quot;heap_mutex&quot;) &lt; B_OK) {
+		panic(&quot;heap_init_post_sem(): error creating heap mutex\n&quot;);
+		return B_ERROR;
+	}
+
+	sHeapGrowSem = create_sem(0, &quot;heap_grow_sem&quot;);
+	if (sHeapGrowSem &lt; 0) {
+		panic(&quot;heap_init_post_sem(): failed to create heap grow sem\n&quot;);
+		return B_ERROR;
+	}
+
+	sHeapGrownNotify = create_sem(0, &quot;heap_grown_notify&quot;);
+	if (sHeapGrownNotify &lt; 0) {
+		panic(&quot;heap_init_post_sem(): failed to create heap grown notify sem\n&quot;);
+		return B_ERROR;
+	}
+
+	return B_OK;
+}
+
+
+status_t
+heap_init_post_thread()
+{
+	thread_id thread = spawn_kernel_thread(heap_grow_thread, &quot;heap grower&quot;,
+		B_URGENT_PRIORITY, NULL);
+	if (thread &lt; 0) {
+		panic(&quot;heap_init_post_thread(): cannot create heap grow thread\n&quot;);
+		return B_ERROR;
+	}
+
+	send_signal_etc(thread, SIGCONT, B_DO_NOT_RESCHEDULE);
+	return B_OK;
+}
+
+
+//	#pragma mark -
+
+
+void *
+memalign(size_t alignment, size_t size)
+{
+	if (!kernel_startup &amp;&amp; !are_interrupts_enabled()) {
+		panic(&quot;memalign(): called with interrupts disabled\n&quot;);
+		return NULL;
+	}
+
+	if (size &gt; (HEAP_GROW_SIZE * 3) / 4) {
+		// don't even attempt such a huge allocation
+		panic(&quot;heap: huge allocation of %lu bytes asked!\n&quot;, size);
+		return NULL;
+	}
+
+	heap_allocator *heap = sHeapList;
+	while (heap) {
+		bool shouldGrow = false;
+		void *result = heap_memalign(heap, alignment, size, &amp;shouldGrow);
+		if (heap-&gt;next == NULL &amp;&amp; (shouldGrow || result == NULL)) {
+			// the last heap will or has run out of memory, notify the grower
+			sLastGrowRequest = heap;
+			if (result == NULL) {
+				// urgent request, do the request and wait for at max 250ms
+				release_sem(sHeapGrowSem);
+				acquire_sem_etc(sHeapGrownNotify, 1, B_RELATIVE_TIMEOUT, 250000);
+			} else {
+				// not so urgent, just notify the grower
+				release_sem_etc(sHeapGrowSem, 1, B_DO_NOT_RESCHEDULE);
+			}
+		}
+
+		if (result == NULL) {
+			heap = heap-&gt;next;
+			continue;
+		}
+
+#if PARANOID_VALIDATION
+		heap_validate_heap(heap);
+#endif
+
+		return result;
+	}
+
+	panic(&quot;heap: kernel heap has run out of memory\n&quot;);
+	return NULL;
+}
+
+
+void *
+malloc(size_t size)
+{
+	return memalign(0, size);
+}
+
+
+void
+free(void *address)
+{
+	if (!kernel_startup &amp;&amp; !are_interrupts_enabled()) {
+		panic(&quot;free(): called with interrupts disabled\n&quot;);
+		return;
+	}
+
+	heap_allocator *heap = sHeapList;
+	while (heap) {
+		if (heap_free(heap, address) == B_OK) {
+#if PARANOID_VALIDATION
+			heap_validate_heap(heap);
+#endif
+			return;
+		}
+
+		heap = heap-&gt;next;
+	}
+
+	panic(&quot;free(): free failed for address %p\n&quot;, address);
+}
+
+
+void *
+realloc(void *address, size_t newSize)
+{
+	if (!kernel_startup &amp;&amp; !are_interrupts_enabled()) {
+		panic(&quot;realloc(): called with interrupts disabled\n&quot;);
+		return NULL;
+	}
+
+	if (address == NULL)
+		return malloc(newSize);
+
+	if (newSize == 0) {
+		free(address);
+		return NULL;
+	}
+
+	heap_allocator *heap = sHeapList;
+	while (heap) {
+		void *newAddress = NULL;
+		if (heap_realloc(heap, address, &amp;newAddress, newSize) == B_OK) {
+#if PARANOID_VALIDATION
+			heap_validate_heap(heap);
+#endif
+			return newAddress;
+		}
+
+		heap = heap-&gt;next;
+	}
+
+	panic(&quot;realloc(): failed to realloc address %p to size %lu\n&quot;, address, newSize);
+	return NULL;
+}
+
+
+void *
+calloc(size_t numElements, size_t size)
+{
+	void *address = memalign(0, numElements * size);
+	if (address != NULL)
+		memset(address, 0, numElements * size);
+
+	return address;
+}

Modified: haiku/trunk/src/system/kernel/vm/vm.cpp
===================================================================
--- haiku/trunk/src/system/kernel/vm/vm.cpp	2008-02-09 18:08:34 UTC (rev 23938)
+++ haiku/trunk/src/system/kernel/vm/vm.cpp	2008-02-10 21:00:13 UTC (rev 23939)
@@ -25,7 +25,7 @@
 #include &lt;vm_cache.h&gt;
 #include &lt;vm_low_memory.h&gt;
 #include &lt;file_cache.h&gt;
-#include &lt;memheap.h&gt;
+#include &lt;heap.h&gt;
 #include &lt;condition_variable.h&gt;
 #include &lt;debug.h&gt;
 #include &lt;console.h&gt;
@@ -3480,16 +3480,8 @@
 	vm_page_init_num_pages(args);

[... truncated: 37 lines follow ...]

</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="006041.html">[Haiku-commits] r23938 - haiku/trunk/src/servers/app/drawing
</A></li>
	<LI>Next message: <A HREF="006051.html">[Haiku-commits] r23939 - in haiku/trunk: headers/private/kernel src/system/kernel  src/system/kernel/vm
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#6045">[ date ]</a>
              <a href="thread.html#6045">[ thread ]</a>
              <a href="subject.html#6045">[ subject ]</a>
              <a href="author.html#6045">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/haiku-commits">More information about the Haiku-commits
mailing list</a><br>
</body></html>
